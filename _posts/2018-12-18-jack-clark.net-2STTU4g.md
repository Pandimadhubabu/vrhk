---

layout: post
category: product
title: "Import AI 125: SenseTime trains AIs to imitate human AI architects; Berkeley researchers fuse AI for FrankenRL system; and fake images from NVIDIA cross the uncanny valley."
date: 2018-12-18 07:51:59
link: https://vrhk.co/2STTU4g
image: 
domain: jack-clark.net
author: "Jack Clark"
icon: https://s2.wp.com/i/webclip.png
excerpt: "Berkeley researchers create Franken-RL, fusing hand-engineered systems and RL-based controllers:&hellip;Use hand-engineered controllers for the stuff they&rsquo;re good at, and use RL to learn the tricky things&hellip;Researchers with the University of California at Berkeley, Siemens Corporation, and the Hamburg University of Technology have combined classical robotics control techniques with reinforcement learning to create robots that can deal with complex tasks like block-stacking. &nbsp;&nbsp;The technique, which they call Residual Reinforcement Learning, uses &ldquo;conventional feedback control theory&rdquo; to learn to control the robot, and reinforcement learning to learn how to interact with the objects in the robot&rsquo;s world. The described technique mushes both of these techniques together. &ldquo;The key idea is to combine the flexibility of RL with the efficiency of conventional controllers by additively combining a learnable parametrized policy with a fixed hand-engineered controller&rdquo;, the researchers write.&nbsp; Testing on a real robot: The researchers show that residual RL approaches are more sample-efficient than those without it, with these traits verified in both simulation learning as well as in tests on a real robot. They also show that systems trained with Residual RL can better deal with confounding situations, like working out how to perform block assembly when the blocks have been moved into situations designed to confused the hand-written controller. &nbsp;&nbsp;Why it matters: Approaches like this show how today&rsquo;s contemporary AI techniques, like TD3 trained via RL as in the experiments here, can be combined with hand-written rule-based systems to create powerful AI applications. This is a trend that is likely to continue, and it suggests that the distinctions between systems which contain AI and which don&rsquo;t contain AI will become increasingly blurred.&nbsp; Read more: Residual Reinforcement Learning for Robot Control (Arxiv).
NVIDIA researchers show how fake image news is getting closer:&hellip;Synthetic faces roll through the uncanny valley, with a little help from GANs and the use of noise&hellip;Researchers with NVIDIA have shown how to use techniques cribbed from style transfer work on image generation, to create synthetic images of unparalleled quality. The research indicates that we&rsquo;re now at the point where neural networks are capable of generating single-frame synthetic images of a quality sufficient to trick (most) humans. While this paper does include a brief discussion of bias inherent to training images (good!) it does not at any point discuss what the policy implications are of systems capable of generating customizable fake human faces, which feels like a missed opportunity.&nbsp;&nbsp;How it works: &ldquo;Our generator starts from a learned constant input and adjusts the &ldquo;style&rdquo; of the image at each convolution layer based on the latent code, therefore directly controlling the strength of image features at different scales&rdquo;, the researchers explain. They also inject noise into the network at various different and find that the addition of noise helps create complex and coherent structures in subtle facial features like hair, earlobes, and so on. &ldquo;We hypothesize that at any point in the generator, there is pressure to introduce new content as soon as possible, and the easiest way for our network to create stochastic variation is to rely on the noise provided.&rdquo; &nbsp;&nbsp;Why it matters: These photorealistic faces are especially striking when we consider that ~4 years ago the best things AI systems were capable of was generating smeared, flattened, black&amp;white pixelated faces, as seen in the original generative adversarial networks paper (Arxiv). I wonder how long it will take us till we can generate coherent videos over lengthy time periods.&nbsp; Read more: A Style-Based Generator Architecture for Generative Adversarial Networks (Arxiv). &nbsp;&nbsp;Get more information and the data: NVIDIA has said it plans to release the source code, some pre-trained networks, and the FFHQ dataset &ldquo;soon&rdquo;. Get them from here (NVIDIA placeholding Google Doc).
Attacking AWS and Microsoft with &lsquo;TextBugger&rsquo; adversarial text attack framework:&hellip;Compromising text analysis systems with &lsquo;TextBugger&rsquo;&hellip;Researchers with the Institute of Cyberspace Research and College of Compute Science and Technology in Zheijiang University; the Alibaba-Zhejiang University Joint Research Institute of Frontier Technologies; the University of Illinois Urbana-Champaign; and Leheigh University, have published details on TextBugger &ldquo;a general attack framework for generating adversarial texts&rdquo;.&nbsp; Adversarial texts are chunks of text that have been manipulated in such a way that they don&rsquo;t set off alarms when automated classifiers look at them. For example, simply by altering the spelling and spacing of some words (eg, terrible becomes &lsquo;terrib1e&rsquo;, weak become &lsquo;wea k&rsquo;), the researchers have shown they can confused a deployed commercial classifier. Similarly, they show how you can change a chunk of text from being classified with 92% as being Toxic to 78% chance of non-toxix by changing the spelling of &lsquo;shit&rsquo; to &lsquo;shti&rsquo;, &lsquo;fucking&rsquo; to &lsquo;fuckimg&rsquo;, and &lsquo;hell&rsquo; to &lsquo;helled&rsquo;. &nbsp;&nbsp;Attacks against real systems: TextBugger can perform both white-box attacks (where the attacker has access to the underlying classification algorithm), and black-box attacks (where the precise inner details of a targeted system are now known). The researchers show that their approach works against deployed system, including: Google Cloud NLP, Microsoft Azure Text Analytics, IBM Watson Natural Language Understanding and Amazon AWS Comprehend. The researchers are able to use TextBugger to easily break Microsoft Azure and Amazon AWS NLP systems with a 100% success rate; by comparison, Google Cloud NLP holds up quite well, with them only able to get a 70.1% success rate against the system.&nbsp; To conduct the black box attacks, the researchers use the spaCy language processing framework to help them automatically identify the important words and sentences within a given chunk of text, which they then add adversarial examples to. &nbsp;&nbsp;Defending against adversarial examples: The researchers find that it&rsquo;s possible to better defend against adversarial examples by spellchecking submitted text and using this to identify adversarial examples. Additionally, they show that you can train models to automatically spot adversarial text, but this requires details of the attack.&nbsp; Why it matters: Now that companies around the world have deployed commercial and non-commercial AI systems at scale, it&rsquo;s logical that attackers will try to subvert them. As is the case with visual adversarial examples, today&rsquo;s neural network-based systems are quite vulnerable to subtle perturbations; we&rsquo;ll need to make systems more robust to deploy AI more widely with confidence. &nbsp;&nbsp;Read more: TextBugger: Generating Adversarial Text Against Real-world Applications (Arxiv).
Training AI systems to build AI systems by copying people:&hellip;Teaching AI to copy the good parts of human-designed systems, while still being creative&hellip;Researchers with Chinese computer vision giant SenseTime and the Chinese University of Hong Kong have published details on IRLAS, a technique to create AI agents that learn to design AI architectures inspired by human-designed networks.&nbsp; The technique, Inverse Reinforcement Learning for Architecture Search (IRLAS) works by training a neural network with reinforcement learning to design new networks based on a template derived from a human design. &ldquo;Given the architecture sampled by the agent as the self-generated demo…"

---

### Import AI 125: SenseTime trains AIs to imitate human AI architects; Berkeley researchers fuse AI for FrankenRL system; and fake images from NVIDIA cross the uncanny valley.

Berkeley researchers create Franken-RL, fusing hand-engineered systems and RL-based controllers:&hellip;Use hand-engineered controllers for the stuff they&rsquo;re good at, and use RL to learn the tricky things&hellip;Researchers with the University of California at Berkeley, Siemens Corporation, and the Hamburg University of Technology have combined classical robotics control techniques with reinforcement learning to create robots that can deal with complex tasks like block-stacking. &nbsp;&nbsp;The technique, which they call Residual Reinforcement Learning, uses &ldquo;conventional feedback control theory&rdquo; to learn to control the robot, and reinforcement learning to learn how to interact with the objects in the robot&rsquo;s world. The described technique mushes both of these techniques together. &ldquo;The key idea is to combine the flexibility of RL with the efficiency of conventional controllers by additively combining a learnable parametrized policy with a fixed hand-engineered controller&rdquo;, the researchers write.&nbsp; Testing on a real robot: The researchers show that residual RL approaches are more sample-efficient than those without it, with these traits verified in both simulation learning as well as in tests on a real robot. They also show that systems trained with Residual RL can better deal with confounding situations, like working out how to perform block assembly when the blocks have been moved into situations designed to confused the hand-written controller. &nbsp;&nbsp;Why it matters: Approaches like this show how today&rsquo;s contemporary AI techniques, like TD3 trained via RL as in the experiments here, can be combined with hand-written rule-based systems to create powerful AI applications. This is a trend that is likely to continue, and it suggests that the distinctions between systems which contain AI and which don&rsquo;t contain AI will become increasingly blurred.&nbsp; Read more: Residual Reinforcement Learning for Robot Control (Arxiv).
NVIDIA researchers show how fake image news is getting closer:&hellip;Synthetic faces roll through the uncanny valley, with a little help from GANs and the use of noise&hellip;Researchers with NVIDIA have shown how to use techniques cribbed from style transfer work on image generation, to create synthetic images of unparalleled quality. The research indicates that we&rsquo;re now at the point where neural networks are capable of generating single-frame synthetic images of a quality sufficient to trick (most) humans. While this paper does include a brief discussion of bias inherent to training images (good!) it does not at any point discuss what the policy implications are of systems capable of generating customizable fake human faces, which feels like a missed opportunity.&nbsp;&nbsp;How it works: &ldquo;Our generator starts from a learned constant input and adjusts the &ldquo;style&rdquo; of the image at each convolution layer based on the latent code, therefore directly controlling the strength of image features at different scales&rdquo;, the researchers explain. They also inject noise into the network at various different and find that the addition of noise helps create complex and coherent structures in subtle facial features like hair, earlobes, and so on. &ldquo;We hypothesize that at any point in the generator, there is pressure to introduce new content as soon as possible, and the easiest way for our network to create stochastic variation is to rely on the noise provided.&rdquo; &nbsp;&nbsp;Why it matters: These photorealistic faces are especially striking when we consider that ~4 years ago the best things AI systems were capable of was generating smeared, flattened, black&amp;white pixelated faces, as seen in the original generative adversarial networks paper (Arxiv). I wonder how long it will take us till we can generate coherent videos over lengthy time periods.&nbsp; Read more: A Style-Based Generator Architecture for Generative Adversarial Networks (Arxiv). &nbsp;&nbsp;Get more information and the data: NVIDIA has said it plans to release the source code, some pre-trained networks, and the FFHQ dataset &ldquo;soon&rdquo;. Get them from here (NVIDIA placeholding Google Doc).
Attacking AWS and Microsoft with &lsquo;TextBugger&rsquo; adversarial text attack framework:&hellip;Compromising text analysis systems with &lsquo;TextBugger&rsquo;&hellip;Researchers with the Institute of Cyberspace Research and College of Compute Science and Technology in Zheijiang University; the Alibaba-Zhejiang University Joint Research Institute of Frontier Technologies; the University of Illinois Urbana-Champaign; and Leheigh University, have published details on TextBugger &ldquo;a general attack framework for generating adversarial texts&rdquo;.&nbsp; Adversarial texts are chunks of text that have been manipulated in such a way that they don&rsquo;t set off alarms when automated classifiers look at them. For example, simply by altering the spelling and spacing of some words (eg, terrible becomes &lsquo;terrib1e&rsquo;, weak become &lsquo;wea k&rsquo;), the researchers have shown they can confused a deployed commercial classifier. Similarly, they show how you can change a chunk of text from being classified with 92% as being Toxic to 78% chance of non-toxix by changing the spelling of &lsquo;shit&rsquo; to &lsquo;shti&rsquo;, &lsquo;fucking&rsquo; to &lsquo;fuckimg&rsquo;, and &lsquo;hell&rsquo; to &lsquo;helled&rsquo;. &nbsp;&nbsp;Attacks against real systems: TextBugger can perform both white-box attacks (where the attacker has access to the underlying classification algorithm), and black-box attacks (where the precise inner details of a targeted system are now known). The researchers show that their approach works against deployed system, including: Google Cloud NLP, Microsoft Azure Text Analytics, IBM Watson Natural Language Understanding and Amazon AWS Comprehend. The researchers are able to use TextBugger to easily break Microsoft Azure and Amazon AWS NLP systems with a 100% success rate; by comparison, Google Cloud NLP holds up quite well, with them only able to get a 70.1% success rate against the system.&nbsp; To conduct the black box attacks, the researchers use the spaCy language processing framework to help them automatically identify the important words and sentences within a given chunk of text, which they then add adversarial examples to. &nbsp;&nbsp;Defending against adversarial examples: The researchers find that it&rsquo;s possible to better defend against adversarial examples by spellchecking submitted text and using this to identify adversarial examples. Additionally, they show that you can train models to automatically spot adversarial text, but this requires details of the attack.&nbsp; Why it matters: Now that companies around the world have deployed commercial and non-commercial AI systems at scale, it&rsquo;s logical that attackers will try to subvert them. As is the case with visual adversarial examples, today&rsquo;s neural network-based systems are quite vulnerable to subtle perturbations; we&rsquo;ll need to make systems more robust to deploy AI more widely with confidence. &nbsp;&nbsp;Read more: TextBugger: Generating Adversarial Text Against Real-world Applications (Arxiv).
Training AI systems to build AI systems by copying people:&hellip;Teaching AI to copy the good parts of human-designed systems, while still being creative&hellip;Researchers with Chinese computer vision giant SenseTime and the Chinese University of Hong Kong have published details on IRLAS, a technique to create AI agents that learn to design AI architectures inspired by human-designed networks.&nbsp; The technique, Inverse Reinforcement Learning for Architecture Search (IRLAS) works by training a neural network with reinforcement learning to design new networks based on a template derived from a human design. &ldquo;Given the architecture sampled by the agent as the self-generated demo…