---

layout: post
category: threads
title: "[D] Could Multi-Head Attention Transformer from “Attention is all you need” replace RNN/LSTM in other domain too?"
date: 2018-01-10 18:23:55
link: https://vrhk.co/2FmELSX
image: https://www.redditstatic.com/icon.png
domain: reddit.com
author: "reddit"
icon: http://www.redditstatic.com/desktop2x/img/favicon/apple-icon-57x57.png
excerpt: "My impression from reading is that Transformer block is capable to maintain hidden state memory like RNN. Is that mean we can use this to replace..."

---

### [D] Could Multi-Head Attention Transformer from “Attention is all you need” replace RNN/LSTM in other domain too? • r/MachineLearning

My impression from reading is that Transformer block is capable to maintain hidden state memory like RNN. Is that mean we can use this to replace...