---

layout: post
category: product
title: "Import AI: #75: Virtual Beijing with ParallelEye, NVIDIA tweaks GPU licensing, and saving money by getting AI to help humans label data generated by AI"
date: 2018-01-01 21:47:26
link: https://vrhk.co/2lEFPZ8
image: 
domain: jack-clark.net
author: "Jack Clark"
icon: https://s0.wp.com/i/webclip.png
excerpt: "Synthetic cities and what they mean: Virtual Beijing via ParallelEye:&hellip;As AI moves from the era of data competition to the era of environment competition researchers try to work out how best to harvest real-world data&hellip;Researchers with The State Key Laboratory for Management and Control of Complex Systems, within the Chinese Academy of Sciences in Beijing have published details on the &lsquo;ParallelEye&rsquo; dataset, a 3D urban environment modeled on Beijing&rsquo;s Zhongguancun region. They constructed the dataset by grabbing the available Open Street Map (OSM) layout data for a 2km*3km area, then modeled that data using CityEngine, and built the whole environment in the Unity3D engine.&nbsp; This seems like an involved, overly human-in-the-loop process, compared to other approaches at large-scale 3D environment design, like UofT/Uber&rsquo;s semi-autonomous data-augmentation techniques for creating a giant 3D map of Toronto, to the work being done by others on designing generators for procedural homesteads. However, it does provide a relatively labour-intensive pipeline for scraping and ingesting the world without needing expensive sensors and/or satellites. If enough datasets were constructed via this method it&rsquo;s likely you could use deep learning approaches to automate the pipeline, like the transforms from OSM maps into full 3D models into Unity. &nbsp;&nbsp;The researchers carry out some basic testing of ParallelEye by creating synthetic cameras meant to be like those mounted on self-driving cars or in large-scale surveillance systems, then testing usability around this. They leave the application of actual AI techniques to the dataset for future research.&ndash; Read more: The ParallelEye Dataset: Constructing Large-Scale Artificial Scenes for Traffic Vision Research (Arxiv).
Intel releases its 3D environment, the &lsquo;CARLA&rsquo; simulator:&hellip;It is said that any sufficiently large wealthy company is now keen to also own at least one bespoke AI simulator. Why? Good question!&hellip;Intel recently released code for CARLA, a 3D simulator for testing and evaluating AI systems, like those deployed in self-driving cars.&nbsp; &ldquo;CARLA is an open-source simulator for autonomous driving research. CARLA has been developed from the ground up to support development, training, and validation of autonomous urban driving systems,&rdquo; Intel writes. &nbsp;&nbsp;The AI environment world right now is reminiscent of the world of programming languages a few years ago, or the dynamic and large ecosystem of supercomputer vendors a few years before that; we&rsquo;re in an initial period of experimentation in which many ideas are going to be tried and it&rsquo;ll be a few years yet before things shake out and a clear winner emerges. The world of AI programming frameworks is going through its own Cambrian explosion right now, though is further on in the process than 3D environments, as developers appear to be consolidating around Tensorflow and pyTorch, while dabbling in a bunch of other (sometimes complementary) frameworks, eg Caffe/Torch/CNTK/Keras/MXNet.&nbsp; &nbsp;Question:&nbsp;Technical projects have emerged to help developers transfer models built in one framework into another, either via direct transfer mechanisms or through meta-abstractions like ONNX. What would be the equivalent for 3D environments beyond a set of resolution constraints and physics constants?&ndash; Read more: CARLA: An Open Urban Driving Simulator (PDF).&ndash; Read more: CARLA 0.7 release notes.
Google Photos: 1, Clarifai Forevery: 0&hellip;Competing on photo classification seems to be a risky proposition for startups in the era of mega cloud vendors&hellip;Image recognition startup Clarifai is shutting down its consumer-facing mobile app Forevery to focus instead on its own image recognition services and associated SDKs. This seems like a small bit of evidence for how large companies like Google or Apple can overwhelm startups by competing with them on products that tap into large-scale ML capabilities &ndash; something Google and Apple are reasonably well positioned to use, whereas smaller startups will struggle.&ndash; Read more: Goodbye Forevery. Hello Future. (Blog).
NVIDIA says NO to consumer graphics card in big datacenters:&hellip;NVIDIA tweaks licensing terms to discourage people from repurposing its cheaper cards for data center usage&hellip;For several years NVIDIA has been the undisputed king of AI compute, with developers turning to its cards en masse to train neural networks, primarily because of the company&rsquo;s significant support for scientific/AI computation via tools like CUDA/cuDNN, etc. &nbsp;&nbsp;During the last few years NVIDIA has courted these developers by producing more expensive cards designed for 24/7 data center operation, incorporating enterprise-grade features relating to reliability and error correction. This is to help NVIDIA charge higher prices to some of its larger-scale customers. But adoption of these cards has been relatively slight as many developers are instead filling vast data centers with&nbsp;the somewhat cheaper consumer cards and accepting a marginally higher failure rate in exchange for a better FLOPS/dollar ratio.&nbsp; So now NVIDIA has moved from courting these developers to seeking to force them to change their buying habits: the company confirmed last week to CNBC that it recently changed its user agreement to help it extract money from some of the larger users. &nbsp;&nbsp;&ldquo;We recently added a provision to our GeForce-specific EULA to discourage potential misuse of our GeForce and TITAN products in demanding, large-scale enterprise environments,&rdquo; the company said in a statement to CNBC. &ldquo;We recognize that researchers often adapt GeForce and TITAN products for non-commercial uses or other research uses that do not operate at data center scale. NVIDIA does not intend to prohibit such uses.&rdquo;&ndash; Read NVIDIA&rsquo;s statement in full in this CNBC article.The Titan(ic) Price premium: Even within NVIDIA&rsquo;s desktop card range there is a significant delta in performance among cards, even when factoring in dollars/flops, as this article comparing a 1080 versus a Titan V shows.&ndash; Read more: Titan V vs 1080 Ti&#8202;&mdash;&#8202;Head-to-head battle of the best desktop GPUs on CNNs. Is Titan V worth it? 110 TFLOPS! no brainer, right?
A &lsquo;Ray&rsquo; of training light emerges from Berkeley:&hellip;Systems geeks reframe reinforcement learning programming for better performance, scaling&hellip;Berkeley researchers have released Ray RLLib, software to make it easier to set up and run reinforcement learning experiments. The researchers, which include the creator of the &lsquo;Spark&rsquo; data processing engine, say that reinforcement learning algorithms are somewhat more complicated than typical AI models (mostly classification models trained via supervised learning) and that this means there&rsquo;s some value in designing a framework that optimizes the basic components commonly used in RL. &ldquo;RLlib proposes using a task-based programming model to let each component control its own resources and degree of parallelism, enabling the easy composition and reuse of components,&rdquo; they write.&nbsp; &ldquo;Unlike typical operators in deep learning frameworks, individual components may require parallelism across a cluster, use a neural network defined by a deep learning framework, recursively issue calls to other components, or interface with black-box third-party simulators,&rdquo; they write. &ldquo;Meanwhile, the main algorithms that connect these components are rapidly evolving and expose opportunities for parallelism at varying levels. Finally, RL algorithms manipulate substantial amounts of state (e.g., replay buffers and model parameters) that must be managed across multiple levels of parallelism and different physical devices.&rdquo; &nbsp;&nbsp;The research…"

---

### Import AI: #75: Virtual Beijing with ParallelEye, NVIDIA tweaks GPU licensing, and saving money by getting AI to help humans label data generated by AI

Synthetic cities and what they mean: Virtual Beijing via ParallelEye:&hellip;As AI moves from the era of data competition to the era of environment competition researchers try to work out how best to harvest real-world data&hellip;Researchers with The State Key Laboratory for Management and Control of Complex Systems, within the Chinese Academy of Sciences in Beijing have published details on the &lsquo;ParallelEye&rsquo; dataset, a 3D urban environment modeled on Beijing&rsquo;s Zhongguancun region. They constructed the dataset by grabbing the available Open Street Map (OSM) layout data for a 2km*3km area, then modeled that data using CityEngine, and built the whole environment in the Unity3D engine.&nbsp; This seems like an involved, overly human-in-the-loop process, compared to other approaches at large-scale 3D environment design, like UofT/Uber&rsquo;s semi-autonomous data-augmentation techniques for creating a giant 3D map of Toronto, to the work being done by others on designing generators for procedural homesteads. However, it does provide a relatively labour-intensive pipeline for scraping and ingesting the world without needing expensive sensors and/or satellites. If enough datasets were constructed via this method it&rsquo;s likely you could use deep learning approaches to automate the pipeline, like the transforms from OSM maps into full 3D models into Unity. &nbsp;&nbsp;The researchers carry out some basic testing of ParallelEye by creating synthetic cameras meant to be like those mounted on self-driving cars or in large-scale surveillance systems, then testing usability around this. They leave the application of actual AI techniques to the dataset for future research.&ndash; Read more: The ParallelEye Dataset: Constructing Large-Scale Artificial Scenes for Traffic Vision Research (Arxiv).
Intel releases its 3D environment, the &lsquo;CARLA&rsquo; simulator:&hellip;It is said that any sufficiently large wealthy company is now keen to also own at least one bespoke AI simulator. Why? Good question!&hellip;Intel recently released code for CARLA, a 3D simulator for testing and evaluating AI systems, like those deployed in self-driving cars.&nbsp; &ldquo;CARLA is an open-source simulator for autonomous driving research. CARLA has been developed from the ground up to support development, training, and validation of autonomous urban driving systems,&rdquo; Intel writes. &nbsp;&nbsp;The AI environment world right now is reminiscent of the world of programming languages a few years ago, or the dynamic and large ecosystem of supercomputer vendors a few years before that; we&rsquo;re in an initial period of experimentation in which many ideas are going to be tried and it&rsquo;ll be a few years yet before things shake out and a clear winner emerges. The world of AI programming frameworks is going through its own Cambrian explosion right now, though is further on in the process than 3D environments, as developers appear to be consolidating around Tensorflow and pyTorch, while dabbling in a bunch of other (sometimes complementary) frameworks, eg Caffe/Torch/CNTK/Keras/MXNet.&nbsp; &nbsp;Question:&nbsp;Technical projects have emerged to help developers transfer models built in one framework into another, either via direct transfer mechanisms or through meta-abstractions like ONNX. What would be the equivalent for 3D environments beyond a set of resolution constraints and physics constants?&ndash; Read more: CARLA: An Open Urban Driving Simulator (PDF).&ndash; Read more: CARLA 0.7 release notes.
Google Photos: 1, Clarifai Forevery: 0&hellip;Competing on photo classification seems to be a risky proposition for startups in the era of mega cloud vendors&hellip;Image recognition startup Clarifai is shutting down its consumer-facing mobile app Forevery to focus instead on its own image recognition services and associated SDKs. This seems like a small bit of evidence for how large companies like Google or Apple can overwhelm startups by competing with them on products that tap into large-scale ML capabilities &ndash; something Google and Apple are reasonably well positioned to use, whereas smaller startups will struggle.&ndash; Read more: Goodbye Forevery. Hello Future. (Blog).
NVIDIA says NO to consumer graphics card in big datacenters:&hellip;NVIDIA tweaks licensing terms to discourage people from repurposing its cheaper cards for data center usage&hellip;For several years NVIDIA has been the undisputed king of AI compute, with developers turning to its cards en masse to train neural networks, primarily because of the company&rsquo;s significant support for scientific/AI computation via tools like CUDA/cuDNN, etc. &nbsp;&nbsp;During the last few years NVIDIA has courted these developers by producing more expensive cards designed for 24/7 data center operation, incorporating enterprise-grade features relating to reliability and error correction. This is to help NVIDIA charge higher prices to some of its larger-scale customers. But adoption of these cards has been relatively slight as many developers are instead filling vast data centers with&nbsp;the somewhat cheaper consumer cards and accepting a marginally higher failure rate in exchange for a better FLOPS/dollar ratio.&nbsp; So now NVIDIA has moved from courting these developers to seeking to force them to change their buying habits: the company confirmed last week to CNBC that it recently changed its user agreement to help it extract money from some of the larger users. &nbsp;&nbsp;&ldquo;We recently added a provision to our GeForce-specific EULA to discourage potential misuse of our GeForce and TITAN products in demanding, large-scale enterprise environments,&rdquo; the company said in a statement to CNBC. &ldquo;We recognize that researchers often adapt GeForce and TITAN products for non-commercial uses or other research uses that do not operate at data center scale. NVIDIA does not intend to prohibit such uses.&rdquo;&ndash; Read NVIDIA&rsquo;s statement in full in this CNBC article.The Titan(ic) Price premium: Even within NVIDIA&rsquo;s desktop card range there is a significant delta in performance among cards, even when factoring in dollars/flops, as this article comparing a 1080 versus a Titan V shows.&ndash; Read more: Titan V vs 1080 Ti&#8202;&mdash;&#8202;Head-to-head battle of the best desktop GPUs on CNNs. Is Titan V worth it? 110 TFLOPS! no brainer, right?
A &lsquo;Ray&rsquo; of training light emerges from Berkeley:&hellip;Systems geeks reframe reinforcement learning programming for better performance, scaling&hellip;Berkeley researchers have released Ray RLLib, software to make it easier to set up and run reinforcement learning experiments. The researchers, which include the creator of the &lsquo;Spark&rsquo; data processing engine, say that reinforcement learning algorithms are somewhat more complicated than typical AI models (mostly classification models trained via supervised learning) and that this means there&rsquo;s some value in designing a framework that optimizes the basic components commonly used in RL. &ldquo;RLlib proposes using a task-based programming model to let each component control its own resources and degree of parallelism, enabling the easy composition and reuse of components,&rdquo; they write.&nbsp; &ldquo;Unlike typical operators in deep learning frameworks, individual components may require parallelism across a cluster, use a neural network defined by a deep learning framework, recursively issue calls to other components, or interface with black-box third-party simulators,&rdquo; they write. &ldquo;Meanwhile, the main algorithms that connect these components are rapidly evolving and expose opportunities for parallelism at varying levels. Finally, RL algorithms manipulate substantial amounts of state (e.g., replay buffers and model parameters) that must be managed across multiple levels of parallelism and different physical devices.&rdquo; &nbsp;&nbsp;The research…