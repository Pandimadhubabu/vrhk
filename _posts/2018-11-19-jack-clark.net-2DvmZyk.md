---

layout: post
category: product
title: "Import AI 121: Sony researchers make ultra-fast ImageNet training breakthrough; Berkeley researchers tackle StarCraft II with modular RL system; and Germany adds €3bn for AI research"
date: 2018-11-19 16:56:51
link: https://vrhk.co/2DvmZyk
image: 
domain: jack-clark.net
author: "Jack Clark"
icon: https://s2.wp.com/i/webclip.png
excerpt: "Berkeley researchers take on StarCraft II with modular RL system:&hellip;Self play + modular structure makes challenging game tractable&hellip;Researchers with the University of California at Berkeley have shown how to use self-play to have AI agents learn to play real-time strategy game StarCraft II. &ldquo;We propose a flexible modular architecture that shares the decision responsibilities among multiple independent modules, including worker management, build order, tactics, micromanagement, and scouting&rdquo;, the researchers write. &ldquo;We adopt an iterative training approach that first trains one module while others follow very simple scripted behaviors, and then replace the scripted component of another module with a neural network policy, which continues to train while the previously trained modules remain fixed&rdquo;.&nbsp; Results: The resulting system can comfortably beat the Easy and Medium in-game AI systems, but struggles against more difficult in-built bots; the best AI systems discussed in the paper use a combination of learned tactics and learned build orders to obtain win rates of around 30% when playing against the game&rsquo;s in-built &lsquo;Elite&rsquo; difficulty AI agents.&nbsp; Transfer learning: The researchers also try to test how general their various learned modules are by trying their agent out against competitors in different maps from the map on which it was trained. The agent&rsquo;s performance drops a bit, but only by a few percentage points. &ldquo;Though our agent&rsquo;s win rates drop by 7.5% on average against Harder, it is still very competitive,&rdquo; they write.&nbsp; What is next: &ldquo;Many improvements are under research, including deeper neural networks, multi-army-group tactics, researching upgrades, and learned micromanagement policies. We believe that such improvements can eventually close the gap between our modular agent and professional human players&rdquo;.Why it matters: Approaches like those outlined in this paper suggest that contemporary reinforcement learning techniques are tractable when applied against StarCraft II, and the somewhat complex modular system used by these researchers suggests that a simple system that obtained high performance would be an indication of algorithmic advancement.&nbsp; Read more: Modular Architecture for StarCraft II with Deep Reinforcement Learning (Arxiv).
For better AI safety, learn about worms and fruitflies:&hellip;New position paper argues for fusion of biological agents and AI safety research&hellip;Researchers with Emory University, Northwestern University, and AI startup Vicarious AI, have proposed bringing the world&rsquo;s of biology and AI development together to create safer and more robust systems. The idea, laid out in a discussion paper, is that researchers should aim to simulate different AI safety challenges on biological platforms modelled on the real world, and should use insights from this as well as neuropsychology and comparative neuroanatomy to guide research.&nbsp; The humbling sophistication of insects: The paper also includes some numbers that highlight just how impressive even simple creatures are, especially when compared to AI systems. &ldquo;elegans, with only 302 neurons, shows simple behavior of learning and memory. Drosophila melanogaster, despite only having 10^5 neurons and no comparable structure to a cerebral cortex, has sophisticated spatial navigation abilities easily rivaling the best autonomous vehicles with a minuscule fraction of the power consumption&rdquo;. (By contrast, a brown rat has around 10^8 neurons, and a human has around 10^10).&nbsp; Human values, where do they come from? One motivation for building AI systems that take a greater inspiration from biology is that biology may hold significant sway over our own moral values, say the researchers &ndash; perhaps human values are correlated with the internal reward systems people have in their own brains, which are themselves conditioned by the embodied context in which people evolved? Understanding how values are or aren&rsquo;t related to biological context may help researchers design safer AI systems, they say.&nbsp; Why it matters: Speculative as it is, it&rsquo;s encouraging to see researchers think about some of the tougher long-term challenges of making powerful AI systems safe. Though it does seem likely that for now most AI organizations will evaluate agents on typical (aka, not hugely biologically-accurate) substrates, I do wonder if we&rsquo;ll experiment with more organic-style systems in the future. If we do, perhaps we&rsquo;ll return to this paper then. &ldquo;Understanding how to translate the highly simplified models of current AI safety frameworks to the complex neural networks of real organisms in realistic physical environments will be a substantial undertaking&rdquo;, the researchers write.&nbsp; Read more: Integrative Biological Simulation, Neuropsychology, and AI Safety (Arxiv).
Sony researchers claim ImageNet training breakthrough:&hellip;The industrialization of AI continues&hellip;In military circles there&rsquo;s a concept called the OODA loop (Observe, Orient, Decide, Act). The goal of any effective military organization is to have an OODA loop that is faster than their competitors, as a faster, tighter OODA loop corresponds to a greater ability to process data and take appropriate actions.&nbsp; What might contribute to an OODA-style loop for an AI development organization? I think one key ingredient is the speed with which researchers can validate ideas on large-scale datasets. That&rsquo;s because while many AI techniques show promise on small-scale datasets, many techniques fail to show success when tested on significantly larger domains, eg, going from testing a reinforcement learning approach on Atari to on a far harder domain such as Go or Dota 2, or going from testing a new supervised classification method on MNIST to going to ImageNet. Therefore, being able to rapidly validate ideas against big datasets helps researchers identify fruitful, scalable techniques to pursue.&nbsp; Fast ImageNet training: Measuring the time it takes to train an ImageNet model to reasonable accuracy is a good way to assess how rapidly AI is industrializing, as the faster people are able to train these models, the faster they&rsquo;re able to validate ideas on flexible research infrastructure. The nice thing about ImageNet is that it&rsquo;s a good proxy for the ability of an org to more rapidly iterate on tests of supervised learning systems, so progress here maps quite nicely to the ability for self-driving car companies to train and test new large-scale image-based perception systems. New research from Sony Corporation gives us an idea of exactly what it takes to industrialize AI and gives an indication of how much work is needed to properly scale-up AI training infrastructure systems.&nbsp; 224 seconds: The Sony system can train a ResNet-50 on ImageNet to an accuracy of approximately 75% within 224 seconds (~4 minutes). That&rsquo;s down from one hour in mid-2017, and around 29 hours in late 2015.&nbsp; All the tricks, including the kitchen sink: The researchers attribute two main techniques to their score, which should be familiar to practitioners already industrializing AI systems within their own companies &ndash; the use of very large batch sizes (which basically means you process bigger chunks of data with each step of your deep learning system), as well as a clever 2D-Torus All-reduce interface (which is basically a system to speed up the movement of data around the training system to efficiently consume the capacity of available GPUs).GPU Scaling: As with all things, scaling GPUs appears to have a law of diminishing returns &ndash; the Sony researchers note that they&rsquo;re able to achieve a maximum GPU utilization efficiency of 66.67% across 2720 GPUs, which decreases to 52.47% once you get up to 3264 GPUs.&nbsp; Why it matters: Me…"

---

### Import AI 121: Sony researchers make ultra-fast ImageNet training breakthrough; Berkeley researchers tackle StarCraft II with modular RL system; and Germany adds €3bn for AI research

Berkeley researchers take on StarCraft II with modular RL system:&hellip;Self play + modular structure makes challenging game tractable&hellip;Researchers with the University of California at Berkeley have shown how to use self-play to have AI agents learn to play real-time strategy game StarCraft II. &ldquo;We propose a flexible modular architecture that shares the decision responsibilities among multiple independent modules, including worker management, build order, tactics, micromanagement, and scouting&rdquo;, the researchers write. &ldquo;We adopt an iterative training approach that first trains one module while others follow very simple scripted behaviors, and then replace the scripted component of another module with a neural network policy, which continues to train while the previously trained modules remain fixed&rdquo;.&nbsp; Results: The resulting system can comfortably beat the Easy and Medium in-game AI systems, but struggles against more difficult in-built bots; the best AI systems discussed in the paper use a combination of learned tactics and learned build orders to obtain win rates of around 30% when playing against the game&rsquo;s in-built &lsquo;Elite&rsquo; difficulty AI agents.&nbsp; Transfer learning: The researchers also try to test how general their various learned modules are by trying their agent out against competitors in different maps from the map on which it was trained. The agent&rsquo;s performance drops a bit, but only by a few percentage points. &ldquo;Though our agent&rsquo;s win rates drop by 7.5% on average against Harder, it is still very competitive,&rdquo; they write.&nbsp; What is next: &ldquo;Many improvements are under research, including deeper neural networks, multi-army-group tactics, researching upgrades, and learned micromanagement policies. We believe that such improvements can eventually close the gap between our modular agent and professional human players&rdquo;.Why it matters: Approaches like those outlined in this paper suggest that contemporary reinforcement learning techniques are tractable when applied against StarCraft II, and the somewhat complex modular system used by these researchers suggests that a simple system that obtained high performance would be an indication of algorithmic advancement.&nbsp; Read more: Modular Architecture for StarCraft II with Deep Reinforcement Learning (Arxiv).
For better AI safety, learn about worms and fruitflies:&hellip;New position paper argues for fusion of biological agents and AI safety research&hellip;Researchers with Emory University, Northwestern University, and AI startup Vicarious AI, have proposed bringing the world&rsquo;s of biology and AI development together to create safer and more robust systems. The idea, laid out in a discussion paper, is that researchers should aim to simulate different AI safety challenges on biological platforms modelled on the real world, and should use insights from this as well as neuropsychology and comparative neuroanatomy to guide research.&nbsp; The humbling sophistication of insects: The paper also includes some numbers that highlight just how impressive even simple creatures are, especially when compared to AI systems. &ldquo;elegans, with only 302 neurons, shows simple behavior of learning and memory. Drosophila melanogaster, despite only having 10^5 neurons and no comparable structure to a cerebral cortex, has sophisticated spatial navigation abilities easily rivaling the best autonomous vehicles with a minuscule fraction of the power consumption&rdquo;. (By contrast, a brown rat has around 10^8 neurons, and a human has around 10^10).&nbsp; Human values, where do they come from? One motivation for building AI systems that take a greater inspiration from biology is that biology may hold significant sway over our own moral values, say the researchers &ndash; perhaps human values are correlated with the internal reward systems people have in their own brains, which are themselves conditioned by the embodied context in which people evolved? Understanding how values are or aren&rsquo;t related to biological context may help researchers design safer AI systems, they say.&nbsp; Why it matters: Speculative as it is, it&rsquo;s encouraging to see researchers think about some of the tougher long-term challenges of making powerful AI systems safe. Though it does seem likely that for now most AI organizations will evaluate agents on typical (aka, not hugely biologically-accurate) substrates, I do wonder if we&rsquo;ll experiment with more organic-style systems in the future. If we do, perhaps we&rsquo;ll return to this paper then. &ldquo;Understanding how to translate the highly simplified models of current AI safety frameworks to the complex neural networks of real organisms in realistic physical environments will be a substantial undertaking&rdquo;, the researchers write.&nbsp; Read more: Integrative Biological Simulation, Neuropsychology, and AI Safety (Arxiv).
Sony researchers claim ImageNet training breakthrough:&hellip;The industrialization of AI continues&hellip;In military circles there&rsquo;s a concept called the OODA loop (Observe, Orient, Decide, Act). The goal of any effective military organization is to have an OODA loop that is faster than their competitors, as a faster, tighter OODA loop corresponds to a greater ability to process data and take appropriate actions.&nbsp; What might contribute to an OODA-style loop for an AI development organization? I think one key ingredient is the speed with which researchers can validate ideas on large-scale datasets. That&rsquo;s because while many AI techniques show promise on small-scale datasets, many techniques fail to show success when tested on significantly larger domains, eg, going from testing a reinforcement learning approach on Atari to on a far harder domain such as Go or Dota 2, or going from testing a new supervised classification method on MNIST to going to ImageNet. Therefore, being able to rapidly validate ideas against big datasets helps researchers identify fruitful, scalable techniques to pursue.&nbsp; Fast ImageNet training: Measuring the time it takes to train an ImageNet model to reasonable accuracy is a good way to assess how rapidly AI is industrializing, as the faster people are able to train these models, the faster they&rsquo;re able to validate ideas on flexible research infrastructure. The nice thing about ImageNet is that it&rsquo;s a good proxy for the ability of an org to more rapidly iterate on tests of supervised learning systems, so progress here maps quite nicely to the ability for self-driving car companies to train and test new large-scale image-based perception systems. New research from Sony Corporation gives us an idea of exactly what it takes to industrialize AI and gives an indication of how much work is needed to properly scale-up AI training infrastructure systems.&nbsp; 224 seconds: The Sony system can train a ResNet-50 on ImageNet to an accuracy of approximately 75% within 224 seconds (~4 minutes). That&rsquo;s down from one hour in mid-2017, and around 29 hours in late 2015.&nbsp; All the tricks, including the kitchen sink: The researchers attribute two main techniques to their score, which should be familiar to practitioners already industrializing AI systems within their own companies &ndash; the use of very large batch sizes (which basically means you process bigger chunks of data with each step of your deep learning system), as well as a clever 2D-Torus All-reduce interface (which is basically a system to speed up the movement of data around the training system to efficiently consume the capacity of available GPUs).GPU Scaling: As with all things, scaling GPUs appears to have a law of diminishing returns &ndash; the Sony researchers note that they&rsquo;re able to achieve a maximum GPU utilization efficiency of 66.67% across 2720 GPUs, which decreases to 52.47% once you get up to 3264 GPUs.&nbsp; Why it matters: Me…