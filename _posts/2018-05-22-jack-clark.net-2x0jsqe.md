---

layout: post
category: product
title: "Import AI: #95: Learning to predict and avoid internet arguments with deep learning, White House announces Select Committee on AI, and BMW trains cars to safely change lanes"
date: 2018-05-22 04:11:56
link: https://vrhk.co/2x0jsqe
image: 
domain: jack-clark.net
author: "Jack Clark"
icon: https://s2.wp.com/i/webclip.png
excerpt: "Cornell, Google, and Wikimedia researchers train AI to predict when we&rsquo;ll get angry on the internet:&hellip;Spoiler: Very blunt comments with little attempt made at being polite tend to lead to aggressive conversations&hellip;Have you ever read a comment addressed to you on the internet and decided not to reply because your intuition tells you the person is looking to start a fight? Is it possible to train AI systems to have a similar predictive ability and thereby create automated systems that can flag conversations as having a likelihood of spiraling into aggression? That&rsquo;s the idea behind new research from Cornell University, Jigsaw, and the Wikimedia Foundation. The research tries to predict troublesome conversations based on a dataset taken from the discussion sections of &lsquo;Wikipedia Talk&rsquo; pages. &nbsp;&nbsp;Dataset: To carry out the experiment, the researchers gathered a total of 1,270 conversations, half consisting of ones which became aggressive following the initial comments, and half consisting of ones which remained civil. (Categorizing civil versus on-track was done via a combination of the use of Jigsaw&rsquo;s &ldquo;Perspective&rdquo; API, and gathering labels from humans via CrowdFlower.) These conversations had an average length of 4.6 comments. &nbsp;&nbsp;How it works: Armed with this dataset, the researchers characterized conversations via what they call &ldquo;pragmatic devices signalling politeness&rdquo;. This is a set of features that correspond to whether the conversation includes attempts to be friendly (liberal use of &lsquo;thanks&rsquo;, &lsquo;please&rsquo;, and so on), along with words used to indicate a position that welcomes debate (eg, by clarifying statements with phrases like &ldquo;I believe&rdquo; or &ldquo;I think&rdquo;). They then study the initial comment and see if their system can learn to predict whether it will yield negative comments in the future.&nbsp; Results: Humans are successful about 72% of the time at predicting nasty conversations from this dataset. The system designed by these researchers (which relies on logistic regression &ndash; nothing too fancy) is about 61.6% accurate, and baselines (bag of words and sentiment lexicon) get around ~56%. (One variant of the proposed technique gets accuracy of 64.9%, but this is a little dubious as it is trained on way more data and it&rsquo;s unclear whether it is overfitting, as it is also trained on the same data corpus.) The researchers also derive some statistical correlations that could help humans as well as machines better spot comments that are prone to spiral into aggresion. &ldquo;We find a rough correspondence between linguistic directness and the likelihood of future personal attacks. In particular, comments which contain direct questions, or exhibit sentence initial you (i.e., &ldquo;2nd person start&rdquo;), tend to start awry-turning conversations significantly more often than ones that stay on track,&rdquo; they write. &ldquo;This effect coheres with our intuition that directness signals some latent hostility from the conversation&rsquo;s initiator, and perhaps reinforces the forcefulness of contentious impositions.&rdquo;&nbsp; Why it matters: Systems like this show how with a relatively small amount of data it is possible to build classification systems that can, if paired with the right features, effectively categorize subtle human interactions online. While here such a system is used to do something that seems to be for the purpose of social good (figuring out how to identify and potentially avoid aggressive conversations), it&rsquo;s worth remembering that a very similar approach could be used to, for instance, identify conversations where initial comments could correlate to conversations that have a high chance of displaying political views that are contrary to those views of the people building such systems, and so on. It would be nice to see an acknowledgement of this in the paper itself.&nbsp; Read more: Conversations Gone Awry: Detecting Early Signs of Conversational Failure (Arxiv).
Chinese researchers tackle Dota-like game King of Glory with RL + MCTS:&hellip;Tencent researchers take inspiration from AlphaGo Zero to tackle Chinese MOBA King of Glory&hellip;Modern multiplayer strategy games are becoming a testbed for reinforcement learning and multi-agent algorithms. Following work by Facebook and DeepMind on StarCraft 1 and 2, and work by OpenAI on Dota, researchers with the University of Pittsburgh and Tencent AI Lab have published details on an AI technique which they evaluate on King of Glory, a Tencent-made massively multiplayer online battle arena (MOBA) game. The proposed system uses Monte Carlo Tree Search (MCTS &ndash; a technique also crucial to DeepMind&rsquo;s work on tackling the board game Go) and incorporates techniques from AlphaGo Zero to &ldquo;to produce a stronger tree search using previous tree results&rdquo;. &ldquo;Our proposed algorithm is a provably near-optimal variant (and in some respects, generalization) of the AlphaGo Zero algorithm&rdquo; they write.&nbsp; Results: The researchers test out their technique within King of Glory by evaluating agents trained with their technique against other agents controlled by the in-game AI. They also test it against four variants of their proposed technique which, respectively: have no rollouts; use direct policy iteration; implement approximate value iteration; and one trained via supervised learning on 100,000 state-action pairs of human gameplay data. (This also functions as a basic ablation study of the proposed technique, also). Their system beats all of these approaches, with the closest competitor being the variant with no rollouts (this one also looks most similar to AlphaGo Zero). &nbsp;&nbsp;Things that make you go hmmm: Researchers still tend to attack problems like this by training the AI systems over a multitude of hand-selected features, so it&rsquo;s not like these algorithms are automatically inferring optimal inputs from which to learn from. &ldquo;The state variable of the system is taken to be a 41-dimensional vector containing information obtained directly from the game engine, including hero locations, hero health, minion health, hero skill state, and relative locations to various structures,&rdquo; they write. A lot of human ingenuity goes into selecting these inputs and likely adjusting hyperparameters to denote the importance of any particular input, so there&rsquo;s a significant unacknowledged human component to this work. &nbsp;&nbsp;Why it matters: This paper provides more evidence that AI researchers are going to use increasingly modern, sophisticated games to test and evaluate AI systems. It&rsquo;s also quite interesting that this work comes from a Chinese AI lab, indicating that these research organizations are pursuing similarly large-scale problems to some labs in the West &ndash; there&rsquo;s more commonality here than I think people presume, and it&rsquo;d be interesting to see the various researchers come together and discuss ideas in the future about how to tackle even more advanced games.&nbsp; Read more: Feedback-Based Tree Search for Reinforcement Learning (Arxiv).
Today&rsquo;s AI amounts to little more than curve-fitting, says Turing Award winner:&hellip;Judea Pearl is impressed by deep learning success, but worries researchers have become complacent about inability to deal with causality&hellip;Turing Award-winner Judea Pearl is concerned that the AI industry&rsquo;s current obsession with deep learning is causing it to ignore harder problems, like developing machines that can build causal models of the world. He discusses some of these concerns in an interview with Quanta Magazine to discuss his new book &ldquo;The Book of Why: The New Science of Cause and Effect&ldquo;.&nbsp; Selected quotes:&ndash; &ldquo;Mathematics has not developed the asymmetric language required to capture our understandi…"

---

### Import AI: #95: Learning to predict and avoid internet arguments with deep learning, White House announces Select Committee on AI, and BMW trains cars to safely change lanes

Cornell, Google, and Wikimedia researchers train AI to predict when we&rsquo;ll get angry on the internet:&hellip;Spoiler: Very blunt comments with little attempt made at being polite tend to lead to aggressive conversations&hellip;Have you ever read a comment addressed to you on the internet and decided not to reply because your intuition tells you the person is looking to start a fight? Is it possible to train AI systems to have a similar predictive ability and thereby create automated systems that can flag conversations as having a likelihood of spiraling into aggression? That&rsquo;s the idea behind new research from Cornell University, Jigsaw, and the Wikimedia Foundation. The research tries to predict troublesome conversations based on a dataset taken from the discussion sections of &lsquo;Wikipedia Talk&rsquo; pages. &nbsp;&nbsp;Dataset: To carry out the experiment, the researchers gathered a total of 1,270 conversations, half consisting of ones which became aggressive following the initial comments, and half consisting of ones which remained civil. (Categorizing civil versus on-track was done via a combination of the use of Jigsaw&rsquo;s &ldquo;Perspective&rdquo; API, and gathering labels from humans via CrowdFlower.) These conversations had an average length of 4.6 comments. &nbsp;&nbsp;How it works: Armed with this dataset, the researchers characterized conversations via what they call &ldquo;pragmatic devices signalling politeness&rdquo;. This is a set of features that correspond to whether the conversation includes attempts to be friendly (liberal use of &lsquo;thanks&rsquo;, &lsquo;please&rsquo;, and so on), along with words used to indicate a position that welcomes debate (eg, by clarifying statements with phrases like &ldquo;I believe&rdquo; or &ldquo;I think&rdquo;). They then study the initial comment and see if their system can learn to predict whether it will yield negative comments in the future.&nbsp; Results: Humans are successful about 72% of the time at predicting nasty conversations from this dataset. The system designed by these researchers (which relies on logistic regression &ndash; nothing too fancy) is about 61.6% accurate, and baselines (bag of words and sentiment lexicon) get around ~56%. (One variant of the proposed technique gets accuracy of 64.9%, but this is a little dubious as it is trained on way more data and it&rsquo;s unclear whether it is overfitting, as it is also trained on the same data corpus.) The researchers also derive some statistical correlations that could help humans as well as machines better spot comments that are prone to spiral into aggresion. &ldquo;We find a rough correspondence between linguistic directness and the likelihood of future personal attacks. In particular, comments which contain direct questions, or exhibit sentence initial you (i.e., &ldquo;2nd person start&rdquo;), tend to start awry-turning conversations significantly more often than ones that stay on track,&rdquo; they write. &ldquo;This effect coheres with our intuition that directness signals some latent hostility from the conversation&rsquo;s initiator, and perhaps reinforces the forcefulness of contentious impositions.&rdquo;&nbsp; Why it matters: Systems like this show how with a relatively small amount of data it is possible to build classification systems that can, if paired with the right features, effectively categorize subtle human interactions online. While here such a system is used to do something that seems to be for the purpose of social good (figuring out how to identify and potentially avoid aggressive conversations), it&rsquo;s worth remembering that a very similar approach could be used to, for instance, identify conversations where initial comments could correlate to conversations that have a high chance of displaying political views that are contrary to those views of the people building such systems, and so on. It would be nice to see an acknowledgement of this in the paper itself.&nbsp; Read more: Conversations Gone Awry: Detecting Early Signs of Conversational Failure (Arxiv).
Chinese researchers tackle Dota-like game King of Glory with RL + MCTS:&hellip;Tencent researchers take inspiration from AlphaGo Zero to tackle Chinese MOBA King of Glory&hellip;Modern multiplayer strategy games are becoming a testbed for reinforcement learning and multi-agent algorithms. Following work by Facebook and DeepMind on StarCraft 1 and 2, and work by OpenAI on Dota, researchers with the University of Pittsburgh and Tencent AI Lab have published details on an AI technique which they evaluate on King of Glory, a Tencent-made massively multiplayer online battle arena (MOBA) game. The proposed system uses Monte Carlo Tree Search (MCTS &ndash; a technique also crucial to DeepMind&rsquo;s work on tackling the board game Go) and incorporates techniques from AlphaGo Zero to &ldquo;to produce a stronger tree search using previous tree results&rdquo;. &ldquo;Our proposed algorithm is a provably near-optimal variant (and in some respects, generalization) of the AlphaGo Zero algorithm&rdquo; they write.&nbsp; Results: The researchers test out their technique within King of Glory by evaluating agents trained with their technique against other agents controlled by the in-game AI. They also test it against four variants of their proposed technique which, respectively: have no rollouts; use direct policy iteration; implement approximate value iteration; and one trained via supervised learning on 100,000 state-action pairs of human gameplay data. (This also functions as a basic ablation study of the proposed technique, also). Their system beats all of these approaches, with the closest competitor being the variant with no rollouts (this one also looks most similar to AlphaGo Zero). &nbsp;&nbsp;Things that make you go hmmm: Researchers still tend to attack problems like this by training the AI systems over a multitude of hand-selected features, so it&rsquo;s not like these algorithms are automatically inferring optimal inputs from which to learn from. &ldquo;The state variable of the system is taken to be a 41-dimensional vector containing information obtained directly from the game engine, including hero locations, hero health, minion health, hero skill state, and relative locations to various structures,&rdquo; they write. A lot of human ingenuity goes into selecting these inputs and likely adjusting hyperparameters to denote the importance of any particular input, so there&rsquo;s a significant unacknowledged human component to this work. &nbsp;&nbsp;Why it matters: This paper provides more evidence that AI researchers are going to use increasingly modern, sophisticated games to test and evaluate AI systems. It&rsquo;s also quite interesting that this work comes from a Chinese AI lab, indicating that these research organizations are pursuing similarly large-scale problems to some labs in the West &ndash; there&rsquo;s more commonality here than I think people presume, and it&rsquo;d be interesting to see the various researchers come together and discuss ideas in the future about how to tackle even more advanced games.&nbsp; Read more: Feedback-Based Tree Search for Reinforcement Learning (Arxiv).
Today&rsquo;s AI amounts to little more than curve-fitting, says Turing Award winner:&hellip;Judea Pearl is impressed by deep learning success, but worries researchers have become complacent about inability to deal with causality&hellip;Turing Award-winner Judea Pearl is concerned that the AI industry&rsquo;s current obsession with deep learning is causing it to ignore harder problems, like developing machines that can build causal models of the world. He discusses some of these concerns in an interview with Quanta Magazine to discuss his new book &ldquo;The Book of Why: The New Science of Cause and Effect&ldquo;.&nbsp; Selected quotes:&ndash; &ldquo;Mathematics has not developed the asymmetric language required to capture our understandi…