---

layout: post
category: product
title: "Import AI: #78: Google gives away free K80 GPUs; Chinese researchers benchmark thermal imagery pedestrian trackers; and AI triumphs against dermatologists in diagnosis competition"
date: 2018-01-22 16:17:31
link: https://vrhk.co/2n07ghM
image: 
domain: jack-clark.net
author: "Jack Clark"
icon: https://s0.wp.com/i/webclip.png
excerpt: "AI beats panel of 42 dermatologists at spotting symptoms of a particular skin disorder:&hellip;R-CNN + large amounts of data beats hundreds of years of combined medical schooling&hellip;Scientists have gathered together a large medical-grade dataset of photos of fingernails and toenails and used it to train a neural network to distinguish symptoms of onychomycosis better than a panel of experts. The approach relies on faster R-CNN (GitHub), an object classifier originally developed by Microsoft Research (Arxiv), as well as convolutional neural networks that implement a resnet-152 model (also developed by Microsoft Research). It&rsquo;s another datapoint that, at least in the perceptual domain, it seems like given enough data&amp;compute we can design systems that can match or exceed humans&rsquo; capabilities at narrowly specified tasks.&nbsp;&nbsp;Data janitorial work: The researchers also contribute a dataset of almost ~50,000 nail photographs for use in further research. The paper includes details on how they shaped and cleaned their data to obtain this dataset &ndash; a process that involved the researchers having to train an object localizing system to be able to automatically crop their images to just feature nails, rather than misclassified other things (apparently initially the network would mistake teeth or warts for fingers.)&nbsp;&nbsp;Results: They comprehensively test the resulting networks against a variety of different humans with different skills, ranging from nurses to clinicians to professors with a dermatology specialism. In all cases the AI-based networks matched or exceed large groups of human experts on medical classification tasks. &ldquo;Only one dermatologist performed better than the ensemble model trained with the A1 dataset, and only once in three experiments,&rdquo; they write.&nbsp;&nbsp;The future: One of the promises of AI for medical use-cases is that it can dramatically lower the cost of initial analysis of a given set of symptoms. This experiment backs up that view, and in addition to gathering the dataset and developing the AI techniques, the scientists have also developed a web- and smartphone-based platform to collect and classify further medical data. &ldquo;The results from this study suggest that the CNNs developed in this study and the smartphone platform we developed may be useful in a telemedicine environment where the access to dermatologists is unavailable,&rdquo; they write.&ndash;&nbsp; &nbsp;Read more: Deep neural networks show an equivalent and often superior performance &nbsp;to dermatologists in onychomycosis diagnosis: Automatic construction of onychomycosis datasets by region-based convolutional deep neural network (PLOS One).
US defense establishment to invest in AI, robots:&hellip;New National Defense Strategy memo mentions AI&hellip;The US&rsquo;s new National Defense Strategy calls for the government to &ldquo;invest broadly in military application of autonomy, artificial intelligence, and machine learning, including rapid application of commercial breakthroughs&rdquo;.&nbsp; The summary also hints at the troubling dual use nature of AI and other technologies. &ldquo;The security environment is also affected by rapid technological advancements and the changing character of war. The drive to develop new technologies is relentless, expanding to more actors with lower barriers of entry, and moving at accelerating speed. New technologies include advanced computing, &ldquo;big data&rdquo; analytics, artificial intelligence, autonomy, robotics, directed energy, hypersonics, and biotechnology&mdash; the very technologies that ensure we will be able to fight and win the wars of the future.&ndash;&nbsp; &nbsp;Read more: Summary of the 2018 National Defense Strategy of The United States of America (PDF).
A movable feast of language modeling techniques from , calibration, calibration&hellip;Researchers with  and Aylient Ltd have published details on Fine-tuned Language Models (FitLaM), a set of transfer learning methods to optimize language models for given domains. This paper has a similar flavor to DeepMind&rsquo;s recent &lsquo;Rainbow&rsquo; algorithm, where in both cases researchers integrate a bunch of recent innovations in their field (language modelling and reinforcement learning, respectively), to create an &lsquo;everything-and-the-kitchen-sink&rsquo;-style model, which attains good task performance.&nbsp; Results: FitLaM models attain state-of-the-art scores on five distinct text classification tasks, reducing errors by between 18 and 24 percent on the majority of the datasets.&nbsp; How it works: FitLaM models consists of an RNN with one or more task-specific linear layers, along with a tuning technique that manipulates more data in the higher layers of the network and less in the depths, aiding preservation of information gleaned from general-domain language modelling. Along with this, the authors develop a bunch of different techniques to further facilitate transfer, detailed exhaustively in the paper.&nbsp; Transfer learning: To aid transfer learning, the researchers pre-train a language model on a large text corpus &ndash; in this case Wikitext, which consists of over ~28,000 pre-processed Wikipedia articles. Other techniques they use include &lsquo;gradual unfreezing&rsquo; of neural network layers during re-training, using cosine annealing for fine-tuning, and using reverse annealing as well.&nbsp; Tested domains: Sentiment analysis (two separate datasets), question classification, topic classification (two datasets).&ndash;&nbsp; &nbsp;Read more: Fine-tuned language models for text classification (Arxiv).
Google-owned Kaggle adds free GPUs to online coding service:&hellip;Free GPUS with very few strings attached&hellip;Google says users of Colaboratory, its live coding mashup that works like a cross between a Jupyter Notebook and a Google Doc, no comes with free GPUs. Users can write a few code snippets, detailed here, and get access to two vCPUs with 13GB of RAM and, the icing on the cake &ndash; an NVIDIA K80 GPU, according to a comment from an account linked to Michael Piatek at Google.&ndash;&nbsp; &nbsp;&nbsp;Access Colaboratory here.
First came ResNets, then DenseNets, now&hellip; SparseNets?&hellip;Researchers chain networks together in weird ways to attain state-of-the-art results&hellip;Neural networks can in one way be viewed as machines that operate over distinct datasets and figure out the transformation that ties them together, researchers have developed approaches (Residual Networks and DenseNets) that are able to pick up on successively finer-grained features that distinguish different visual phenomena, while insuring that as much information as possible can propagate from one layer of a network to another.&nbsp; Now, researchers with Simon Fraser University have tried to take the best traits from ResNets and DenseNets and synthesize them into SparseNets, a way of structuring networks that &ldquo;aggregates features from previous layers: each layer only takes features from layers that have exponential offsets away from it&hellip; Experimental results on the CIFAR-10 and CIFAR-100 datasets demonstrate that SparseNets are able to achieve comparable performance to current state-of-the art models with significantly fewer parameters,&rdquo; they write.&nbsp;&nbsp;Thrifty networks: So, what&rsquo;s the motivation for structuring networks in such a way? It&rsquo;s that if you can expand the size of the network without adding too many parameters, then you know you&rsquo;ll ultimately be able to exploit this efficiency to build even larger networks in the future. Experiments with SparseNet show that networks built like this can attain accuracies similar to those obtained by ResNets and DenseNets on a far, far smaller parameter budget.&ndash;&nbsp; &nbsp;Read more: Sparsely Connected Convolutional Networks.
Bootstrapping data quality with neural networks:&hellip;Chinese research…"

---

### Import AI: #78: Google gives away free K80 GPUs; Chinese researchers benchmark thermal imagery pedestrian trackers; and AI triumphs against dermatologists in diagnosis competition

AI beats panel of 42 dermatologists at spotting symptoms of a particular skin disorder:&hellip;R-CNN + large amounts of data beats hundreds of years of combined medical schooling&hellip;Scientists have gathered together a large medical-grade dataset of photos of fingernails and toenails and used it to train a neural network to distinguish symptoms of onychomycosis better than a panel of experts. The approach relies on faster R-CNN (GitHub), an object classifier originally developed by Microsoft Research (Arxiv), as well as convolutional neural networks that implement a resnet-152 model (also developed by Microsoft Research). It&rsquo;s another datapoint that, at least in the perceptual domain, it seems like given enough data&amp;compute we can design systems that can match or exceed humans&rsquo; capabilities at narrowly specified tasks.&nbsp;&nbsp;Data janitorial work: The researchers also contribute a dataset of almost ~50,000 nail photographs for use in further research. The paper includes details on how they shaped and cleaned their data to obtain this dataset &ndash; a process that involved the researchers having to train an object localizing system to be able to automatically crop their images to just feature nails, rather than misclassified other things (apparently initially the network would mistake teeth or warts for fingers.)&nbsp;&nbsp;Results: They comprehensively test the resulting networks against a variety of different humans with different skills, ranging from nurses to clinicians to professors with a dermatology specialism. In all cases the AI-based networks matched or exceed large groups of human experts on medical classification tasks. &ldquo;Only one dermatologist performed better than the ensemble model trained with the A1 dataset, and only once in three experiments,&rdquo; they write.&nbsp;&nbsp;The future: One of the promises of AI for medical use-cases is that it can dramatically lower the cost of initial analysis of a given set of symptoms. This experiment backs up that view, and in addition to gathering the dataset and developing the AI techniques, the scientists have also developed a web- and smartphone-based platform to collect and classify further medical data. &ldquo;The results from this study suggest that the CNNs developed in this study and the smartphone platform we developed may be useful in a telemedicine environment where the access to dermatologists is unavailable,&rdquo; they write.&ndash;&nbsp; &nbsp;Read more: Deep neural networks show an equivalent and often superior performance &nbsp;to dermatologists in onychomycosis diagnosis: Automatic construction of onychomycosis datasets by region-based convolutional deep neural network (PLOS One).
US defense establishment to invest in AI, robots:&hellip;New National Defense Strategy memo mentions AI&hellip;The US&rsquo;s new National Defense Strategy calls for the government to &ldquo;invest broadly in military application of autonomy, artificial intelligence, and machine learning, including rapid application of commercial breakthroughs&rdquo;.&nbsp; The summary also hints at the troubling dual use nature of AI and other technologies. &ldquo;The security environment is also affected by rapid technological advancements and the changing character of war. The drive to develop new technologies is relentless, expanding to more actors with lower barriers of entry, and moving at accelerating speed. New technologies include advanced computing, &ldquo;big data&rdquo; analytics, artificial intelligence, autonomy, robotics, directed energy, hypersonics, and biotechnology&mdash; the very technologies that ensure we will be able to fight and win the wars of the future.&ndash;&nbsp; &nbsp;Read more: Summary of the 2018 National Defense Strategy of The United States of America (PDF).
A movable feast of language modeling techniques from , calibration, calibration&hellip;Researchers with  and Aylient Ltd have published details on Fine-tuned Language Models (FitLaM), a set of transfer learning methods to optimize language models for given domains. This paper has a similar flavor to DeepMind&rsquo;s recent &lsquo;Rainbow&rsquo; algorithm, where in both cases researchers integrate a bunch of recent innovations in their field (language modelling and reinforcement learning, respectively), to create an &lsquo;everything-and-the-kitchen-sink&rsquo;-style model, which attains good task performance.&nbsp; Results: FitLaM models attain state-of-the-art scores on five distinct text classification tasks, reducing errors by between 18 and 24 percent on the majority of the datasets.&nbsp; How it works: FitLaM models consists of an RNN with one or more task-specific linear layers, along with a tuning technique that manipulates more data in the higher layers of the network and less in the depths, aiding preservation of information gleaned from general-domain language modelling. Along with this, the authors develop a bunch of different techniques to further facilitate transfer, detailed exhaustively in the paper.&nbsp; Transfer learning: To aid transfer learning, the researchers pre-train a language model on a large text corpus &ndash; in this case Wikitext, which consists of over ~28,000 pre-processed Wikipedia articles. Other techniques they use include &lsquo;gradual unfreezing&rsquo; of neural network layers during re-training, using cosine annealing for fine-tuning, and using reverse annealing as well.&nbsp; Tested domains: Sentiment analysis (two separate datasets), question classification, topic classification (two datasets).&ndash;&nbsp; &nbsp;Read more: Fine-tuned language models for text classification (Arxiv).
Google-owned Kaggle adds free GPUs to online coding service:&hellip;Free GPUS with very few strings attached&hellip;Google says users of Colaboratory, its live coding mashup that works like a cross between a Jupyter Notebook and a Google Doc, no comes with free GPUs. Users can write a few code snippets, detailed here, and get access to two vCPUs with 13GB of RAM and, the icing on the cake &ndash; an NVIDIA K80 GPU, according to a comment from an account linked to Michael Piatek at Google.&ndash;&nbsp; &nbsp;&nbsp;Access Colaboratory here.
First came ResNets, then DenseNets, now&hellip; SparseNets?&hellip;Researchers chain networks together in weird ways to attain state-of-the-art results&hellip;Neural networks can in one way be viewed as machines that operate over distinct datasets and figure out the transformation that ties them together, researchers have developed approaches (Residual Networks and DenseNets) that are able to pick up on successively finer-grained features that distinguish different visual phenomena, while insuring that as much information as possible can propagate from one layer of a network to another.&nbsp; Now, researchers with Simon Fraser University have tried to take the best traits from ResNets and DenseNets and synthesize them into SparseNets, a way of structuring networks that &ldquo;aggregates features from previous layers: each layer only takes features from layers that have exponential offsets away from it&hellip; Experimental results on the CIFAR-10 and CIFAR-100 datasets demonstrate that SparseNets are able to achieve comparable performance to current state-of-the art models with significantly fewer parameters,&rdquo; they write.&nbsp;&nbsp;Thrifty networks: So, what&rsquo;s the motivation for structuring networks in such a way? It&rsquo;s that if you can expand the size of the network without adding too many parameters, then you know you&rsquo;ll ultimately be able to exploit this efficiency to build even larger networks in the future. Experiments with SparseNet show that networks built like this can attain accuracies similar to those obtained by ResNets and DenseNets on a far, far smaller parameter budget.&ndash;&nbsp; &nbsp;Read more: Sparsely Connected Convolutional Networks.
Bootstrapping data quality with neural networks:&hellip;Chinese research…