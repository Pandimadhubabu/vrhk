---

layout: post
category: product
title: "Import AI 128: Better pose estimation through AI; Amazon Alexa gets smarter by tapping insights from Alexa Prize, and differential privacy gets easier to implement in TensorFlow"
date: 2019-01-07 16:41:50
link: https://vrhk.co/2LTKbbT
image: 
domain: jack-clark.net
author: "Jack Clark"
icon: https://s2.wp.com/i/webclip.png
excerpt: "How to test vision systems for reliability: sample from 140 public security cameras:&hellip;More work needed before everyone can get cheap out-of-the-box low light object detection&hellip;Are benchmarks reliable? That&rsquo;s a question many researchers ask themselves, whether testing supervised learning or reinforcement learning algorithms. Now, researchers with Purdue University, Loyola University Chicago, Argonne National Laboratory, Intel, and Facebook have tried to create a reliable, real world benchmark for computer vision applications. The researchers use a network of 140 publicly accessible camera feeds to gather 5 million images over a 24 hour period, then test a widely deployed &lsquo;YOLO&rsquo; object detector against these images.&nbsp;&nbsp;Data: The researchers generate the data for this project by pulling information from CAM2, the Continuous Analysis of Many CAMeras project, which is built and maintained by Purdue University researchers.&nbsp; Can you trust YOLO at night? YOLO performance degrades at night, causing the system to fail to detect cars when they are illuminated only by streetlights (and conversely, at night it sometimes mistakes streetlights for vehicles&rsquo; headlights, causing it to label lights as cars).&nbsp; Is YOLO consistent? YOLO&rsquo;s performance isn&rsquo;t as consistent as people might hope &ndash; there are frequent cases where YOLO&rsquo;s predictions for the total number of cars parked on a street varies over time.&nbsp; Big clusters: The researchers used two supercomputing clusters to perform image classification: one cluster used a mixture of Intel Skylake CPU and Knights Landing Xeon Phi cores, and the other cluster used a combination of CPUs and NVIDIA dual-K80 GPUs. The researchers used this infrastructure to process data in parallel, but did not analyze the different execution times on the different hardware clusters.&nbsp; Labeling: The researchers estimate it would take approximately ~600 days to label all 5 million images, so instead labels a subset (13,440) images, then checks labels from YOLO against this test set.&nbsp; Why it matters: As AI industrializes being able to generate trustworthy data about the performance of systems will be crucial to giving people the confidence necessary to adopt the technology; tests like this both show how to create new, large-scale robust datasets to test systems, and indicate that we need to develop more effective algorithms to have systems sufficiently powerful for real-world deployment.&nbsp;&nbsp;Read more: Large-Scale Object Detection of Images from Network Cameras in Variable Ambient Lighting Conditions (Arxiv).&nbsp;&nbsp;Read more about the dataset (CAM2 site).
Amazon makes Alexa smarter and more conversational via the Alexa Prize:&hellip;Report analyzing results of this year&rsquo;s competition&hellip;Amazon has shared details of how it improved the capabilities of its Alexa personal assistant through running the Alexa open research prize. The tl;dr is that inventions made by the 16 participating teams during the competition have improved Alexa in the following ways: &ldquo;driven improved experiences by Alexa users to an average rating of 3.61, median duration of 2 mins 18 seconds, and average [conversation] turns to 14.6, increases of 14%, 92%, 54% respectively since the launch of the 2018 competition&rdquo;, Amazon wrote.&nbsp;&nbsp;Significant speech recognition improvements: The competition has also meaningfully improved the speech recognition performance of Amazon&rsquo;s system &ndash; significant, given how fundamental speech is to Alexa. &ldquo;For conversational speech recognition, we have improved our relative Word Error Rate by 55% and our relative Entity Error Rate by 34% since the launch of the Alexa Prize,&rdquo; Amazon wrote. &ldquo;Significant improvement in ASR quality have been obtained by ingesting the Alexa Prize conversation transcriptions in the models&rdquo; as well as through algorithmic advancements developed by the teams, they write.&nbsp;&nbsp;Increasing usage: As the competition was in its second year in 2018, Amazon now has some comparative data to use to compare general growth in Alexa usage. &ldquo;Over the course of the 2018 competition, we have driven over 60,000 hours of conversations spanning millions of interactions, 50% higher than we saw in the 2017 competition,&rdquo; they wrote.&nbsp; Why it matters: Competitions like this show how companies can use deployed products to tempt researchers into doing work for them, and highlights how the platforms will likely trade access for AI agents (eg, Alexa) in exchange for the ideas of researchers. It also highlights the benefit of scale: it would be comparatively difficult for a startup with a personal assistant with a small install base to offer a competition offering the same scale and diversity of interaction as the Alexa Prize.&nbsp;&nbsp;Read more: Advancing the State of the Art in Open Domain Dialog Systems through the Alexa Prize (Arxiv).
Chinese researchers create high-performance &lsquo;pose estimation&rsquo; network:&hellip;Omni-use technology highlights the challenges of AI policy; pose estimation can help us make better games and help people get fit, but can also surveil people&hellip;Researchers with facial recognition startup Megvii, Inc; Shanghai Jiao Tong University; Beihang University, and Beijing University of Posts and Telecommunications have improved the performance of surveillance AI technologies via implementing what they call a &lsquo;multi-stage pose estimation network&rsquo; (MSPN). Pose estimation is a general purpose computer vision capability that lets people figure out the wireframe skeleton of a person from images and/or video footage &ndash; this sort of technology has been widely used for things like CGI and game playing (eg, game consoles might extract poses from people via cameras like the Kinect and use this to feed the AI component of an interactive fitness video game, etc). It also has significant applications for automated surveillance and/or image/video analysis, as it lets you label large groups of people from their poses &ndash; one can imagine the utility of being able to automatically flag if a crowd of protestors display a statistically meaningful increase in violent behaviors, or being able to isolate the one person in a crowded train station who is behaving unusually.&nbsp; How it works: MSPN: The MSPN has three tweaks that the researchers say explains its performance: tweaks to the main classification module to prevent information being lost during downscaling of images during processing; improving post localization by adopting a coarse-to-fine supervision strategy, and sharing more features across the network during training.&nbsp; Results: &ldquo;New state-of-the-art performance is achieved, with a large margin compared to all previous methods,&rdquo; the researchers write. Some of the baselines they test against include: AE, G-RMI, CPN, Mask R-CNN, and CMU Pose. The MSPN obtains state-of-the-art scores on the COCO test set, with versions of the MSPN that use purely COCO test-dev data managing to score higher than some systems which augmented themselves with additional data.&nbsp; Why it matters: AI is, day in day out, improving the capabilities of automated surveillance systems. It&rsquo;s worth remembering that for a huge amount of areas of AI research, progress in any one domain (for instance, an improved architecture for supervised classification like a Residual Networks) can have knock-on effects in other more applied domains, like surveillance. This highlights both the omni-use nature of AI, as well as the difficulty of differentiating between benign and less benign applications of the technology.&nbsp; Read more: Rethinking on Multi-Stage Networks for Human Pose Estimation (Arxiv).
Making deep learning more secure: Google releases TensorFlow Privacy&hellip;New library lets people train models compliant with more …"

---

### Import AI 128: Better pose estimation through AI; Amazon Alexa gets smarter by tapping insights from Alexa Prize, and differential privacy gets easier to implement in TensorFlow

How to test vision systems for reliability: sample from 140 public security cameras:&hellip;More work needed before everyone can get cheap out-of-the-box low light object detection&hellip;Are benchmarks reliable? That&rsquo;s a question many researchers ask themselves, whether testing supervised learning or reinforcement learning algorithms. Now, researchers with Purdue University, Loyola University Chicago, Argonne National Laboratory, Intel, and Facebook have tried to create a reliable, real world benchmark for computer vision applications. The researchers use a network of 140 publicly accessible camera feeds to gather 5 million images over a 24 hour period, then test a widely deployed &lsquo;YOLO&rsquo; object detector against these images.&nbsp;&nbsp;Data: The researchers generate the data for this project by pulling information from CAM2, the Continuous Analysis of Many CAMeras project, which is built and maintained by Purdue University researchers.&nbsp; Can you trust YOLO at night? YOLO performance degrades at night, causing the system to fail to detect cars when they are illuminated only by streetlights (and conversely, at night it sometimes mistakes streetlights for vehicles&rsquo; headlights, causing it to label lights as cars).&nbsp; Is YOLO consistent? YOLO&rsquo;s performance isn&rsquo;t as consistent as people might hope &ndash; there are frequent cases where YOLO&rsquo;s predictions for the total number of cars parked on a street varies over time.&nbsp; Big clusters: The researchers used two supercomputing clusters to perform image classification: one cluster used a mixture of Intel Skylake CPU and Knights Landing Xeon Phi cores, and the other cluster used a combination of CPUs and NVIDIA dual-K80 GPUs. The researchers used this infrastructure to process data in parallel, but did not analyze the different execution times on the different hardware clusters.&nbsp; Labeling: The researchers estimate it would take approximately ~600 days to label all 5 million images, so instead labels a subset (13,440) images, then checks labels from YOLO against this test set.&nbsp; Why it matters: As AI industrializes being able to generate trustworthy data about the performance of systems will be crucial to giving people the confidence necessary to adopt the technology; tests like this both show how to create new, large-scale robust datasets to test systems, and indicate that we need to develop more effective algorithms to have systems sufficiently powerful for real-world deployment.&nbsp;&nbsp;Read more: Large-Scale Object Detection of Images from Network Cameras in Variable Ambient Lighting Conditions (Arxiv).&nbsp;&nbsp;Read more about the dataset (CAM2 site).
Amazon makes Alexa smarter and more conversational via the Alexa Prize:&hellip;Report analyzing results of this year&rsquo;s competition&hellip;Amazon has shared details of how it improved the capabilities of its Alexa personal assistant through running the Alexa open research prize. The tl;dr is that inventions made by the 16 participating teams during the competition have improved Alexa in the following ways: &ldquo;driven improved experiences by Alexa users to an average rating of 3.61, median duration of 2 mins 18 seconds, and average [conversation] turns to 14.6, increases of 14%, 92%, 54% respectively since the launch of the 2018 competition&rdquo;, Amazon wrote.&nbsp;&nbsp;Significant speech recognition improvements: The competition has also meaningfully improved the speech recognition performance of Amazon&rsquo;s system &ndash; significant, given how fundamental speech is to Alexa. &ldquo;For conversational speech recognition, we have improved our relative Word Error Rate by 55% and our relative Entity Error Rate by 34% since the launch of the Alexa Prize,&rdquo; Amazon wrote. &ldquo;Significant improvement in ASR quality have been obtained by ingesting the Alexa Prize conversation transcriptions in the models&rdquo; as well as through algorithmic advancements developed by the teams, they write.&nbsp;&nbsp;Increasing usage: As the competition was in its second year in 2018, Amazon now has some comparative data to use to compare general growth in Alexa usage. &ldquo;Over the course of the 2018 competition, we have driven over 60,000 hours of conversations spanning millions of interactions, 50% higher than we saw in the 2017 competition,&rdquo; they wrote.&nbsp; Why it matters: Competitions like this show how companies can use deployed products to tempt researchers into doing work for them, and highlights how the platforms will likely trade access for AI agents (eg, Alexa) in exchange for the ideas of researchers. It also highlights the benefit of scale: it would be comparatively difficult for a startup with a personal assistant with a small install base to offer a competition offering the same scale and diversity of interaction as the Alexa Prize.&nbsp;&nbsp;Read more: Advancing the State of the Art in Open Domain Dialog Systems through the Alexa Prize (Arxiv).
Chinese researchers create high-performance &lsquo;pose estimation&rsquo; network:&hellip;Omni-use technology highlights the challenges of AI policy; pose estimation can help us make better games and help people get fit, but can also surveil people&hellip;Researchers with facial recognition startup Megvii, Inc; Shanghai Jiao Tong University; Beihang University, and Beijing University of Posts and Telecommunications have improved the performance of surveillance AI technologies via implementing what they call a &lsquo;multi-stage pose estimation network&rsquo; (MSPN). Pose estimation is a general purpose computer vision capability that lets people figure out the wireframe skeleton of a person from images and/or video footage &ndash; this sort of technology has been widely used for things like CGI and game playing (eg, game consoles might extract poses from people via cameras like the Kinect and use this to feed the AI component of an interactive fitness video game, etc). It also has significant applications for automated surveillance and/or image/video analysis, as it lets you label large groups of people from their poses &ndash; one can imagine the utility of being able to automatically flag if a crowd of protestors display a statistically meaningful increase in violent behaviors, or being able to isolate the one person in a crowded train station who is behaving unusually.&nbsp; How it works: MSPN: The MSPN has three tweaks that the researchers say explains its performance: tweaks to the main classification module to prevent information being lost during downscaling of images during processing; improving post localization by adopting a coarse-to-fine supervision strategy, and sharing more features across the network during training.&nbsp; Results: &ldquo;New state-of-the-art performance is achieved, with a large margin compared to all previous methods,&rdquo; the researchers write. Some of the baselines they test against include: AE, G-RMI, CPN, Mask R-CNN, and CMU Pose. The MSPN obtains state-of-the-art scores on the COCO test set, with versions of the MSPN that use purely COCO test-dev data managing to score higher than some systems which augmented themselves with additional data.&nbsp; Why it matters: AI is, day in day out, improving the capabilities of automated surveillance systems. It&rsquo;s worth remembering that for a huge amount of areas of AI research, progress in any one domain (for instance, an improved architecture for supervised classification like a Residual Networks) can have knock-on effects in other more applied domains, like surveillance. This highlights both the omni-use nature of AI, as well as the difficulty of differentiating between benign and less benign applications of the technology.&nbsp; Read more: Rethinking on Multi-Stage Networks for Human Pose Estimation (Arxiv).
Making deep learning more secure: Google releases TensorFlow Privacy&hellip;New library lets people train models compliant with more …