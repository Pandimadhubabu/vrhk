---

layout: post
category: product
title: "Import AI: #105: Why researchers should explore the potential negative effects of their work; fusing deep learning with classical planning for better robots, and who needs adversarial examples when a blur will do?"
date: 2018-07-30 15:07:07
link: https://vrhk.co/2KdFAik
image: 
domain: jack-clark.net
author: "Jack Clark"
icon: https://s2.wp.com/i/webclip.png
excerpt: "Computer scientist calls for researchers to discuss downsides of work, as well as upsides:&hellip;Interview with Brent Hecht, chair of the Association for Computing Machinery (ACM)&rsquo;s Future of Computing Academy, which said in March that researchers should list downsides of their work&hellip;One of the repeated problems AI researchers deal with is the omni-use nature of the technology: a system designed to recognize a wide variety of people in different poses and scenes can also be used to surveil people; auto-navigation systems for disaster response can be repurposed for weaponizing consumer platforms; systems to read lips and thereby improve the quality of life of people with hearing and/or speech difficulties can also be used to surreptitiously analyze people in the wild; and so on.&nbsp; Recently, the omni-use nature of this tech has been highlighted as companies like Amazon develop facial recognition tools which are subsequently used by the police, or how Google uses computer vision techniques to develop systems for the &lsquo;MAVEN&rsquo; program from the DoD. What can companies and researchers do to increase the positive effects of their research and minimize some of the downsides? Computer science professor Brent Hecht says in an interview with Nature that scientists should consider changing the process of peer review to encourage scientists to talk about the potential for abuse of their work.&nbsp; &ldquo;In the past few years, there&rsquo;s been a sea-change in how the public views the real-world impacts of computer science, which doesn&rsquo;t align with how many in the computing community view our work,&rdquo; he says. &ldquo;A sizeable population in computer science thinks that this is not our problem. But while that perspective was common ten years ago, I hear it less and less these days.&rdquo;&nbsp; Why it matters: &ldquo;Disclosing negative impacts is not just an end in itself, but a public statement of new problems that need to be solved,&rdquo; he says. &ldquo;We need to bend the incentives in computer science towards making the net impact of innovations positive.&rdquo;&nbsp; Read more: The ethics of computer science: this researcher has a controversial proposal (Nature).
Sponsored: The AI Conference &ndash; San Francisco, Sept 4&ndash;7:&hellip;Join the leading minds in AI, including Kai-Fu Lee, Meredith Whittaker, Peter Norvig, Dave Patterson, and Matt Wood. No other conference combines this depth of technical expertise with a laser focus on how to apply AI in your products and in your business today.&hellip;Register soon. Last year this event sold out; training courses and tutorials are filling up fast. Save an extra 20% on most passes with code IMPORTAI20.
Worried about adversarial examples and self-driving cars? You should really be worried about blurry images:&hellip;Very basic corruptions to images can cause significant accuracy drops, research shows&hellip;Researchers with the National Robotics Engineering Center and the Electrical and Computer Engineering Department at CMU have shown that simply applying basic image degradations that blur images, or add haze to them, leads to significant performance issues. &ldquo;We show cases where performance drops catastrophically in response to barely perceptible changes,&rdquo; writes researcher Phil Koopman in a blog post that explains the research. &ldquo;You don&rsquo;t need adversarial attacks to foil machine learning-based perception &ndash; straightforward image degradations such as blur or haze can cause problems too&rdquo;.&nbsp; Testing: The researchers test a variety of algorithms across three different architectures (Faster R-CNN, Single Shot Detector (SSD), and Region-based Fully Convolutional Network (R-FCN); they test these architectures with a variety of feature extractors, like Inception or MobileNets. They evaluate these algorithms by testing them on the NREC &lsquo;Agricultural Person Detection Dataset&rsquo;. The researchers apply two types of mutation to the images: &ldquo;simple&rdquo; mutators which modify the image, and &ldquo;contextual&rdquo; mutators which mutate the image while adding additional information. To test the &ldquo;simple&rdquo; mutations they apply simple image transformations, like Gaussian blur, JPEG Compression, the addition of salt and pepper noise, and so on. For the &ldquo;contextual&rdquo; mutations they apply things like haze to the image.&nbsp; Results: In tests, the researchers show that very few detectors are immune from the effects of these perturbations, with results indicating that the Single Shot Detectors (SSD)s have the greatest amount of trouble with dealing with these relatively minor tweaks. One point of interest is that some of the systems which are resilient to these mutations are resilient to quite a few of them quite consistently &ndash; the presence of these patterns shows &ldquo;generalized robustness trends&rdquo;, which may serve as signposts for future researchers to further evaluate generalization.&nbsp; Read more: Putting image manipulations in context: robustness testing for safe perception (Safe Autonomy / Phil Koopman blogspot).&nbsp; Read more: Putting Image Manipulations in Context: Robustness Testing for Safe Perception (PDF).
Researchers count on blobs to solve counting problems:&hellip;Segmenting objects may be hard, but placing dots on them may be easy&hellip;Precisely counting objects in scenes, like the number of cars on a road or people walking through a city, is a task that challenges both humans and machines. Researchers are training object counters to label individual entities via dots to indicate each entity, rather than pixel segmentation masks or bounding boxes, as is typical. &ldquo;We propose a novel loss function that encourages the model to output instance regions such that each region contains a single object instance (i.e. a single point-level annotation),&rdquo; they explain. This tweak significantly improves performance relative to other baselines based on segmentation and depth.They evaluate their approach on diverse datasets, consisting of images of parking lots, images taken by traffic cameras, images of penguins, PASCAL VOC 2007, another surveillance dataset called MIT Traffic, and Crowd Counting Datasets.&nbsp; Why it matters: Counting objects is a difficult task for AI systems, and approaches like this indicate other ways to tackle the problem. In the future, the researchers want to design new network architectures that can better distinguish between overlapping objects that have complicated shapes and appearances.&nbsp; Read more: Where are the Blobs: Counting by Localization with Point Supervision (Arxiv).
Predicting win states in Dota 2 for better reinforcement learning research:&hellip;System&rsquo;s recommendations outperform proprietary product&rsquo;s&hellip;Researchers have trained a system to predict the probability of a given team winning or losing a game of popular online game Dota 2. This is occurring at the same time that researchers across the world try to turn MOBAs into test-beds for reinforcement learning.&nbsp; To train their model, the researchers downloaded and parsed replay files from over 100,000 Dota 2 matches. They generate discrete bits of data for each 60 second period of a game, containing a vector which encodes information about the players state at that point in time. They then use these slices to inform a point-in-time &lsquo;Time Slice Evaluation&rsquo; (TSE) model which attempts to predict the outcome of the match from a given point in time. . The researchers do detect some correlation between the elapsed game time, the ultimate outcome of the match, and the data contained within the slice being studied at this point in time. Specifically, they find that after the first fifty percent of games it becomes fairly easy to train a model to accurately predict win likelihoods, so they train their system on this data.&nbsp; Results: The resulting syst…"

---

### Import AI: #105: Why researchers should explore the potential negative effects of their work; fusing deep learning with classical planning for better robots, and who needs adversarial examples when a blur will do?

Computer scientist calls for researchers to discuss downsides of work, as well as upsides:&hellip;Interview with Brent Hecht, chair of the Association for Computing Machinery (ACM)&rsquo;s Future of Computing Academy, which said in March that researchers should list downsides of their work&hellip;One of the repeated problems AI researchers deal with is the omni-use nature of the technology: a system designed to recognize a wide variety of people in different poses and scenes can also be used to surveil people; auto-navigation systems for disaster response can be repurposed for weaponizing consumer platforms; systems to read lips and thereby improve the quality of life of people with hearing and/or speech difficulties can also be used to surreptitiously analyze people in the wild; and so on.&nbsp; Recently, the omni-use nature of this tech has been highlighted as companies like Amazon develop facial recognition tools which are subsequently used by the police, or how Google uses computer vision techniques to develop systems for the &lsquo;MAVEN&rsquo; program from the DoD. What can companies and researchers do to increase the positive effects of their research and minimize some of the downsides? Computer science professor Brent Hecht says in an interview with Nature that scientists should consider changing the process of peer review to encourage scientists to talk about the potential for abuse of their work.&nbsp; &ldquo;In the past few years, there&rsquo;s been a sea-change in how the public views the real-world impacts of computer science, which doesn&rsquo;t align with how many in the computing community view our work,&rdquo; he says. &ldquo;A sizeable population in computer science thinks that this is not our problem. But while that perspective was common ten years ago, I hear it less and less these days.&rdquo;&nbsp; Why it matters: &ldquo;Disclosing negative impacts is not just an end in itself, but a public statement of new problems that need to be solved,&rdquo; he says. &ldquo;We need to bend the incentives in computer science towards making the net impact of innovations positive.&rdquo;&nbsp; Read more: The ethics of computer science: this researcher has a controversial proposal (Nature).
Sponsored: The AI Conference &ndash; San Francisco, Sept 4&ndash;7:&hellip;Join the leading minds in AI, including Kai-Fu Lee, Meredith Whittaker, Peter Norvig, Dave Patterson, and Matt Wood. No other conference combines this depth of technical expertise with a laser focus on how to apply AI in your products and in your business today.&hellip;Register soon. Last year this event sold out; training courses and tutorials are filling up fast. Save an extra 20% on most passes with code IMPORTAI20.
Worried about adversarial examples and self-driving cars? You should really be worried about blurry images:&hellip;Very basic corruptions to images can cause significant accuracy drops, research shows&hellip;Researchers with the National Robotics Engineering Center and the Electrical and Computer Engineering Department at CMU have shown that simply applying basic image degradations that blur images, or add haze to them, leads to significant performance issues. &ldquo;We show cases where performance drops catastrophically in response to barely perceptible changes,&rdquo; writes researcher Phil Koopman in a blog post that explains the research. &ldquo;You don&rsquo;t need adversarial attacks to foil machine learning-based perception &ndash; straightforward image degradations such as blur or haze can cause problems too&rdquo;.&nbsp; Testing: The researchers test a variety of algorithms across three different architectures (Faster R-CNN, Single Shot Detector (SSD), and Region-based Fully Convolutional Network (R-FCN); they test these architectures with a variety of feature extractors, like Inception or MobileNets. They evaluate these algorithms by testing them on the NREC &lsquo;Agricultural Person Detection Dataset&rsquo;. The researchers apply two types of mutation to the images: &ldquo;simple&rdquo; mutators which modify the image, and &ldquo;contextual&rdquo; mutators which mutate the image while adding additional information. To test the &ldquo;simple&rdquo; mutations they apply simple image transformations, like Gaussian blur, JPEG Compression, the addition of salt and pepper noise, and so on. For the &ldquo;contextual&rdquo; mutations they apply things like haze to the image.&nbsp; Results: In tests, the researchers show that very few detectors are immune from the effects of these perturbations, with results indicating that the Single Shot Detectors (SSD)s have the greatest amount of trouble with dealing with these relatively minor tweaks. One point of interest is that some of the systems which are resilient to these mutations are resilient to quite a few of them quite consistently &ndash; the presence of these patterns shows &ldquo;generalized robustness trends&rdquo;, which may serve as signposts for future researchers to further evaluate generalization.&nbsp; Read more: Putting image manipulations in context: robustness testing for safe perception (Safe Autonomy / Phil Koopman blogspot).&nbsp; Read more: Putting Image Manipulations in Context: Robustness Testing for Safe Perception (PDF).
Researchers count on blobs to solve counting problems:&hellip;Segmenting objects may be hard, but placing dots on them may be easy&hellip;Precisely counting objects in scenes, like the number of cars on a road or people walking through a city, is a task that challenges both humans and machines. Researchers are training object counters to label individual entities via dots to indicate each entity, rather than pixel segmentation masks or bounding boxes, as is typical. &ldquo;We propose a novel loss function that encourages the model to output instance regions such that each region contains a single object instance (i.e. a single point-level annotation),&rdquo; they explain. This tweak significantly improves performance relative to other baselines based on segmentation and depth.They evaluate their approach on diverse datasets, consisting of images of parking lots, images taken by traffic cameras, images of penguins, PASCAL VOC 2007, another surveillance dataset called MIT Traffic, and Crowd Counting Datasets.&nbsp; Why it matters: Counting objects is a difficult task for AI systems, and approaches like this indicate other ways to tackle the problem. In the future, the researchers want to design new network architectures that can better distinguish between overlapping objects that have complicated shapes and appearances.&nbsp; Read more: Where are the Blobs: Counting by Localization with Point Supervision (Arxiv).
Predicting win states in Dota 2 for better reinforcement learning research:&hellip;System&rsquo;s recommendations outperform proprietary product&rsquo;s&hellip;Researchers have trained a system to predict the probability of a given team winning or losing a game of popular online game Dota 2. This is occurring at the same time that researchers across the world try to turn MOBAs into test-beds for reinforcement learning.&nbsp; To train their model, the researchers downloaded and parsed replay files from over 100,000 Dota 2 matches. They generate discrete bits of data for each 60 second period of a game, containing a vector which encodes information about the players state at that point in time. They then use these slices to inform a point-in-time &lsquo;Time Slice Evaluation&rsquo; (TSE) model which attempts to predict the outcome of the match from a given point in time. . The researchers do detect some correlation between the elapsed game time, the ultimate outcome of the match, and the data contained within the slice being studied at this point in time. Specifically, they find that after the first fifty percent of games it becomes fairly easy to train a model to accurately predict win likelihoods, so they train their system on this data.&nbsp; Results: The resulting syst…