---

layout: post
category: product
title: "Import AI: #85: Keeping it simple with temporal convolutional networks instead of RNNs, learning to prefetch with neural nets, and India’s automation challenge."
date: 2018-03-12 21:21:56
link: https://vrhk.co/2DmJmCp
image: 
domain: jack-clark.net
author: "Jack Clark"
icon: https://s0.wp.com/i/webclip.png
excerpt: "Administrative note: a somewhat abbreviated issue this week as I&rsquo;ve been traveling quite a lot and have chosen sleep above reading papers (gasp!).
It&rsquo;s simpler than you think: researchers show convolutional networks frequently beat recurrent ones:&hellip;The rise and rise of simplistic techniques continues&hellip;Researchers with Carnegie Mellon University &nbsp;and Intel Labs have rigorously tested the capabilities of convolutional neural networks (via a &lsquo;temporal convolutional network&rsquo; (TCN) architecture, inspired by Wavenet and other recent innovations) against sequence modeling architectures like Recurrent Nets (via LSTMs and GRUs). The advantages of TCNs for sequence modeling are as follows: easily parallelizable rather than relying on sequential processing; a flexible receptive field size; stable gradients; low memory requirements for training; and variable length inputs. Disadvantages include: a greater data storage need than RNNs; parameters need to be fiddled with when shifting into different data domains.&nbsp;&nbsp;Testing: The researchers test out TCNs against RNNS, GRUs, and LSTMs on a variety of sequence modeling tasks, ranging from MNIST, to adding and copy tasks, to word-level and character-level perplexity on language tasks. In nine out of eleven cases the TCN comes out far ahead of other techniques, in one of the eleven cases it roughly matches GRU performance, and in another case it is noticeably worse then an LSTM (though still comes in second).&nbsp; What happens now: &ldquo;The preeminence enjoyed by recurrent networks in sequence modeling may be largely a vestige of history. Until recently, before the introduction of architectural elements such as dilated convolutions and residual connections, convolutional architectures were indeed weaker. Our results indicate that with these elements, a simple convolutional architecture is more effective across diverse sequence modeling tasks than recurrent architectures such as LSTMs. Due to the comparable clarity and simplicity of TCNs, we conclude that convolutional networks should be regarded as a natural starting point and a powerful toolkit for sequence modeling,&rdquo; write the researchers.&nbsp;&nbsp;Why it matters: One of the most confusing things about machine learning is that it&rsquo;s a defiantly empirical science, with new techniques appearing and proliferating in response to measured performance on given tasks. What studies like this indicate is that many of these new architectures could be overly complex relative to their utility and it&rsquo;s likely that, with just a few tweaks, the basic building blocks still reign supreme; we&rsquo;ve seen a similar phenomenon with basic LSTMs and GANs doing better than many other more-recent innovations, given thorough analysis. In one sense this seems good as it seems intuitive that simpler architectures tend to be more flexible and general, and in another sense it&rsquo;s unnerving, as it suggests much of the complexity that abounds in AI is an artifact of empirical science rather than theoretically justified.&nbsp; Read more: An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling (Arxiv).&nbsp;&nbsp;Code for the TCN used in the experiments here (GitHub).Automation &amp; economies: it&rsquo;s complicated:&hellip;Where AI technology comes from, why automation could be challenging for India, and more&hellip;In a podcast, three employees of the McKinsey Global Institute discuss how automation will impact China, Europe, and India. Some of the particularly interesting points include:&ndash; China has an incentive to automate its own industries to improve labor productivity, as its labor pool has peaked and is now in similar demographic-based decline as other developed economies.&ndash; The supply of AI technology seems to come from the United States and China, with Europe lagging.&ndash; &ldquo;A large effect is actually job reorganization. Companies adopting this technology will have to reorganize the type of jobs they offer. How easy would it be to do that? Companies are going to have to reorganize the way they work to make sure they get the juice out of this technology.&rdquo;&ndash; India may struggle as it transitions tens to hundreds of millions of people out of agriculture jobs. &ldquo;We have to make this transition in an era where creating jobs out of manufacturing is going to be more challenging, simply because of automation playing a bigger role in several types of manufacturing.&rdquo;&ndash; Read more: How will automation affect economies around the world? (McKinsey Global Institute).
DeepChem 2.0 bubbles out of the lab:&hellip;Open source scientific computing platform gets its second major release&hellip;DeepChem&rsquo;s authors have released version 2.0 of the scientific computing library, bringing with it improvements to the TensorGraph API, tools for molecular analysis, new models, tutorial tweaks and adds, and a whole host of general improvements. DeepChem &ldquo;aims to provide a high quality open-source toolchain that democratizes the use of deep-learning in drug discovery, materials science, quantum chemistry, and biology.&rdquo;&nbsp; Read more: DeepChem 2.0 release notes.&nbsp; Read more: DeepChem website.
Google researchers tackle prefetching with neural networks:&hellip;First databases, now memory&hellip;One of the weirder potential futures of AI is one where the fundamental aspects of computing, like implementing systems that search over database indexes or prefetch data to boost performance, are mostly learned rather than pre-programmed. That&rsquo;s the idea in a new paper from researchers at Google, which tries to use machine learning techniques to solve prefetching, which is &ldquo;the process of predicting future memory accesses that will miss in the on-chip cache and access memory based on past history&rdquo;. Prefetching is a somewhat fundamental problem, as the better one becomes at prefetching, the higher the chance of being able to better intuit which data to load-in to memory before it is called upon, which increases the performance of your system.&nbsp; How it works: Can prefetching be learned? &ldquo;Prefetching is fundamentally a regression problem. The output space, however, is both vast and extremely sparse, making it a poor fit for standard regression models,&rdquo; the Google researchers write. Instead, they turn to using LSTMs and find that two variants are able to demonstrate competitive prefetching performance when compared to handwritten systems. &ldquo;The first version is analogous to a standard language model, while the second exploits the structure of the memory access space in order to reduce the vocabulary size and reduce the model memory footprint,&rdquo; the researchers write. They test out their approach on data from Google&rsquo;s web search workload and demonstrate competitive performance.&nbsp; &ldquo;The models described in this paper demonstrate significantly higher precision and recall than table-based approaches. This study also motivates a rich set of questions that this initial exploration does not solve, and we leave these for future research,&rdquo; they write. This research is philosophically similar to work from Google last autumn in using neural networks to learn database index structures (covered in #73), which also found that you could learn indexes that had competitive to superior performance to hand-tuned systems.&nbsp;&nbsp;One weird thing: When developing one of their LSTMs the researchers created a t-SNE embedding of the program counters ingested by the system and discovered that the learned features contained quite a lot of information. &ldquo;The t-SNE results also indicate that an interesting view of memory access traces is that they are a reflection of program behavior. A trace representation is necessarily different from e.g., input-output pairs of functions, as in particular, traces are a repre…"

---

### Import AI: #85: Keeping it simple with temporal convolutional networks instead of RNNs, learning to prefetch with neural nets, and India’s automation challenge.

Administrative note: a somewhat abbreviated issue this week as I&rsquo;ve been traveling quite a lot and have chosen sleep above reading papers (gasp!).
It&rsquo;s simpler than you think: researchers show convolutional networks frequently beat recurrent ones:&hellip;The rise and rise of simplistic techniques continues&hellip;Researchers with Carnegie Mellon University &nbsp;and Intel Labs have rigorously tested the capabilities of convolutional neural networks (via a &lsquo;temporal convolutional network&rsquo; (TCN) architecture, inspired by Wavenet and other recent innovations) against sequence modeling architectures like Recurrent Nets (via LSTMs and GRUs). The advantages of TCNs for sequence modeling are as follows: easily parallelizable rather than relying on sequential processing; a flexible receptive field size; stable gradients; low memory requirements for training; and variable length inputs. Disadvantages include: a greater data storage need than RNNs; parameters need to be fiddled with when shifting into different data domains.&nbsp;&nbsp;Testing: The researchers test out TCNs against RNNS, GRUs, and LSTMs on a variety of sequence modeling tasks, ranging from MNIST, to adding and copy tasks, to word-level and character-level perplexity on language tasks. In nine out of eleven cases the TCN comes out far ahead of other techniques, in one of the eleven cases it roughly matches GRU performance, and in another case it is noticeably worse then an LSTM (though still comes in second).&nbsp; What happens now: &ldquo;The preeminence enjoyed by recurrent networks in sequence modeling may be largely a vestige of history. Until recently, before the introduction of architectural elements such as dilated convolutions and residual connections, convolutional architectures were indeed weaker. Our results indicate that with these elements, a simple convolutional architecture is more effective across diverse sequence modeling tasks than recurrent architectures such as LSTMs. Due to the comparable clarity and simplicity of TCNs, we conclude that convolutional networks should be regarded as a natural starting point and a powerful toolkit for sequence modeling,&rdquo; write the researchers.&nbsp;&nbsp;Why it matters: One of the most confusing things about machine learning is that it&rsquo;s a defiantly empirical science, with new techniques appearing and proliferating in response to measured performance on given tasks. What studies like this indicate is that many of these new architectures could be overly complex relative to their utility and it&rsquo;s likely that, with just a few tweaks, the basic building blocks still reign supreme; we&rsquo;ve seen a similar phenomenon with basic LSTMs and GANs doing better than many other more-recent innovations, given thorough analysis. In one sense this seems good as it seems intuitive that simpler architectures tend to be more flexible and general, and in another sense it&rsquo;s unnerving, as it suggests much of the complexity that abounds in AI is an artifact of empirical science rather than theoretically justified.&nbsp; Read more: An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling (Arxiv).&nbsp;&nbsp;Code for the TCN used in the experiments here (GitHub).Automation &amp; economies: it&rsquo;s complicated:&hellip;Where AI technology comes from, why automation could be challenging for India, and more&hellip;In a podcast, three employees of the McKinsey Global Institute discuss how automation will impact China, Europe, and India. Some of the particularly interesting points include:&ndash; China has an incentive to automate its own industries to improve labor productivity, as its labor pool has peaked and is now in similar demographic-based decline as other developed economies.&ndash; The supply of AI technology seems to come from the United States and China, with Europe lagging.&ndash; &ldquo;A large effect is actually job reorganization. Companies adopting this technology will have to reorganize the type of jobs they offer. How easy would it be to do that? Companies are going to have to reorganize the way they work to make sure they get the juice out of this technology.&rdquo;&ndash; India may struggle as it transitions tens to hundreds of millions of people out of agriculture jobs. &ldquo;We have to make this transition in an era where creating jobs out of manufacturing is going to be more challenging, simply because of automation playing a bigger role in several types of manufacturing.&rdquo;&ndash; Read more: How will automation affect economies around the world? (McKinsey Global Institute).
DeepChem 2.0 bubbles out of the lab:&hellip;Open source scientific computing platform gets its second major release&hellip;DeepChem&rsquo;s authors have released version 2.0 of the scientific computing library, bringing with it improvements to the TensorGraph API, tools for molecular analysis, new models, tutorial tweaks and adds, and a whole host of general improvements. DeepChem &ldquo;aims to provide a high quality open-source toolchain that democratizes the use of deep-learning in drug discovery, materials science, quantum chemistry, and biology.&rdquo;&nbsp; Read more: DeepChem 2.0 release notes.&nbsp; Read more: DeepChem website.
Google researchers tackle prefetching with neural networks:&hellip;First databases, now memory&hellip;One of the weirder potential futures of AI is one where the fundamental aspects of computing, like implementing systems that search over database indexes or prefetch data to boost performance, are mostly learned rather than pre-programmed. That&rsquo;s the idea in a new paper from researchers at Google, which tries to use machine learning techniques to solve prefetching, which is &ldquo;the process of predicting future memory accesses that will miss in the on-chip cache and access memory based on past history&rdquo;. Prefetching is a somewhat fundamental problem, as the better one becomes at prefetching, the higher the chance of being able to better intuit which data to load-in to memory before it is called upon, which increases the performance of your system.&nbsp; How it works: Can prefetching be learned? &ldquo;Prefetching is fundamentally a regression problem. The output space, however, is both vast and extremely sparse, making it a poor fit for standard regression models,&rdquo; the Google researchers write. Instead, they turn to using LSTMs and find that two variants are able to demonstrate competitive prefetching performance when compared to handwritten systems. &ldquo;The first version is analogous to a standard language model, while the second exploits the structure of the memory access space in order to reduce the vocabulary size and reduce the model memory footprint,&rdquo; the researchers write. They test out their approach on data from Google&rsquo;s web search workload and demonstrate competitive performance.&nbsp; &ldquo;The models described in this paper demonstrate significantly higher precision and recall than table-based approaches. This study also motivates a rich set of questions that this initial exploration does not solve, and we leave these for future research,&rdquo; they write. This research is philosophically similar to work from Google last autumn in using neural networks to learn database index structures (covered in #73), which also found that you could learn indexes that had competitive to superior performance to hand-tuned systems.&nbsp;&nbsp;One weird thing: When developing one of their LSTMs the researchers created a t-SNE embedding of the program counters ingested by the system and discovered that the learned features contained quite a lot of information. &ldquo;The t-SNE results also indicate that an interesting view of memory access traces is that they are a reflection of program behavior. A trace representation is necessarily different from e.g., input-output pairs of functions, as in particular, traces are a repre…