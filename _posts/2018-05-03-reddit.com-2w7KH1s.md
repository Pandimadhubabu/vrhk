---

layout: post
category: threads
title: "[D] Fake gradients for activation functions"
date: 2018-05-03 15:08:11
link: https://vrhk.co/2w7KH1s
image: https://www.redditstatic.com/icon.png
domain: reddit.com
author: "reddit"
icon: http://www.redditstatic.com/desktop2x/img/favicon/apple-icon-57x57.png
excerpt: "Is there any theoretical reason that the error derivatives of an activation function have to be related to the exact derivative of that function..."

---

### [D] Fake gradients for activation functions â€¢ r/MachineLearning

Is there any theoretical reason that the error derivatives of an activation function have to be related to the exact derivative of that function...