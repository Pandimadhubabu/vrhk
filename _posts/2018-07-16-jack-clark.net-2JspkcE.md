---

layout: post
category: product
title: "Import AI: #103: Testing brain-like alternatives to backpropagation, why imagining goals can lead to better robots, and why navigating cities is a useful research avenue for AI"
date: 2018-07-16 16:42:15
link: https://vrhk.co/2JspkcE
image: 
domain: jack-clark.net
author: "Jack Clark"
icon: https://s2.wp.com/i/webclip.png
excerpt: "Backpropagation may not be brain-like, but at least it works:&hellip;Researchers test more brain-like approaches to learning systems, discover that backpropagation is hard to beat&hellip;Backpropagation is one of the fundamental tools of modern deep learning &ndash; it&rsquo;s one of the key mechanisms for propagating and updating information through networks during training. Unfortunately, there&rsquo;s relatively little evidence available that our own human brains perform a process analogous to backpropagation (this is a question Geoff Hinton has struggled with for several years in talks like &lsquo;Can the brain do back-propagation&lsquo;?). That has given some concern to researchers for some years who worry that though we&rsquo;re seeing significant gains from developing things based on backpropagation, we may need to investigate other approaches in the future. &nbsp;Now, researchers with Google Brain and the University of Toronto have performed an empirical analysis of a range of fundamental learning algorithms, testing approaches based on backpropagation against ones using target propagation and other variants. &nbsp;&nbsp;Motivation: The idea behind this research is that &ldquo;there is a need for behavioural realism, in addition to physiological realism, when gathering evidence to assess the overall biological realism of a learning algorithm. Given that human beings are able to learn complex tasks that bear little relationship to their evolution, it would appear that the brain possesses a powerful, general-purpose learning algorithm for shaping behavior&rdquo;.&nbsp; Results: The researchers &ldquo;find that none of the tested algorithms are capable of effectively scaling up to training large networks on ImageNet&rdquo;, though they record some success with MNIST and CIFAR. &ldquo;Out-of-the-box application of this class of algorithms does not provide a straightforward solution to real data on even moderately large networks,&rdquo; they write. &nbsp;&nbsp;&nbsp;Why it matters: Given that we know how limited and simplified our neural network systems are, it seems intellectually honest to test and ablate algorithms, particularly by comparing well-studied &lsquo;mainstream&rsquo; approaches like backpropagation with more theoretically-grounded but less-developed algorithms from other parts of the literature.&nbsp; Read more: Assessing the Scalability of Biologically-Motivated Deep Learning Algorithms and Architectures (Arxiv).
AI and Silent Bugs:&hellip;Half-decade old bug in &lsquo;Aliens&rsquo; game found responsible for poor performance&hellip;One of the more irritating things about developing AI systems is that when you mis-program AI it tends to fail silently &ndash; for instance, in OpenAI&rsquo;s Dota project we saw performance dramatically increase simply after fixing non-breaking bugs. Another good example of this phenomenon has turned up in news about Aliens: Colonial Marines, a poorly reviewed half-decade-old game. But it turns out some of the reasons for those poor reviews were likely due to a bug &ndash; subsequent patches have found that the original game mis-named one variable which lead to entire chunks of the game&rsquo;s enemy AI systems not functioning. &nbsp;&nbsp;Read more: A years-old, one-letter typo led to Aliens: Colonial Marines&rsquo; weird AI (Ars Technica).
Berkeley researchers teach machines to dream imaginary goals and solutions for better RL:&hellip;If you want to change the world, first imagine yourself changing it&hellip;Berkeley researchers have developed a way for machines to develop richer representations of the world around them and use this to solve tasks. The method they use to achieve this is a technique called &lsquo;reinforcement learning with imagined goals&rsquo; (RIG). RIG works like this: an AI system interacts with an environment, data from these observations is used to train (and finetune) a variational auto encoder (VAE) latent variable model, then they use this representation to train the AI system to solve different imagined tasks using the representation learned by the VAE. This type of approach is becoming increasingly popular as AI researchers try to increase the capabilities of algorithms by getting them to use and learn from more data.&nbsp; Results: Their approach does well at tasks requiring reaching objects and pushing objects to a goal, beating baselines including algorithms like Hindsight Experience Replay (HER). &nbsp;&nbsp;Why it matters: After spending several years training algorithms to master an environment, we&rsquo;re now trying to train algorithms that can represent their environment, then use that representation as an input to the algorithm to help it solve a new task. This is part of a general push toward greater representative capacity within trained models.&nbsp; Read more: Visual Reinforcement Learning with Imagined Goals (Arxiv).
Facebook thinks the path to smarter AI involves guiding other AIs through cities:&hellip;&rsquo;Talk The Walk&rsquo; task challenges AIs to navigate each other through cities, working as a team&hellip;Have you ever tried giving directions to someone over the phone? It can be quite difficult, and usually involves a series of dialogues between you and the person as you try to figure out where in the city they are in relation to where they need to get to. Now, researchers with Facebook and the Montreal Institute of Learning Algorithms (MILA) have set out to develop and test AIs that can solve this task, so as to further improve the generalization capabilities of AI agents. &ldquo;&rdquo;For artificial agents to solve this challenging problem, some fundamental architecture designs are missing,&rdquo; the researchers say.&nbsp; The challenge: The new &ldquo;Talk The Walk&rdquo; task frames the problem as a discussion between a &lsquo;guide&rsquo; and a &lsquo;tourist&rsquo; agent. The guide agent has access to a map of the city area that the tourist is in, as well as a location the tourist wants to get to, and the tourist has access to an annotated image of their current location along with the ability to turn left, turn right, or move forward. &nbsp;&nbsp;The dataset: The researchers created the testing environment by obtaining 360-degree photographic views of neighborhoods in New York City, including Hell&rsquo;s Kitchen, the East VIllage, Williamsburg, the Financial District, and the Upper East Side. They then annotated each image of each corner of each street intersection with a set of landmarks drawn from the following categories: bar, bank, shop, coffee shop, theater, playfield, hotel, subway, and restaurant. They then had more than six hundred users of Mechanical Turk play a human version of the game, generating 10,000 successful dialogues from which AI systems can be trained (with over 2,000 successful dialogues available for each neighborhood of New York the researchers gathered data for). &nbsp;&nbsp;Results: The researchers tested their developed systems at how well they can localize themselves &ndash; that is, develop a notion of where they are in the city. The results are encouraging, with localization models developed by the researchers achieving a higher localization score than humans. (Though humans take about half the number of steps to effectively localize themselves, showing that human sample efficiency remains substantially better than those of machines. &nbsp;&nbsp;Why it matters: Following a half decade of successful development and commercialization of basis AI capabilities like image and audio processing, researchers are trying to come up with the next major tasks and datasets they can use to test contemporary research algorithms and developing them further. Evaluation methods like those devised here can help us develop AI systems which need to interact with larger amounts of real world data, potentially making it easier to evaluate how &lsquo;intelligent&rsquo; these systems are becoming, as they are being tested directly …"

---

### Import AI: #103: Testing brain-like alternatives to backpropagation, why imagining goals can lead to better robots, and why navigating cities is a useful research avenue for AI

Backpropagation may not be brain-like, but at least it works:&hellip;Researchers test more brain-like approaches to learning systems, discover that backpropagation is hard to beat&hellip;Backpropagation is one of the fundamental tools of modern deep learning &ndash; it&rsquo;s one of the key mechanisms for propagating and updating information through networks during training. Unfortunately, there&rsquo;s relatively little evidence available that our own human brains perform a process analogous to backpropagation (this is a question Geoff Hinton has struggled with for several years in talks like &lsquo;Can the brain do back-propagation&lsquo;?). That has given some concern to researchers for some years who worry that though we&rsquo;re seeing significant gains from developing things based on backpropagation, we may need to investigate other approaches in the future. &nbsp;Now, researchers with Google Brain and the University of Toronto have performed an empirical analysis of a range of fundamental learning algorithms, testing approaches based on backpropagation against ones using target propagation and other variants. &nbsp;&nbsp;Motivation: The idea behind this research is that &ldquo;there is a need for behavioural realism, in addition to physiological realism, when gathering evidence to assess the overall biological realism of a learning algorithm. Given that human beings are able to learn complex tasks that bear little relationship to their evolution, it would appear that the brain possesses a powerful, general-purpose learning algorithm for shaping behavior&rdquo;.&nbsp; Results: The researchers &ldquo;find that none of the tested algorithms are capable of effectively scaling up to training large networks on ImageNet&rdquo;, though they record some success with MNIST and CIFAR. &ldquo;Out-of-the-box application of this class of algorithms does not provide a straightforward solution to real data on even moderately large networks,&rdquo; they write. &nbsp;&nbsp;&nbsp;Why it matters: Given that we know how limited and simplified our neural network systems are, it seems intellectually honest to test and ablate algorithms, particularly by comparing well-studied &lsquo;mainstream&rsquo; approaches like backpropagation with more theoretically-grounded but less-developed algorithms from other parts of the literature.&nbsp; Read more: Assessing the Scalability of Biologically-Motivated Deep Learning Algorithms and Architectures (Arxiv).
AI and Silent Bugs:&hellip;Half-decade old bug in &lsquo;Aliens&rsquo; game found responsible for poor performance&hellip;One of the more irritating things about developing AI systems is that when you mis-program AI it tends to fail silently &ndash; for instance, in OpenAI&rsquo;s Dota project we saw performance dramatically increase simply after fixing non-breaking bugs. Another good example of this phenomenon has turned up in news about Aliens: Colonial Marines, a poorly reviewed half-decade-old game. But it turns out some of the reasons for those poor reviews were likely due to a bug &ndash; subsequent patches have found that the original game mis-named one variable which lead to entire chunks of the game&rsquo;s enemy AI systems not functioning. &nbsp;&nbsp;Read more: A years-old, one-letter typo led to Aliens: Colonial Marines&rsquo; weird AI (Ars Technica).
Berkeley researchers teach machines to dream imaginary goals and solutions for better RL:&hellip;If you want to change the world, first imagine yourself changing it&hellip;Berkeley researchers have developed a way for machines to develop richer representations of the world around them and use this to solve tasks. The method they use to achieve this is a technique called &lsquo;reinforcement learning with imagined goals&rsquo; (RIG). RIG works like this: an AI system interacts with an environment, data from these observations is used to train (and finetune) a variational auto encoder (VAE) latent variable model, then they use this representation to train the AI system to solve different imagined tasks using the representation learned by the VAE. This type of approach is becoming increasingly popular as AI researchers try to increase the capabilities of algorithms by getting them to use and learn from more data.&nbsp; Results: Their approach does well at tasks requiring reaching objects and pushing objects to a goal, beating baselines including algorithms like Hindsight Experience Replay (HER). &nbsp;&nbsp;Why it matters: After spending several years training algorithms to master an environment, we&rsquo;re now trying to train algorithms that can represent their environment, then use that representation as an input to the algorithm to help it solve a new task. This is part of a general push toward greater representative capacity within trained models.&nbsp; Read more: Visual Reinforcement Learning with Imagined Goals (Arxiv).
Facebook thinks the path to smarter AI involves guiding other AIs through cities:&hellip;&rsquo;Talk The Walk&rsquo; task challenges AIs to navigate each other through cities, working as a team&hellip;Have you ever tried giving directions to someone over the phone? It can be quite difficult, and usually involves a series of dialogues between you and the person as you try to figure out where in the city they are in relation to where they need to get to. Now, researchers with Facebook and the Montreal Institute of Learning Algorithms (MILA) have set out to develop and test AIs that can solve this task, so as to further improve the generalization capabilities of AI agents. &ldquo;&rdquo;For artificial agents to solve this challenging problem, some fundamental architecture designs are missing,&rdquo; the researchers say.&nbsp; The challenge: The new &ldquo;Talk The Walk&rdquo; task frames the problem as a discussion between a &lsquo;guide&rsquo; and a &lsquo;tourist&rsquo; agent. The guide agent has access to a map of the city area that the tourist is in, as well as a location the tourist wants to get to, and the tourist has access to an annotated image of their current location along with the ability to turn left, turn right, or move forward. &nbsp;&nbsp;The dataset: The researchers created the testing environment by obtaining 360-degree photographic views of neighborhoods in New York City, including Hell&rsquo;s Kitchen, the East VIllage, Williamsburg, the Financial District, and the Upper East Side. They then annotated each image of each corner of each street intersection with a set of landmarks drawn from the following categories: bar, bank, shop, coffee shop, theater, playfield, hotel, subway, and restaurant. They then had more than six hundred users of Mechanical Turk play a human version of the game, generating 10,000 successful dialogues from which AI systems can be trained (with over 2,000 successful dialogues available for each neighborhood of New York the researchers gathered data for). &nbsp;&nbsp;Results: The researchers tested their developed systems at how well they can localize themselves &ndash; that is, develop a notion of where they are in the city. The results are encouraging, with localization models developed by the researchers achieving a higher localization score than humans. (Though humans take about half the number of steps to effectively localize themselves, showing that human sample efficiency remains substantially better than those of machines. &nbsp;&nbsp;Why it matters: Following a half decade of successful development and commercialization of basis AI capabilities like image and audio processing, researchers are trying to come up with the next major tasks and datasets they can use to test contemporary research algorithms and developing them further. Evaluation methods like those devised here can help us develop AI systems which need to interact with larger amounts of real world data, potentially making it easier to evaluate how &lsquo;intelligent&rsquo; these systems are becoming, as they are being tested directly …