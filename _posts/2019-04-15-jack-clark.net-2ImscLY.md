---

layout: post
category: product
title: "Import AI 142: Import AI 142: Berkeley spawns cheap ‘BLUE’ arm; Google trains neural nets to prove math theorems; seven questions about GANs"
date: 2019-04-15 16:46:38
link: https://vrhk.co/2ImscLY
image: 
domain: jack-clark.net
author: "Jack Clark"
icon: https://s2.wp.com/i/webclip.png
excerpt: "Google reveals HOList, a platform for doing theorem proving research with deep learning-based methods:&hellip;In the future, perhaps more math theorems will be proved by AI systems than humans&hellip;Researchers with Google want to develop and test AI systems that can learn to solve mathematical theorems, so have made tweaks to theorem proving software to make it easier for AI systems to interface with. In addition, they&rsquo;ve created a new theorem proving benchmark to spur development in this part of AI.
HOL List: The software they base their system on is called HOL Light. For this project, they develop &ldquo;an instrumented, pre-packaged version of HOL Light that can be used as a large scale distributed environment of reinforcement learning for practical theorem proving using our new, well-defined, stable Python API&rdquo;. This software ships with 41 &ldquo;tactics&rdquo; which are basically algorithms to use to help prove math theorems.
Benchmarks: The researchers have also released a new benchmark on HOL Light, and they hope this will &ldquo;enable research and measuring progress of AI driven theorem proving in large theories&rdquo;. The benchmarks are initially designed to measure performance on a few tasks, including: predicting the same methodologies used by humans to create a proof; and trying to prove certain subgoals or aspects of proofs without access to full information.
DeepHOL: They design a neural network-based theorem prover called DeepHOL which tries to concurrently encode the goals and premises while generating a proof. &ldquo;In essence, we propose a hybrid architecture that both predicts the correct tactic to be applied, as well as rank the premise parameters required for meaningful application of tactics&rdquo;. They test out a variety of different neural network-based approaches within this overall architecture and train them via reinforcement learning, with the best system able to prove 58% of the proofs in the training set &ndash; no slam-dunk, but very encouraging considering these are learning-based methods.
Why this matters: Theorem proving feels like a very promising way to test the capabilities of increasingly advanced machines, especially if we&rsquo;re able to develop systems that start to generate new proofs. This would be a clear validation of the ability for AI systems to create novel scientific insights in a specific domain, and I suspect would give us better intuitions about AI&rsquo;s ability to transform science more generally as well. &nbsp;&ldquo;We hope that our initial effort fosters collaboration and paves the way for strong and practical AI systems that can learn to reason efficiently in large formal theories,&rdquo; they write. &nbsp;&nbsp;Read more: HOList: An Environment for Machine Learning of Higher-Order Theorem Proving (Extended Version).
#####################################################
Think GANs are interesting? Here are seven underexplored questions:&hellip;Googler searches for the things we know we don&rsquo;t know&hellip;Generative adversarial networks have become a mainstay component of recent AI research given their utility in creative applications, where you need to teach a neural network about some data well enough that it can generate synthetic data that looks similar to the source, whether videos or images or audio.
But GANs are quite poorly understood, so researcher Augustus Odena has published an essay on Distill listing seven open questions about GANs.
The seven questions: These are: &ndash; What are the trade-offs between GANs and other generative models?&ndash; What sorts of distributions can GANs model?&ndash; How can we scale GANs beyond image synthesis?&ndash; What can we say about the global convergence of the training dynamics?&ndash; How should we evaluate GANs and when should we use them?&ndash; How does GAN training scale with batch size?&ndash; What is the relationship between GANs and adversarial examples?Why this matters: Better understanding how to answer these questions will help researchers better understand the technology, which will allow us to make better predictions about economics costs of training GAN systems, likely failures to expect, and point to future directions for work. It&rsquo;s refreshing to see researchers publish exclusively about the problems and questions related to a technique, and I hope to see more scholarship like this.  &nbsp;&nbsp;Read more: Open Questions about Generative Adversarial Networks (Distill).#####################################################Human doctors get better with aid of AI-based diagnosis system:&hellip;MRNet dataset, competition, and research, should spur research into aiding clinicians with pre-trained medical-problem-spotting neural nets&hellip;Stanford University researchers have developed a neural network-based technique to assess Knee MR scans for abnormalities and a few specific diagnoses (eg, ligament tears). They find that clinicians which have access to this model have a lower rate of mistaken diagnoses than those without access to it. When using this model &ldquo;for every 100 healthy patients, ~5 are saved from being unnecessarily considered for surgery,&rdquo; they write.
MRNet dataset: Along with their research, they&rsquo;ve also released an underlying dataset: MRNet, a collection of 1,370 knee MRI exams performed at Stanford University Medical Center, spread across normal and abnormal knees. Competition: &ldquo;We are hosting a competition to encourage others to develop models for automated interpretation of knee MRs,&rdquo; the researchers write. &ldquo;Our test set (called internal validation set in the paper) has its ground truth set using the majority vote of 3 practicing board-certified MSK radiologists&rdquo;.
Why this matters: Many AI systems are going to augment rather than substitute for human skills, and I expect this to be especially frequent in medicine, where we can expect to give clinicians more and more AI advisor systems to use when making diagnoses. In addition, datasets are crucial to the development of more sophisticated medical AI systems and competitions tend to drive attention towards a specific problem &ndash; so the release of both in addition to the paper should spur research in this area. &nbsp;&nbsp;Read more and register to download the dataset here: MRNet Dataset (Stanford ML Group).&nbsp; Read more about the underlying research: MRNet: Deep-learning-assisted diagnosis for knee magnetic resonance imaging (Stanford ML Group).
#####################################################
As AI hype fades, applications arrive:&hellip;Now we&rsquo;ve got to superhuman performance we need to work on human-computer interaction&hellip;Jeffrey Bigham, a human-computer interaction researcher, thinks that AI is heading into an era of less hype &ndash; and that&rsquo;s a good thing. This &lsquo;AI autumn&rsquo; is a natural successor to the period we&rsquo;re currently in, since we&rsquo;re moving from the development to the deployment phase of many AI technologies.
Goodbye hype, hello applications: &ldquo;Hype deflates when humans are considered,&rdquo; Bigham writes. &ldquo;Self-driving cars seem much less possible when you think about all the things human drivers do in addition to the driving on well-known roads in good lighting conditions. They find passengers, they get gas, they fix the car sometimes, they make sure drunk passengers aren&rsquo;t in danger, they walk elderly passengers into the hospital, etc&rdquo;.
Why this matters: &ldquo;If hype is at the rapidly melting tip of the iceberg, then the great human-centered applied work is the super large mass floating underneath supporting everything,&rdquo; he writes. And, as most people know, working with humans is challenging and endlessly surprising, so the true test of AI capabilities will be to first reach human parity at certain things, then be deployed in ways that make sense to humans.&nbsp; Read more: The Coming AI Autumn …"

---

### Import AI 142: Import AI 142: Berkeley spawns cheap ‘BLUE’ arm; Google trains neural nets to prove math theorems; seven questions about GANs

Google reveals HOList, a platform for doing theorem proving research with deep learning-based methods:&hellip;In the future, perhaps more math theorems will be proved by AI systems than humans&hellip;Researchers with Google want to develop and test AI systems that can learn to solve mathematical theorems, so have made tweaks to theorem proving software to make it easier for AI systems to interface with. In addition, they&rsquo;ve created a new theorem proving benchmark to spur development in this part of AI.
HOL List: The software they base their system on is called HOL Light. For this project, they develop &ldquo;an instrumented, pre-packaged version of HOL Light that can be used as a large scale distributed environment of reinforcement learning for practical theorem proving using our new, well-defined, stable Python API&rdquo;. This software ships with 41 &ldquo;tactics&rdquo; which are basically algorithms to use to help prove math theorems.
Benchmarks: The researchers have also released a new benchmark on HOL Light, and they hope this will &ldquo;enable research and measuring progress of AI driven theorem proving in large theories&rdquo;. The benchmarks are initially designed to measure performance on a few tasks, including: predicting the same methodologies used by humans to create a proof; and trying to prove certain subgoals or aspects of proofs without access to full information.
DeepHOL: They design a neural network-based theorem prover called DeepHOL which tries to concurrently encode the goals and premises while generating a proof. &ldquo;In essence, we propose a hybrid architecture that both predicts the correct tactic to be applied, as well as rank the premise parameters required for meaningful application of tactics&rdquo;. They test out a variety of different neural network-based approaches within this overall architecture and train them via reinforcement learning, with the best system able to prove 58% of the proofs in the training set &ndash; no slam-dunk, but very encouraging considering these are learning-based methods.
Why this matters: Theorem proving feels like a very promising way to test the capabilities of increasingly advanced machines, especially if we&rsquo;re able to develop systems that start to generate new proofs. This would be a clear validation of the ability for AI systems to create novel scientific insights in a specific domain, and I suspect would give us better intuitions about AI&rsquo;s ability to transform science more generally as well. &nbsp;&ldquo;We hope that our initial effort fosters collaboration and paves the way for strong and practical AI systems that can learn to reason efficiently in large formal theories,&rdquo; they write. &nbsp;&nbsp;Read more: HOList: An Environment for Machine Learning of Higher-Order Theorem Proving (Extended Version).
#####################################################
Think GANs are interesting? Here are seven underexplored questions:&hellip;Googler searches for the things we know we don&rsquo;t know&hellip;Generative adversarial networks have become a mainstay component of recent AI research given their utility in creative applications, where you need to teach a neural network about some data well enough that it can generate synthetic data that looks similar to the source, whether videos or images or audio.
But GANs are quite poorly understood, so researcher Augustus Odena has published an essay on Distill listing seven open questions about GANs.
The seven questions: These are: &ndash; What are the trade-offs between GANs and other generative models?&ndash; What sorts of distributions can GANs model?&ndash; How can we scale GANs beyond image synthesis?&ndash; What can we say about the global convergence of the training dynamics?&ndash; How should we evaluate GANs and when should we use them?&ndash; How does GAN training scale with batch size?&ndash; What is the relationship between GANs and adversarial examples?Why this matters: Better understanding how to answer these questions will help researchers better understand the technology, which will allow us to make better predictions about economics costs of training GAN systems, likely failures to expect, and point to future directions for work. It&rsquo;s refreshing to see researchers publish exclusively about the problems and questions related to a technique, and I hope to see more scholarship like this.  &nbsp;&nbsp;Read more: Open Questions about Generative Adversarial Networks (Distill).#####################################################Human doctors get better with aid of AI-based diagnosis system:&hellip;MRNet dataset, competition, and research, should spur research into aiding clinicians with pre-trained medical-problem-spotting neural nets&hellip;Stanford University researchers have developed a neural network-based technique to assess Knee MR scans for abnormalities and a few specific diagnoses (eg, ligament tears). They find that clinicians which have access to this model have a lower rate of mistaken diagnoses than those without access to it. When using this model &ldquo;for every 100 healthy patients, ~5 are saved from being unnecessarily considered for surgery,&rdquo; they write.
MRNet dataset: Along with their research, they&rsquo;ve also released an underlying dataset: MRNet, a collection of 1,370 knee MRI exams performed at Stanford University Medical Center, spread across normal and abnormal knees. Competition: &ldquo;We are hosting a competition to encourage others to develop models for automated interpretation of knee MRs,&rdquo; the researchers write. &ldquo;Our test set (called internal validation set in the paper) has its ground truth set using the majority vote of 3 practicing board-certified MSK radiologists&rdquo;.
Why this matters: Many AI systems are going to augment rather than substitute for human skills, and I expect this to be especially frequent in medicine, where we can expect to give clinicians more and more AI advisor systems to use when making diagnoses. In addition, datasets are crucial to the development of more sophisticated medical AI systems and competitions tend to drive attention towards a specific problem &ndash; so the release of both in addition to the paper should spur research in this area. &nbsp;&nbsp;Read more and register to download the dataset here: MRNet Dataset (Stanford ML Group).&nbsp; Read more about the underlying research: MRNet: Deep-learning-assisted diagnosis for knee magnetic resonance imaging (Stanford ML Group).
#####################################################
As AI hype fades, applications arrive:&hellip;Now we&rsquo;ve got to superhuman performance we need to work on human-computer interaction&hellip;Jeffrey Bigham, a human-computer interaction researcher, thinks that AI is heading into an era of less hype &ndash; and that&rsquo;s a good thing. This &lsquo;AI autumn&rsquo; is a natural successor to the period we&rsquo;re currently in, since we&rsquo;re moving from the development to the deployment phase of many AI technologies.
Goodbye hype, hello applications: &ldquo;Hype deflates when humans are considered,&rdquo; Bigham writes. &ldquo;Self-driving cars seem much less possible when you think about all the things human drivers do in addition to the driving on well-known roads in good lighting conditions. They find passengers, they get gas, they fix the car sometimes, they make sure drunk passengers aren&rsquo;t in danger, they walk elderly passengers into the hospital, etc&rdquo;.
Why this matters: &ldquo;If hype is at the rapidly melting tip of the iceberg, then the great human-centered applied work is the super large mass floating underneath supporting everything,&rdquo; he writes. And, as most people know, working with humans is challenging and endlessly surprising, so the true test of AI capabilities will be to first reach human parity at certain things, then be deployed in ways that make sense to humans.&nbsp; Read more: The Coming AI Autumn …