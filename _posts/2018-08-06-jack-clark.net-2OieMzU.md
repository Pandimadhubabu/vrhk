---

layout: post
category: product
title: "Import AI 106: Tencent breaks ImageNet training record with 1000+ GPUs; augmenting the Oxford RobotCar dataset; and PAI adds more members"
date: 2018-08-06 16:57:16
link: https://vrhk.co/2OieMzU
image: 
domain: jack-clark.net
author: "Jack Clark"
icon: https://s2.wp.com/i/webclip.png
excerpt: "What takes 2048 GPUs, takes 4 minutes to train, and can identify a seatbelt with 75% accuracy? &nbsp;Tencent&rsquo;s new deep learning model:&hellip;Ultrafast training thanks to LARS, massive batch sizes, and a field of GPUS&hellip;As supervised learning techniques become more economically valuable, researchers are trying to reduce the time it takes to train deep learning models so that they can run more experiments within a given time period, and therefore increase both the cadence of their internal research efforts, as well as their ability to train new models to account for new data inputs or shifts in existing data distributions. One metric that has emerged as being important here is the time it takes people to train networks on the &lsquo;ImageNet&rsquo; dataset to a baseline accuracy. Now, researchers with Chinese mega-tech company Tencent and Hong Kong Baptist University have shown how to use 2048 GPUs, a 64k batch-size (this is absolutely massive, for those who don&rsquo;t follow this stuff regularly) to train a ResNet-50 model on ImageNet to a top-1 accuracy of 75.8% within 6.6 minutes, and AlexNet to 58.7% accuracy within 4 minutes.&nbsp;&nbsp;Training: To train this, the researchers developed a distributed deep learning training system called &lsquo;Jizhi&rsquo;, which uses tricks including opportunistic data pipelining; hybrid all-reduce; and a training model which incorporates model and variable management, along with optimizations like mixed-precision networks (training using half-precision to increase the amount of throughput ). The authors say one of the largest contributing factors to their results is their ability to use LARS (Layer-wise Adaptive Rate Scaling (Arxiv)) to opportunistically flip between 16- and 32-bit precision during training &ndash; they conduct an ablation study and find that a version trained without LARS gets a Top-1 Accuracy of 73.2%, compared to 76.2% for the version trained with LARS.&nbsp;&nbsp;Model architecture tweaks: The authors eliminate weight decay on the bias and batch normalization, and add batch normalization layers into AlexNet.&nbsp; Communication strategies: The researchers implement a number of tweaks to deal with the problems brought about due to the immense scale of their training infrastructure. To help them do this they use a few tweaks including &lsquo;tensor fusion&rsquo;, which lets them chunk up multiple small-size tensors together before running an all-reduce step; &lsquo;hierarchical all-reduce&rsquo;, which lets them group GPUs together and selectively reduce and broadcast to further increase efficiency; and &lsquo;hybrid All-reduce&rsquo;, which lets them flip between two different implementations of all-reduce according to whatever is most efficient at the time.&nbsp;&nbsp;Why it matters: Because deep learning is fundamentally an empirical discipline, in which scientists launch experiments, observe results, and use hard-won intuitions to re-configure hyperparameters and architectures and repeat the process, then computers are somewhat analogous to telescopes: the bigger the computer, the farther you may be able to see, as you&rsquo;re able to run a faster experimental loop at greater scales than other people. The race between large organizations to scale-up training will likely lead to many interesting research avenues, but it also risks bifurcating research into &ldquo;low compute&rdquo; and &ldquo;high compute&rdquo; environments &ndash; that could further widen the gulf between academia and industry, which could create problems in the future.&nbsp; Read more: Highly Scalable Deep Learning Training System with MIxed-Precision Training ImageNet in Four Minutes (Arxiv).
What&rsquo;s better than the Oxford RobotCar Dataset? An even more elaborate version of this dataset!&hellip;Researchers label 11,000 frames of data to help people build better self-driving cars&hellip;Researchers with Universita degli Studi Federico II in Naples and Oxford Brookes University in Oxford have augmented the Oxford RobotCar Dataset with many more labels designed specifically for training vision-based policies for self-driving cars. The new datasets is called READ, or the &ldquo;Road Event and Activity Detection&rdquo; dataset, and involves a large number of rich labels which have been applied to ~11,000 frames of data gathered from cameras on an autonomous NISSAN Leaf driven around Oxford, UK. The dataset labels include &ldquo;spatiotemporal actions performed not just by humans but by all road users, including cyclists, motor-bikers, drivers of vehicles large and small, and obviously pedestrians.&rdquo; These labels can be quite granular and individual agents in a scene, like a car, can have multiple labels applied to them (for instance, a car in front of the autonomous vehicle at an intersection might be tagged with &ldquo;indicating right&rdquo; and &ldquo;car stopped at the traffic light&rdquo;. Similarly, Cyclists could be tagged with labels like &ldquo;cyclist moving in lane&rdquo; and &ldquo;cyclist indicating left&rdquo;, and so on. This richness might help develop better detectors that can create more adaptable autonomous vehicles.&nbsp; Tools used: They used Microsoft&rsquo;s &lsquo;Visual Object Tagging Tool&rdquo; (VOTT) to annotate the dataset.&nbsp;&nbsp;Next steps: This version of READ is a preliminary one, and the scientists plan to eventually label 40,000 frames. They also have ambitious plans to create a novel, deep learning approach to detecting complex activities&rdquo;. Let&rsquo;s wish them luck.&nbsp; Why it matters: Autonomous cars are going to revolutionize many aspects of the world, but in recent years there has been a major push by industry to productize the technology, which has led to much of the research occurring in private. Academic research initiatives and associated dataset releases like this promise to make it easier for other people to develop this technology, potentially broadening our own understanding of it and letting more people participate in its development.&nbsp; Read more: Action Detection from a Robot-Car Perspective (Arxiv).
Whether rain, fog, or snow &ndash; researchers&rsquo; weather dataset has you covered:&hellip;RFS dataset taken from creative commons images&hellip;Researchers with the University of Essex and the University of Birmingham have created a new weather dataset called the Rain Fog Snow (RFS) dataset which researchers can use to better understand, classify and predict weather patterns.&nbsp; Dataset: The dataset consists of more than 3,000 images taken from websites like Flickr, Pixabay, Wikimedia Commons, and others, depicting images of scenes with different weather conditions, ranging from Rain to Fog to Snow. In total, the researchers gather 1100 images from each class, creating a potentially new useful dataset for researchers to experiment with.&nbsp; Read more: Weather Classification: A new multi-class dataset data augmentation approach and comprehensive evaluations of Convolutional Neural Networks (Arxiv).
DeepMind teaches computers to count:&hellip;Pairing deep learning with specific external modules leads to broadened capabilities&hellip;Neural networks are typically not very good at maths. That&rsquo;s because figuring out a way to train a neural network to develop a differentiable, numeric representation is difficult, with most work typically involving handing off the outputs of a neural network to a non-learned predominantly hand-programmed system. Now, DeepMind has implemented a couple of modules &mdash; a Neural Accumulator (NAC) and a Neural Arithmetic Logic Unit (NALU) &mdash; specifically to help its computers learn to count. These modules are &ldquo;biased to learn systematic numerical computation&rdquo;, write the authors of the research. &ldquo;Our strategy is to represent numerical quantities as individual neurons without a nonlinearity. To these single-value neurons, we apply operators that are capable of representing sim…"

---

### Import AI 106: Tencent breaks ImageNet training record with 1000+ GPUs; augmenting the Oxford RobotCar dataset; and PAI adds more members

What takes 2048 GPUs, takes 4 minutes to train, and can identify a seatbelt with 75% accuracy? &nbsp;Tencent&rsquo;s new deep learning model:&hellip;Ultrafast training thanks to LARS, massive batch sizes, and a field of GPUS&hellip;As supervised learning techniques become more economically valuable, researchers are trying to reduce the time it takes to train deep learning models so that they can run more experiments within a given time period, and therefore increase both the cadence of their internal research efforts, as well as their ability to train new models to account for new data inputs or shifts in existing data distributions. One metric that has emerged as being important here is the time it takes people to train networks on the &lsquo;ImageNet&rsquo; dataset to a baseline accuracy. Now, researchers with Chinese mega-tech company Tencent and Hong Kong Baptist University have shown how to use 2048 GPUs, a 64k batch-size (this is absolutely massive, for those who don&rsquo;t follow this stuff regularly) to train a ResNet-50 model on ImageNet to a top-1 accuracy of 75.8% within 6.6 minutes, and AlexNet to 58.7% accuracy within 4 minutes.&nbsp;&nbsp;Training: To train this, the researchers developed a distributed deep learning training system called &lsquo;Jizhi&rsquo;, which uses tricks including opportunistic data pipelining; hybrid all-reduce; and a training model which incorporates model and variable management, along with optimizations like mixed-precision networks (training using half-precision to increase the amount of throughput ). The authors say one of the largest contributing factors to their results is their ability to use LARS (Layer-wise Adaptive Rate Scaling (Arxiv)) to opportunistically flip between 16- and 32-bit precision during training &ndash; they conduct an ablation study and find that a version trained without LARS gets a Top-1 Accuracy of 73.2%, compared to 76.2% for the version trained with LARS.&nbsp;&nbsp;Model architecture tweaks: The authors eliminate weight decay on the bias and batch normalization, and add batch normalization layers into AlexNet.&nbsp; Communication strategies: The researchers implement a number of tweaks to deal with the problems brought about due to the immense scale of their training infrastructure. To help them do this they use a few tweaks including &lsquo;tensor fusion&rsquo;, which lets them chunk up multiple small-size tensors together before running an all-reduce step; &lsquo;hierarchical all-reduce&rsquo;, which lets them group GPUs together and selectively reduce and broadcast to further increase efficiency; and &lsquo;hybrid All-reduce&rsquo;, which lets them flip between two different implementations of all-reduce according to whatever is most efficient at the time.&nbsp;&nbsp;Why it matters: Because deep learning is fundamentally an empirical discipline, in which scientists launch experiments, observe results, and use hard-won intuitions to re-configure hyperparameters and architectures and repeat the process, then computers are somewhat analogous to telescopes: the bigger the computer, the farther you may be able to see, as you&rsquo;re able to run a faster experimental loop at greater scales than other people. The race between large organizations to scale-up training will likely lead to many interesting research avenues, but it also risks bifurcating research into &ldquo;low compute&rdquo; and &ldquo;high compute&rdquo; environments &ndash; that could further widen the gulf between academia and industry, which could create problems in the future.&nbsp; Read more: Highly Scalable Deep Learning Training System with MIxed-Precision Training ImageNet in Four Minutes (Arxiv).
What&rsquo;s better than the Oxford RobotCar Dataset? An even more elaborate version of this dataset!&hellip;Researchers label 11,000 frames of data to help people build better self-driving cars&hellip;Researchers with Universita degli Studi Federico II in Naples and Oxford Brookes University in Oxford have augmented the Oxford RobotCar Dataset with many more labels designed specifically for training vision-based policies for self-driving cars. The new datasets is called READ, or the &ldquo;Road Event and Activity Detection&rdquo; dataset, and involves a large number of rich labels which have been applied to ~11,000 frames of data gathered from cameras on an autonomous NISSAN Leaf driven around Oxford, UK. The dataset labels include &ldquo;spatiotemporal actions performed not just by humans but by all road users, including cyclists, motor-bikers, drivers of vehicles large and small, and obviously pedestrians.&rdquo; These labels can be quite granular and individual agents in a scene, like a car, can have multiple labels applied to them (for instance, a car in front of the autonomous vehicle at an intersection might be tagged with &ldquo;indicating right&rdquo; and &ldquo;car stopped at the traffic light&rdquo;. Similarly, Cyclists could be tagged with labels like &ldquo;cyclist moving in lane&rdquo; and &ldquo;cyclist indicating left&rdquo;, and so on. This richness might help develop better detectors that can create more adaptable autonomous vehicles.&nbsp; Tools used: They used Microsoft&rsquo;s &lsquo;Visual Object Tagging Tool&rdquo; (VOTT) to annotate the dataset.&nbsp;&nbsp;Next steps: This version of READ is a preliminary one, and the scientists plan to eventually label 40,000 frames. They also have ambitious plans to create a novel, deep learning approach to detecting complex activities&rdquo;. Let&rsquo;s wish them luck.&nbsp; Why it matters: Autonomous cars are going to revolutionize many aspects of the world, but in recent years there has been a major push by industry to productize the technology, which has led to much of the research occurring in private. Academic research initiatives and associated dataset releases like this promise to make it easier for other people to develop this technology, potentially broadening our own understanding of it and letting more people participate in its development.&nbsp; Read more: Action Detection from a Robot-Car Perspective (Arxiv).
Whether rain, fog, or snow &ndash; researchers&rsquo; weather dataset has you covered:&hellip;RFS dataset taken from creative commons images&hellip;Researchers with the University of Essex and the University of Birmingham have created a new weather dataset called the Rain Fog Snow (RFS) dataset which researchers can use to better understand, classify and predict weather patterns.&nbsp; Dataset: The dataset consists of more than 3,000 images taken from websites like Flickr, Pixabay, Wikimedia Commons, and others, depicting images of scenes with different weather conditions, ranging from Rain to Fog to Snow. In total, the researchers gather 1100 images from each class, creating a potentially new useful dataset for researchers to experiment with.&nbsp; Read more: Weather Classification: A new multi-class dataset data augmentation approach and comprehensive evaluations of Convolutional Neural Networks (Arxiv).
DeepMind teaches computers to count:&hellip;Pairing deep learning with specific external modules leads to broadened capabilities&hellip;Neural networks are typically not very good at maths. That&rsquo;s because figuring out a way to train a neural network to develop a differentiable, numeric representation is difficult, with most work typically involving handing off the outputs of a neural network to a non-learned predominantly hand-programmed system. Now, DeepMind has implemented a couple of modules &mdash; a Neural Accumulator (NAC) and a Neural Arithmetic Logic Unit (NALU) &mdash; specifically to help its computers learn to count. These modules are &ldquo;biased to learn systematic numerical computation&rdquo;, write the authors of the research. &ldquo;Our strategy is to represent numerical quantities as individual neurons without a nonlinearity. To these single-value neurons, we apply operators that are capable of representing sim…