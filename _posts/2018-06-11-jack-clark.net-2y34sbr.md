---

layout: post
category: product
title: "Import AI #98: Training self-driving cars with rented firetrucks; spotting (staged) violence with AI-infused drones; what graphs might have to do with the future of AI."
date: 2018-06-11 15:16:58
link: https://vrhk.co/2y34sbr
image: 
domain: jack-clark.net
author: "Jack Clark"
icon: https://s2.wp.com/i/webclip.png
excerpt: "Cruise asks to borrow a firetruck to help train its self-driving cars:&hellip;Emergency training data &ndash; literally&hellip;Cruise, a self-driving car company based in San Francisco, wants to expose its vehicles to more data involving the emergency services, so then it asked the city if it could rent a firetruck, fire engine, and ambulance, and have the vehicles drive around a block in the city with their lights flashing, according to emails surfaced via Freedom of Information Act requests from Jalopnik.&nbsp; Read more: GM Cruise Prepping Launch of Driverless Pilot Car Pilot in San Francisco: Emails (Jalopnik).
Experienced researcher: What to do if winter is coming:&hellip;Tips for surviving the post-bubble era in AI&hellip;John Langford, a well-regarded researcher with Microsoft, has some advice for people in the AI community as they carry out the proverbial yak-shaving act of questioning whether AI is in a bubble or not. Though the field shouldn&rsquo;t optimize for failure, it might be helpful if it planned for it, he says.&nbsp;&ldquo;As a field, we should consider the coordinated failure case a little bit. What fraction of the field is currently at companies or in units at companies which are very expensive without yet justifying that expense? It&rsquo;s no longer a small fraction so there is a chance for something traumatic for both the people and field when/where there is a sudden cut-off,&rdquo; he writes.&nbsp; Read more: When the bubble bursts&hellip; (John Langford&rsquo;s personal blog).
Drone AI paper provides a template for future surveillance:&hellip;Lack of discussion of impact of research raises eyebrows&hellip;Researchers with the University of Cambridge, the National Institute of Technology, and the Indian Institute of Science, have published details on a &ldquo;real-time drone surveillance system&rdquo; that uses deep learning. The system is designed to spot violent activities like strangling, punching, kicking, shooting, stabbing, and so on, by performing image recognition over imagery gathered from a crowd in real-time.&nbsp; It&rsquo;s the data, silly: To carry out this project the researchers create their own (highly staged) collection of around 2,000 images called the &lsquo;Aerial Violent Individual&rsquo; dataset, which they record via a consumer-based Parrot AR Drone. Most of the flaws in the system relate to this data, which sees a bunch of people carry out over-acted expressions of aggression towards each other &ndash; this data doesn&rsquo;t seem to have much of a relationship to real-world violence and it&rsquo;s not obvious how well this would perform in the wild.&nbsp; Results: The resulting system &ldquo;works&rdquo;, in the sense that the researchers are able to obtain high accuracies (90%+) on classifying certain violent behaviors within the dataset, but it&rsquo;s not clear whether this translates to anything of practical use in the real world. The researchers will subsequently test out their work at a music festival in India later this month, they said.&nbsp; Responsibility: Like the &ldquo;Deep Video Networks&rdquo; research which I wrote about last week, much of this research is distinguished by the immense implications it appears to have for society, and it&rsquo;s a little sad to see no discussion of this in the paper &ndash; yes, surveillance systems like this can likely be used to humanitarian ends, but they can also be used by malicious actors to surveil or repress people. I think it&rsquo;s important AI researchers start to acknowledge the omni-use nature of their work and confront questions like this within the research itself, rather than afterwards following public criticism.&nbsp; Read more: Eyes in the Sky: Real-time Drone Surveillance System (DSS) for VIolent Individuals Identification using ScatterNet Hybrid Deep Learning Framework (Arxiv).&nbsp;&nbsp;Watch video (YouTube).
&ldquo;Depth First Learning&rdquo; launches to aid understanding of AI papers:&hellip;Learning through a combination of gathering context and testing understanding&hellip;Industry and academic researchers have launched &lsquo;Depth First Learning&rdquo;, an initiative to make it easier for people to educate themselves about important research papers by going through the key ideas of the paper along with recommended literature to read and various questions throughout each writeup indented to test for the reader having learned enough about the context to answer the question. The idea behind this work is that it makes it easier to understand research papers by breaking them down into their fundamental concepts. &ldquo;We spent some time understanding each paper and writing down the core concepts on which they were built,&rdquo; the researchers write.&nbsp; Read an example: &ldquo;Depth First Learning&rdquo; article on InfoGAN (Depth First Learning website).&nbsp; Read more: Depth First Learning (DFL website, About page).
Graphs, graphs everywhere: The future according to DeepMind:&hellip;Why a little structure can be a very good thing&hellip;New research from DeepMind shows how to fuse structured approaches to AI design with end-to-end learned systems to create systems that can not only learn about the world, but recombine learnings in new ways to solve new problems. This sort of &ldquo;combinatorial generalization&rdquo; is key to intelligence, the authors write, and they claim their approach deals with some of the recent criticisms of deep learning made by people like Judea Pearl, Josh Tenenbaum, and Gary Marcus, among others.&nbsp; Structure, structure everywhere: The authors argue that many of today&rsquo;s deep learning systems already encode this sort of bias towards structure in the form of specific arrangements of learned components, for example, how convolutional neural networks are composed out of convolutional layers and then chained together in increasingly elaborate ways for image recognition. These designs encode within them an implicit relational inductive bias, the authors write, because they take in a bunch of data and operate over its relationships in increasingly elaborate ways. Additionally, most problems can be decomposed into graph representations (for instance, modeling the interactions of a bunch of pool balls can be done by expressing the pool balls and the table as nodes in a graph with the links between them signaling directions in which force may be transmitted, or a molecule can similarly be decomposed as atoms (nodes) and bonds (edges).&nbsp; Graph network: DeepMind has developed the &lsquo;Graph network&rsquo; (GN) block, a generic component &ldquo;which takes a graph as input, performs computations over the structure, and returns a graph as output.&rdquo; This is desirable because a graph structure is fairly flexible, letting you express an arbitrary number of relationships between an arbitrary number of entities, and the same function can be deployed on differently sized graphs, and these graphs represent entities and relations as sets making them invariant to permutations.&nbsp; No silver bullet: Graph networks don&rsquo;t make it easy to support approaches like &ldquo;recursion, control flow, and conditional iteration&rdquo;, they say, and so should not be considered a panacea. Another is the larger question of where to derive the graphs from that the graphs operate over, which the authors leave to other researchers.&nbsp; Read more: Relational inductive biases, deep learning, and graph networks (Arxiv).
Google announces AI principles to guide its business:&hellip;Company releases seven principles, along with description of &lsquo;AI applications we will not pursue&rsquo;&hellip;Google has published its AI principles, following an internal employee outcry in response to the company&rsquo;s participation in a drone surveillance project for the US military. These principles are intended to guide Google&rsquo;s work in the future, according to a blog post written by Google CEO Sundar Pichai. &…"

---

### Import AI #98: Training self-driving cars with rented firetrucks; spotting (staged) violence with AI-infused drones; what graphs might have to do with the future of AI.

Cruise asks to borrow a firetruck to help train its self-driving cars:&hellip;Emergency training data &ndash; literally&hellip;Cruise, a self-driving car company based in San Francisco, wants to expose its vehicles to more data involving the emergency services, so then it asked the city if it could rent a firetruck, fire engine, and ambulance, and have the vehicles drive around a block in the city with their lights flashing, according to emails surfaced via Freedom of Information Act requests from Jalopnik.&nbsp; Read more: GM Cruise Prepping Launch of Driverless Pilot Car Pilot in San Francisco: Emails (Jalopnik).
Experienced researcher: What to do if winter is coming:&hellip;Tips for surviving the post-bubble era in AI&hellip;John Langford, a well-regarded researcher with Microsoft, has some advice for people in the AI community as they carry out the proverbial yak-shaving act of questioning whether AI is in a bubble or not. Though the field shouldn&rsquo;t optimize for failure, it might be helpful if it planned for it, he says.&nbsp;&ldquo;As a field, we should consider the coordinated failure case a little bit. What fraction of the field is currently at companies or in units at companies which are very expensive without yet justifying that expense? It&rsquo;s no longer a small fraction so there is a chance for something traumatic for both the people and field when/where there is a sudden cut-off,&rdquo; he writes.&nbsp; Read more: When the bubble bursts&hellip; (John Langford&rsquo;s personal blog).
Drone AI paper provides a template for future surveillance:&hellip;Lack of discussion of impact of research raises eyebrows&hellip;Researchers with the University of Cambridge, the National Institute of Technology, and the Indian Institute of Science, have published details on a &ldquo;real-time drone surveillance system&rdquo; that uses deep learning. The system is designed to spot violent activities like strangling, punching, kicking, shooting, stabbing, and so on, by performing image recognition over imagery gathered from a crowd in real-time.&nbsp; It&rsquo;s the data, silly: To carry out this project the researchers create their own (highly staged) collection of around 2,000 images called the &lsquo;Aerial Violent Individual&rsquo; dataset, which they record via a consumer-based Parrot AR Drone. Most of the flaws in the system relate to this data, which sees a bunch of people carry out over-acted expressions of aggression towards each other &ndash; this data doesn&rsquo;t seem to have much of a relationship to real-world violence and it&rsquo;s not obvious how well this would perform in the wild.&nbsp; Results: The resulting system &ldquo;works&rdquo;, in the sense that the researchers are able to obtain high accuracies (90%+) on classifying certain violent behaviors within the dataset, but it&rsquo;s not clear whether this translates to anything of practical use in the real world. The researchers will subsequently test out their work at a music festival in India later this month, they said.&nbsp; Responsibility: Like the &ldquo;Deep Video Networks&rdquo; research which I wrote about last week, much of this research is distinguished by the immense implications it appears to have for society, and it&rsquo;s a little sad to see no discussion of this in the paper &ndash; yes, surveillance systems like this can likely be used to humanitarian ends, but they can also be used by malicious actors to surveil or repress people. I think it&rsquo;s important AI researchers start to acknowledge the omni-use nature of their work and confront questions like this within the research itself, rather than afterwards following public criticism.&nbsp; Read more: Eyes in the Sky: Real-time Drone Surveillance System (DSS) for VIolent Individuals Identification using ScatterNet Hybrid Deep Learning Framework (Arxiv).&nbsp;&nbsp;Watch video (YouTube).
&ldquo;Depth First Learning&rdquo; launches to aid understanding of AI papers:&hellip;Learning through a combination of gathering context and testing understanding&hellip;Industry and academic researchers have launched &lsquo;Depth First Learning&rdquo;, an initiative to make it easier for people to educate themselves about important research papers by going through the key ideas of the paper along with recommended literature to read and various questions throughout each writeup indented to test for the reader having learned enough about the context to answer the question. The idea behind this work is that it makes it easier to understand research papers by breaking them down into their fundamental concepts. &ldquo;We spent some time understanding each paper and writing down the core concepts on which they were built,&rdquo; the researchers write.&nbsp; Read an example: &ldquo;Depth First Learning&rdquo; article on InfoGAN (Depth First Learning website).&nbsp; Read more: Depth First Learning (DFL website, About page).
Graphs, graphs everywhere: The future according to DeepMind:&hellip;Why a little structure can be a very good thing&hellip;New research from DeepMind shows how to fuse structured approaches to AI design with end-to-end learned systems to create systems that can not only learn about the world, but recombine learnings in new ways to solve new problems. This sort of &ldquo;combinatorial generalization&rdquo; is key to intelligence, the authors write, and they claim their approach deals with some of the recent criticisms of deep learning made by people like Judea Pearl, Josh Tenenbaum, and Gary Marcus, among others.&nbsp; Structure, structure everywhere: The authors argue that many of today&rsquo;s deep learning systems already encode this sort of bias towards structure in the form of specific arrangements of learned components, for example, how convolutional neural networks are composed out of convolutional layers and then chained together in increasingly elaborate ways for image recognition. These designs encode within them an implicit relational inductive bias, the authors write, because they take in a bunch of data and operate over its relationships in increasingly elaborate ways. Additionally, most problems can be decomposed into graph representations (for instance, modeling the interactions of a bunch of pool balls can be done by expressing the pool balls and the table as nodes in a graph with the links between them signaling directions in which force may be transmitted, or a molecule can similarly be decomposed as atoms (nodes) and bonds (edges).&nbsp; Graph network: DeepMind has developed the &lsquo;Graph network&rsquo; (GN) block, a generic component &ldquo;which takes a graph as input, performs computations over the structure, and returns a graph as output.&rdquo; This is desirable because a graph structure is fairly flexible, letting you express an arbitrary number of relationships between an arbitrary number of entities, and the same function can be deployed on differently sized graphs, and these graphs represent entities and relations as sets making them invariant to permutations.&nbsp; No silver bullet: Graph networks don&rsquo;t make it easy to support approaches like &ldquo;recursion, control flow, and conditional iteration&rdquo;, they say, and so should not be considered a panacea. Another is the larger question of where to derive the graphs from that the graphs operate over, which the authors leave to other researchers.&nbsp; Read more: Relational inductive biases, deep learning, and graph networks (Arxiv).
Google announces AI principles to guide its business:&hellip;Company releases seven principles, along with description of &lsquo;AI applications we will not pursue&rsquo;&hellip;Google has published its AI principles, following an internal employee outcry in response to the company&rsquo;s participation in a drone surveillance project for the US military. These principles are intended to guide Google&rsquo;s work in the future, according to a blog post written by Google CEO Sundar Pichai. &…