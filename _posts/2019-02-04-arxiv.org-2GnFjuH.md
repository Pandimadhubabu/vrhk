---

layout: post
category: threads
title: "Fixup Initialization: Residual Learning Without Normalization"
date: 2019-02-04 01:27:52
link: https://vrhk.co/2GnFjuH
image: 
domain: arxiv.org
author: "arXiv.org"
icon: https://arxiv.org/favicon.ico
excerpt: "Normalization layers are a staple in state-of-the-art deep neural network architectures. They are widely believed to stabilize training, enable higher learning rate, accelerate convergence and..."

---

### Fixup Initialization: Residual Learning Without Normalization

Normalization layers are a staple in state-of-the-art deep neural network architectures. They are widely believed to stabilize training, enable higher learning rate, accelerate convergence and...