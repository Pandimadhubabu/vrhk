---

layout: post
category: C7WHBLNLR
title: "[D] Has there been any studies on of ReLu still maintains it's advantaged over tanh/sigmoid when the layers are batch normalized? • r/MachineLearning"
date: 2017-12-22 03:13:31
link: http://bit.ly/2kCDmPz
image: https://www.redditstatic.com/icon.png
domain: reddit.com
author: "reddit"
icon: http://www.redditstatic.com/desktop2x/img/favicon/apple-icon-57x57.png
excerpt: "The advantage Relu has over sigmoid/tanh is that at the high/low areas of the curve, the gradients get very small. However, batch normalization..."

---

### [D] Has there been any studies on of ReLu still maintains it's advantaged over tanh/sigmoid when the layers are batch normalized? • r/MachineLearning

The advantage Relu has over sigmoid/tanh is that at the high/low areas of the curve, the gradients get very small. However, batch normalization...