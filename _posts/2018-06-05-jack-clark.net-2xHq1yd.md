---

layout: post
category: product
title: "Import AI #97: Faking Obama and Putin with Deep Video Portraits, Berkeley releases a 100,000+ video self-driving car dataset, and what happens when you add the sensation of touch to robots."
date: 2018-06-05 05:08:21
link: https://vrhk.co/2xHq1yd
image: 
domain: jack-clark.net
author: "Jack Clark"
icon: https://s2.wp.com/i/webclip.png
excerpt: "Try a little tenderness: researchers add touch sensors to robots.&hellip;It&rsquo;s easier to manipulate objects if you can feel them&hellip;Researchers with the University of California at Berkeley have added GelSight touch sensors to a standard 7-DoF Rethink Robotics &lsquo;Sawyer&rsquo; robot with an attached Weiss WSG-50 parallel gripper to explore how touch inputs can improve performance at grasping objects &ndash; a crucial skill for robots to have if used in commercial settings. &nbsp;&nbsp;Technique:&nbsp;The researchers construct four sub-networks that operate over specific data inputs (camera image, two GelSight images to model texture senses before and after contact, and an action network that processes 3D motion, in-plane rotation, and change in force. They link these networks together within a larger network and train the resulting model over a dataset of objects.&nbsp;The researchers pre-train the image components of the network with a model previously trained to classify objects on ImageNet.&nbsp;The approach yields a model that adapts to novel surfaces, learns interpretable policies, and can be taught to apply specific constraints when handling an object, like grasping it gently.&nbsp; &nbsp;&nbsp;Results: The researchers test their model and find that systems trained with vision and action inputs get 73.03% accuracy, compared to 79.34% for systems trained on tactile inputs and action, compared to 80.28% for systems trained with tactile and vision and action.&nbsp; &nbsp;Harder than you think: This task, like most that require applying deep learning components to real-world systems, contains a few quirks which might seem non-obvious from the outset, for example: &ldquo;The robot only receives tactile input intermittently, when its fingers are in contact with the object and, since each re-grasp attempt can disturb the object position and pose, the scene changes with each interaction&rdquo;. &nbsp;&nbsp;Read more: More Than a Feeling: Learning to Grasp and Regrasp using Vision and Touch (Arxiv).
Want 100,000 self-driving car videos? Berkeley has you covered!&hellip;&rdquo;The largest and most diverse open driving video dataset so far for computer vision research&rdquo;., according to the researchers..Researchers with the University of California at Berkeley and Nexar have published BDD100K, a self-driving car dataset which BDD100K contains&nbsp;~120,000,000 images spread across ~100,000 videos. &ldquo;Our database covers different weather conditions, including sunny, overcast, and rainy, as well as different times of day including daytime and nighttime,&rdquo; they say. The dataset is substantially larger than ones released by the University of Toronto (KITTI), Baidu (ApolloScape), Mapillary, and others, they say.DeepDrive:&nbsp;The dataset release is significant for where it comes from: DeepDrive, a Berkeley-led self-driving car research effort with a vast range of capable partners, including automotive companies such as Honda, Toyota, and Ford. DeepDrive was set up partially so its many sponsors could pool research efforts on self-driving cars, seeking to close an implicit gap with other players. &nbsp;&nbsp;Rich data: The videos are annotated with hundreds of thousands of labels for objects like cars, trucks, persons, bicycles, and so on, as well as richer annotations for road lines drivable areas, and more; they also provide a subset of roughly ~10,000 images with full-frame instance segmentation.&nbsp; Why it matters &ndash; the rise of the multi-modal dataset: The breadth of the dataset with its millions of labels and carefully refined aspects will likely empower researchers in other areas of AI, as well as its obvious self-driving car audience. I expect that in the future these multi-modal datasets will become increasingly attractive targets to use to evaluate transfer learning from other systems, for instance by training a self-driving car model in a rich simulated world then applying it to real-world data, such as BDD100K. &nbsp;&nbsp;Challenges: The researchers are hosting three challenges at computer vision conference CVPR relating to the dataset, and are asking groups to compete to develop systems for road object detection, drivable area prediction, and domain adaptation.&nbsp;&nbsp;Read more: BDD100K: A Large-scale Diverse Driving Video Database (Berkeley AI Research blog).
KPCB&rsquo;s Mary Meeker breaks down AI&rsquo;s rise and China&rsquo;s possible advantage in annual presentation:&hellip;Annual slide-a-thon shows rise of China, points to image and speech recognition scores as evidence for impact of AI&hellip;Mary Meeker&rsquo;s annual presentation of research serves as a useful refresher for what is front-of-mind for venture capitalists focused on understanding the dynamics that affect the technology ecosystem.&nbsp;This year, at Code Conference in California, Meeker&rsquo;s slides were distinguished via large sections spent on China, combined with a few notable slides situating AI progress metrics (specifically in object recognition and speech recognition) in relation to the growth of new markets for business. &nbsp;&nbsp;Read more: Mary Meeker&rsquo;s 2018 internet trends report: All the slides, plus analysis (Recode).
SPECIAL SECTION: FAKE EVERYTHING:
An incomplete timeline of dubious things that people have synthesized via AI&ndash; Early 2017: Montreal Startup Lyrebird launches with audio recording featuring synthesized voices of Donald Trump, Barack Obama, Hillary Clinton.&ndash; Late 2017: &ldquo;DeepFakes&rdquo; arrive on the internet via Reddit with a user posting pornographic movies with celebrity faces animated onto them. A consumer-oriented free editing application follows and DeepFakes rapidly proliferate across the internet, then consumer sites start to clamp down on them.&ndash; 2018: Belgian socialist party makes a video containing a synthesized Donald Trump giving a (fake) speech about climate change. Party says video designed to create debate and not trick viewers.&ndash; Listen: Politicians discussing about Lyrebird (Lyrebird Soundcloud).&ndash; Read more: DeepFakes Wikipedia entry.&ndash; Read more: Belgian Socialist Party Circulates &ldquo;Deep Fake&rdquo; Donald Trump Video (Politico Europe).
Why all footage of all politicians is about to become suspect:&hellip;Think fake news is bad now? &lsquo;Deep Video Portraits&rsquo; will make it much, much worse&hellip;A couple of years ago European researchers caused a stir with &lsquo;face2face&rsquo;, technology which they demonstrated by mapping their own facial expressions onto synthetically rendered footage of famous VIPs, like George Bush, Barack Obama, and so on. Now, new research from a group of American and European researchers has pushed this fake-anyone technology further, increasing the fidelity of the rendered footage, reducing the amount of data needed to construct such convincing fakes, and also dealing with visual bugs that would make it easier to identify the output as being synthesized.&nbsp; In their words: &ldquo;We address the problem of synthesizing a photo-realistic video portrait of a target actor that mimics the actions of a source actor, where source and target can be different subjects,&rdquo; they write. &ldquo;Our approach enables a source actor to take full control of the rigid head pose, face expressions and eye motion of the target actor&rdquo;. (Emphasis mine.) &nbsp;&nbsp;Technique: The technique involves a few stages: first, the researchers track the people within the source and target videos via a monocular face reconstruction approach, which allows them to extract information about the identity, head pose, expression, eye gaze, and scene lighting for each video frame. They also separately track the gaze of each subject. They then essentially transfer the synthetic renderings of the input actor onto the target actor and perform a couple of clever tricks to make the resulting output high fidelity and less prone to synth…"

---

### Import AI #97: Faking Obama and Putin with Deep Video Portraits, Berkeley releases a 100,000+ video self-driving car dataset, and what happens when you add the sensation of touch to robots.

Try a little tenderness: researchers add touch sensors to robots.&hellip;It&rsquo;s easier to manipulate objects if you can feel them&hellip;Researchers with the University of California at Berkeley have added GelSight touch sensors to a standard 7-DoF Rethink Robotics &lsquo;Sawyer&rsquo; robot with an attached Weiss WSG-50 parallel gripper to explore how touch inputs can improve performance at grasping objects &ndash; a crucial skill for robots to have if used in commercial settings. &nbsp;&nbsp;Technique:&nbsp;The researchers construct four sub-networks that operate over specific data inputs (camera image, two GelSight images to model texture senses before and after contact, and an action network that processes 3D motion, in-plane rotation, and change in force. They link these networks together within a larger network and train the resulting model over a dataset of objects.&nbsp;The researchers pre-train the image components of the network with a model previously trained to classify objects on ImageNet.&nbsp;The approach yields a model that adapts to novel surfaces, learns interpretable policies, and can be taught to apply specific constraints when handling an object, like grasping it gently.&nbsp; &nbsp;&nbsp;Results: The researchers test their model and find that systems trained with vision and action inputs get 73.03% accuracy, compared to 79.34% for systems trained on tactile inputs and action, compared to 80.28% for systems trained with tactile and vision and action.&nbsp; &nbsp;Harder than you think: This task, like most that require applying deep learning components to real-world systems, contains a few quirks which might seem non-obvious from the outset, for example: &ldquo;The robot only receives tactile input intermittently, when its fingers are in contact with the object and, since each re-grasp attempt can disturb the object position and pose, the scene changes with each interaction&rdquo;. &nbsp;&nbsp;Read more: More Than a Feeling: Learning to Grasp and Regrasp using Vision and Touch (Arxiv).
Want 100,000 self-driving car videos? Berkeley has you covered!&hellip;&rdquo;The largest and most diverse open driving video dataset so far for computer vision research&rdquo;., according to the researchers..Researchers with the University of California at Berkeley and Nexar have published BDD100K, a self-driving car dataset which BDD100K contains&nbsp;~120,000,000 images spread across ~100,000 videos. &ldquo;Our database covers different weather conditions, including sunny, overcast, and rainy, as well as different times of day including daytime and nighttime,&rdquo; they say. The dataset is substantially larger than ones released by the University of Toronto (KITTI), Baidu (ApolloScape), Mapillary, and others, they say.DeepDrive:&nbsp;The dataset release is significant for where it comes from: DeepDrive, a Berkeley-led self-driving car research effort with a vast range of capable partners, including automotive companies such as Honda, Toyota, and Ford. DeepDrive was set up partially so its many sponsors could pool research efforts on self-driving cars, seeking to close an implicit gap with other players. &nbsp;&nbsp;Rich data: The videos are annotated with hundreds of thousands of labels for objects like cars, trucks, persons, bicycles, and so on, as well as richer annotations for road lines drivable areas, and more; they also provide a subset of roughly ~10,000 images with full-frame instance segmentation.&nbsp; Why it matters &ndash; the rise of the multi-modal dataset: The breadth of the dataset with its millions of labels and carefully refined aspects will likely empower researchers in other areas of AI, as well as its obvious self-driving car audience. I expect that in the future these multi-modal datasets will become increasingly attractive targets to use to evaluate transfer learning from other systems, for instance by training a self-driving car model in a rich simulated world then applying it to real-world data, such as BDD100K. &nbsp;&nbsp;Challenges: The researchers are hosting three challenges at computer vision conference CVPR relating to the dataset, and are asking groups to compete to develop systems for road object detection, drivable area prediction, and domain adaptation.&nbsp;&nbsp;Read more: BDD100K: A Large-scale Diverse Driving Video Database (Berkeley AI Research blog).
KPCB&rsquo;s Mary Meeker breaks down AI&rsquo;s rise and China&rsquo;s possible advantage in annual presentation:&hellip;Annual slide-a-thon shows rise of China, points to image and speech recognition scores as evidence for impact of AI&hellip;Mary Meeker&rsquo;s annual presentation of research serves as a useful refresher for what is front-of-mind for venture capitalists focused on understanding the dynamics that affect the technology ecosystem.&nbsp;This year, at Code Conference in California, Meeker&rsquo;s slides were distinguished via large sections spent on China, combined with a few notable slides situating AI progress metrics (specifically in object recognition and speech recognition) in relation to the growth of new markets for business. &nbsp;&nbsp;Read more: Mary Meeker&rsquo;s 2018 internet trends report: All the slides, plus analysis (Recode).
SPECIAL SECTION: FAKE EVERYTHING:
An incomplete timeline of dubious things that people have synthesized via AI&ndash; Early 2017: Montreal Startup Lyrebird launches with audio recording featuring synthesized voices of Donald Trump, Barack Obama, Hillary Clinton.&ndash; Late 2017: &ldquo;DeepFakes&rdquo; arrive on the internet via Reddit with a user posting pornographic movies with celebrity faces animated onto them. A consumer-oriented free editing application follows and DeepFakes rapidly proliferate across the internet, then consumer sites start to clamp down on them.&ndash; 2018: Belgian socialist party makes a video containing a synthesized Donald Trump giving a (fake) speech about climate change. Party says video designed to create debate and not trick viewers.&ndash; Listen: Politicians discussing about Lyrebird (Lyrebird Soundcloud).&ndash; Read more: DeepFakes Wikipedia entry.&ndash; Read more: Belgian Socialist Party Circulates &ldquo;Deep Fake&rdquo; Donald Trump Video (Politico Europe).
Why all footage of all politicians is about to become suspect:&hellip;Think fake news is bad now? &lsquo;Deep Video Portraits&rsquo; will make it much, much worse&hellip;A couple of years ago European researchers caused a stir with &lsquo;face2face&rsquo;, technology which they demonstrated by mapping their own facial expressions onto synthetically rendered footage of famous VIPs, like George Bush, Barack Obama, and so on. Now, new research from a group of American and European researchers has pushed this fake-anyone technology further, increasing the fidelity of the rendered footage, reducing the amount of data needed to construct such convincing fakes, and also dealing with visual bugs that would make it easier to identify the output as being synthesized.&nbsp; In their words: &ldquo;We address the problem of synthesizing a photo-realistic video portrait of a target actor that mimics the actions of a source actor, where source and target can be different subjects,&rdquo; they write. &ldquo;Our approach enables a source actor to take full control of the rigid head pose, face expressions and eye motion of the target actor&rdquo;. (Emphasis mine.) &nbsp;&nbsp;Technique: The technique involves a few stages: first, the researchers track the people within the source and target videos via a monocular face reconstruction approach, which allows them to extract information about the identity, head pose, expression, eye gaze, and scene lighting for each video frame. They also separately track the gaze of each subject. They then essentially transfer the synthetic renderings of the input actor onto the target actor and perform a couple of clever tricks to make the resulting output high fidelity and less prone to synth…