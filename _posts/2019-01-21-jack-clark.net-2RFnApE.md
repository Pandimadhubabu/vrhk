---

layout: post
category: product
title: "Import AI 130: Pushing neural architecture search further with transfer learning; Facebook funds European center on AI ethics; and analysis shows BERT is more powerful than people might think"
date: 2019-01-21 18:07:00
link: https://vrhk.co/2RFnApE
image: 
domain: jack-clark.net
author: "Jack Clark"
icon: https://s2.wp.com/i/webclip.png
excerpt: "Facebook reveals its &ldquo;self-feeding chatbot&rdquo;:&hellip;Towards AI systems that continuously update themselves&hellip;AI systems are a bit like dumb, toy robots: you spend months or years laboring away in a research lab and eventually a factory (in the case of AI, a data center) to design an exquisite little doohickey that does something very well, then you start selling it in the market, observe what users do with it, and use those insights to help you design a new, better robot. Wouldn&rsquo;t it be better if the toy robot was able to understand how users were interacting with it, and adjust its behavior to make the users more satisfied with it? That&rsquo;s the idea behind new research from Facebook which proposes &ldquo;the self-feeding chatbot, a dialogue agent with the ability to extract new examples from the conversations it participates in after deployment&rdquo;.&nbsp; How it works &ndash; pre-training: Facebook&rsquo;s chatbot is trained on two tasks: DIALOGUE, where the bot tries to predict the next utterance in a conversation (which it can use to calibrate itself), and SATISFACTION, where it tries to assess how satisfied the speaking partner is with the conversation. Data for both these tasks comes from conversations between humans. The DIALOGUE dataset comes from the &lsquo;PERSONACHAT&rsquo; dataset consists of short dialogs (6-8 turns) between two humans who have been instructed to try and get to know eachother. &nbsp;&nbsp;How it works &ndash; updating in the wild: Once deployed, the chatbot learns from its interactions with people in two ways: if the bot predicts with high-confidence that its response will satisfy its conversation partner, then it extracts a new structured dialogue example from the discussion with the human. If the bot thinks that the human is unsatisfied with the bot&rsquo;s most recent interaction with it, then the bot generates a question for the person to request feedback, and this conversation exchange is used to generate a feedback example, which the bot stores and learns from. (&ldquo;We rely on the fact that the feedback is not random: regardless of whether it is a verbatim response, a description of a response, or a list of possible responses&rdquo;, Facebook writes.&nbsp; Results: Facebook shows that it can further improve the performance of its chatbots by using data generated by its chatbot during interactions with humans. Additionally, the use of this data displays solid improvements on performance regardless of the number of data examples in the system &ndash; suggesting that a little bit of data gathered in the wild can improve performance in most places. &ldquo;Even when the entire PERSONACHAT dataset of 131k examples is used &ndash; a much larger dataset than what is available for most dialogue tasks &ndash; adding deployment examples is still able to provide an additional 1.6 points of accuracy on what is otherwise a very flat region of the learning curve.,&rdquo; they write. &nbsp;&nbsp;Why this matters: Being able to design AI systems that can automatically gather their own data once deployed feels like a middle ground between the systems we have today, and systems which do fully autonomous continuous learning. It&rsquo;ll be fascinating to see if techniques like these are experimented more widely, as that might lead to the chatbots around us getting substantially better. Because this system requires on its human conversation partners to improve itself it is implicit that their data has some trace economic value, so perhaps work like this also will also further support some of the debates people have about whether users should be able to own their own data or not.&nbsp; Read more: Learning from Dialogue after Deployment: Feed Yourself, Chatbot! (Arxiv).
BERT: More powerful than you think:&hellip;Language researcher remarks on the surprisingly well-performing Transformer-based system&hellip;Yoav Goldberg, a researcher with Bar Ilan University in Israel and the Allen Institute for AI, has analyzed BERT, a language model recently released by Google. The goal of this research is to see how well BERT can represent challenging language concepts, like &ldquo;naturally-occurring subject-verb agreement stimuli&rdquo;, &rdquo; &lsquo;colorless green ideas&rsquo; subject-verb agreement stimuli, in which content words in natural sentences are randomly replaced with words sharing the same part-of-speech and inflection&rdquo;, and &ldquo;manually crafted stimuli for subject-verb agreement and reflexive anaphora phenomena&rdquo;. To Goldberg&rsquo;s surprise, standard BERT models &ldquo;perform very well on all the syntactic tasks&rdquo; without any task-specific fine-tuning.&nbsp; BERT, a refresher: BERT is based on a technology called a Transformer which, unlike recurrent neural networks, &ldquo;relies purely on attention mechanisms, and does not have an explicit notion of word order beyond marking each word with its absolute-position embedding.&rdquo; BERT is bidirectional, so it gains language capabilities by being trained to predict the identity of masked words based on both the prefix and suffix surrounding the words. &nbsp;&nbsp;Results: One tricky thing about assessing BERT performance is that it has been trained on different and larger datasets, and can access the suffix of the sentence as well as the prefix of the sentence. Nonetheless,Goldberg concludes that &ldquo;BERT models are likely capable of capturing the same kind of syntactic regularities that LSTM-based models are capable of capturing, at least as well as the LSTM models and probably better.&rdquo;&nbsp; Why it matters: I think this paper is further evidence that 2018 really was, as some have said, the year of ImageNet for NLP. What I mean by that is: in 2012 the ImageNet results blew all other image analysis approaches on the ImageNet challenge out of the water and sparked a re-orientation of a huge part of the AI research computer toward neural networks, ending a long, cold winter, and leading almost directly to significant commercial applications that drove a rise in industry investment into AI, which has fundamentally reshaped AI research. By comparison, 2018 had a series of impressive results &ndash; work from Allen AI on Elmo, work by OpenAI on the General Purpose Transformer, and work by Google on BERT. &nbsp;&nbsp;These results, taken together, show the arrival of scalable, simple methods for language understanding that seem to work better than prior approaches, while also being in some senses simpler. (And a rule that has tended to hold in AI research is that simpler techniques win out in the long run by virtue of being easy for researchers to fiddle with and chain together into larger systems). If this really has happened, then we should expect bigger, more significant language results in the future &ndash; and just as ImageNet&rsquo;s 2012 success ultimately reshaped societies (enabling everything from follow-the-human drones, to better self-driving cars, to doorbells that use AI to automatically police neighborhoods), it&rsquo;s possible 2018&rsquo;s series of advances could do be year zero for NLP.&nbsp; Read more: Assessing BERT&rsquo;s Syntactic Abilities (Arxiv).
Towards a future where all infrastructure is surveyed and analyzed by drones:&hellip;Radio instead of GPS, light drones, and a wind turbine&hellip;Researchers with Lulea University of Technology in Sweden have developed techniques to let small drones (sometimes called Micro Aerial Vehicles, or MAVs) autonomously inspect very large machines and/or buildings, such as wind turbines. The primary technical inventions outlined in the report are the creation of a localization technique to let multiple drones coordinate with eachother as they inspect something, as well as the creation of a path planning algorithm to help them not only inspect the structure, but also gather enough data &ldquo;to enable the generation of an off-line 3D model of the structure&rdquo;.&…"

---

### Import AI 130: Pushing neural architecture search further with transfer learning; Facebook funds European center on AI ethics; and analysis shows BERT is more powerful than people might think

Facebook reveals its &ldquo;self-feeding chatbot&rdquo;:&hellip;Towards AI systems that continuously update themselves&hellip;AI systems are a bit like dumb, toy robots: you spend months or years laboring away in a research lab and eventually a factory (in the case of AI, a data center) to design an exquisite little doohickey that does something very well, then you start selling it in the market, observe what users do with it, and use those insights to help you design a new, better robot. Wouldn&rsquo;t it be better if the toy robot was able to understand how users were interacting with it, and adjust its behavior to make the users more satisfied with it? That&rsquo;s the idea behind new research from Facebook which proposes &ldquo;the self-feeding chatbot, a dialogue agent with the ability to extract new examples from the conversations it participates in after deployment&rdquo;.&nbsp; How it works &ndash; pre-training: Facebook&rsquo;s chatbot is trained on two tasks: DIALOGUE, where the bot tries to predict the next utterance in a conversation (which it can use to calibrate itself), and SATISFACTION, where it tries to assess how satisfied the speaking partner is with the conversation. Data for both these tasks comes from conversations between humans. The DIALOGUE dataset comes from the &lsquo;PERSONACHAT&rsquo; dataset consists of short dialogs (6-8 turns) between two humans who have been instructed to try and get to know eachother. &nbsp;&nbsp;How it works &ndash; updating in the wild: Once deployed, the chatbot learns from its interactions with people in two ways: if the bot predicts with high-confidence that its response will satisfy its conversation partner, then it extracts a new structured dialogue example from the discussion with the human. If the bot thinks that the human is unsatisfied with the bot&rsquo;s most recent interaction with it, then the bot generates a question for the person to request feedback, and this conversation exchange is used to generate a feedback example, which the bot stores and learns from. (&ldquo;We rely on the fact that the feedback is not random: regardless of whether it is a verbatim response, a description of a response, or a list of possible responses&rdquo;, Facebook writes.&nbsp; Results: Facebook shows that it can further improve the performance of its chatbots by using data generated by its chatbot during interactions with humans. Additionally, the use of this data displays solid improvements on performance regardless of the number of data examples in the system &ndash; suggesting that a little bit of data gathered in the wild can improve performance in most places. &ldquo;Even when the entire PERSONACHAT dataset of 131k examples is used &ndash; a much larger dataset than what is available for most dialogue tasks &ndash; adding deployment examples is still able to provide an additional 1.6 points of accuracy on what is otherwise a very flat region of the learning curve.,&rdquo; they write. &nbsp;&nbsp;Why this matters: Being able to design AI systems that can automatically gather their own data once deployed feels like a middle ground between the systems we have today, and systems which do fully autonomous continuous learning. It&rsquo;ll be fascinating to see if techniques like these are experimented more widely, as that might lead to the chatbots around us getting substantially better. Because this system requires on its human conversation partners to improve itself it is implicit that their data has some trace economic value, so perhaps work like this also will also further support some of the debates people have about whether users should be able to own their own data or not.&nbsp; Read more: Learning from Dialogue after Deployment: Feed Yourself, Chatbot! (Arxiv).
BERT: More powerful than you think:&hellip;Language researcher remarks on the surprisingly well-performing Transformer-based system&hellip;Yoav Goldberg, a researcher with Bar Ilan University in Israel and the Allen Institute for AI, has analyzed BERT, a language model recently released by Google. The goal of this research is to see how well BERT can represent challenging language concepts, like &ldquo;naturally-occurring subject-verb agreement stimuli&rdquo;, &rdquo; &lsquo;colorless green ideas&rsquo; subject-verb agreement stimuli, in which content words in natural sentences are randomly replaced with words sharing the same part-of-speech and inflection&rdquo;, and &ldquo;manually crafted stimuli for subject-verb agreement and reflexive anaphora phenomena&rdquo;. To Goldberg&rsquo;s surprise, standard BERT models &ldquo;perform very well on all the syntactic tasks&rdquo; without any task-specific fine-tuning.&nbsp; BERT, a refresher: BERT is based on a technology called a Transformer which, unlike recurrent neural networks, &ldquo;relies purely on attention mechanisms, and does not have an explicit notion of word order beyond marking each word with its absolute-position embedding.&rdquo; BERT is bidirectional, so it gains language capabilities by being trained to predict the identity of masked words based on both the prefix and suffix surrounding the words. &nbsp;&nbsp;Results: One tricky thing about assessing BERT performance is that it has been trained on different and larger datasets, and can access the suffix of the sentence as well as the prefix of the sentence. Nonetheless,Goldberg concludes that &ldquo;BERT models are likely capable of capturing the same kind of syntactic regularities that LSTM-based models are capable of capturing, at least as well as the LSTM models and probably better.&rdquo;&nbsp; Why it matters: I think this paper is further evidence that 2018 really was, as some have said, the year of ImageNet for NLP. What I mean by that is: in 2012 the ImageNet results blew all other image analysis approaches on the ImageNet challenge out of the water and sparked a re-orientation of a huge part of the AI research computer toward neural networks, ending a long, cold winter, and leading almost directly to significant commercial applications that drove a rise in industry investment into AI, which has fundamentally reshaped AI research. By comparison, 2018 had a series of impressive results &ndash; work from Allen AI on Elmo, work by OpenAI on the General Purpose Transformer, and work by Google on BERT. &nbsp;&nbsp;These results, taken together, show the arrival of scalable, simple methods for language understanding that seem to work better than prior approaches, while also being in some senses simpler. (And a rule that has tended to hold in AI research is that simpler techniques win out in the long run by virtue of being easy for researchers to fiddle with and chain together into larger systems). If this really has happened, then we should expect bigger, more significant language results in the future &ndash; and just as ImageNet&rsquo;s 2012 success ultimately reshaped societies (enabling everything from follow-the-human drones, to better self-driving cars, to doorbells that use AI to automatically police neighborhoods), it&rsquo;s possible 2018&rsquo;s series of advances could do be year zero for NLP.&nbsp; Read more: Assessing BERT&rsquo;s Syntactic Abilities (Arxiv).
Towards a future where all infrastructure is surveyed and analyzed by drones:&hellip;Radio instead of GPS, light drones, and a wind turbine&hellip;Researchers with Lulea University of Technology in Sweden have developed techniques to let small drones (sometimes called Micro Aerial Vehicles, or MAVs) autonomously inspect very large machines and/or buildings, such as wind turbines. The primary technical inventions outlined in the report are the creation of a localization technique to let multiple drones coordinate with eachother as they inspect something, as well as the creation of a path planning algorithm to help them not only inspect the structure, but also gather enough data &ldquo;to enable the generation of an off-line 3D model of the structure&rdquo;.&…