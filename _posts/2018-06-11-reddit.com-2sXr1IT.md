---

layout: post
category: threads
title: "[D] In DNNs, since Softmax is a generalization for the logistic function (sigmoid), does it suffer from the vanishing gradient problem?"
date: 2018-06-11 02:12:53
link: https://vrhk.co/2sXr1IT
image: https://www.redditstatic.com/icon.png
domain: reddit.com
author: "reddit"
icon: http://www.redditstatic.com/desktop2x/img/favicon/apple-icon-57x57.png
excerpt: "If yes, then why don't people talk about it? If no, then should I use Softmax in place of sigmoid even for binary classification problems?"

---

### [D] In DNNs, since Softmax is a generalization for the logistic function (sigmoid), does it suffer from the vanishing gradient problem? â€¢ r/MachineLearning

If yes, then why don't people talk about it? If no, then should I use Softmax in place of sigmoid even for binary classification problems?