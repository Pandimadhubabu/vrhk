---

layout: post
category: product
title: "Import AI: #89: Chinese facial recognition startup raises $600 million; why GPUs could alter AI progress; and using context to deal with language ambiguity"
date: 2018-04-09 18:17:02
link: https://vrhk.co/2H8qGMO
image: 
domain: jack-clark.net
author: "Jack Clark"
icon: https://s2.wp.com/i/webclip.png
excerpt: "Beating Moore&rsquo;s Law with GPUs:&hellip;Could a rise in GPU and other novel AI-substrates help deal with the decline of Moore&rsquo;s Law?&hellip;CPU performance has been stagnating for several years as it has become harder to improve linear execution pipelines across whole chips in relation to the reduction in transistor sizes, and the related problems which come from having an increasingly large number of things needing to work in lock-step with one another at minute scales. Could GPUs give us a way around this performance impasse? That&rsquo;s the idea in a new blog from AI researcher Bharath Ramsundar who thinks that increases in GPU capabilities and the arrival of semiconductor substrates specialized for deep learning means that we can expect performance of AI applications to increase in coming years faster than typical computing jobs running on typical processors. He might be right &ndash; one of the weird things about deep learning is that its most essential elements, like big blocks of neural networks, can be scaled up to immense sizes without terrible scaling tradeoffs as their innards consist of relatively simple and parallel tasks like matrix multiplication, so new chips can easily be networked together to further boost base capabilities. Plus, standardization in a few software libraries, like NVIDIA&rsquo;s cuDNN and CUDA GPU-interfaces, or the rise of TensorFlow for AI programming, means that some applications are getting faster over time purely as a consequence of software updates to these other fundamental improvements.&nbsp; Why it matters: Much of the recent progress in AI has occurred because around the mid-2000s processors became capable enough to easily train large neural networks on chunks of data &ndash; this underlying hardware improvement unlocked breakthroughs like the 2012 &lsquo;AlexNet&rsquo; result for image recognition, related work in speech recognition, and subsequently significant innovations in research (AlphaGo) and application (large-scale sequence-to-sequence learning for &lsquo;Smart Reply&rsquo;, or the emergence of neural translation systems. If the arrival of things like GPUs and further software standardization and innovation has a good chance of further boosting performance, then researchers will be able to explore even larger or more complex models in the future, as well as run things like neural architecture search at a higher rate, which should combine to further drive progress. &nbsp;&nbsp;Read more: The Advent of Huang&rsquo;s Law (Bharath Ramsundar blog post).
Microsoft launches AI training course including &lsquo;Ethics&rsquo; segment:&hellip;New Professional Program for Artificial Intelligence sees Microsoft get into the AI certification business&hellip;Microsoft has followed other companies in making its internal training courses available externally via the Microsoft Professional Program in AI. This program is based on internal training initiatives the software company developed to ramp up their own professional skills.&nbsp;The Microsoft course is all fairly typical, teaching people about Python, statistics, the construction and deployment of deep learning and reinforcement learning projects, and deployment. It also includes a specific &ldquo;Ethics and Law in Data and Analytics&rdquo; course, which promises to teach developers how to &lsquo;apply ethical and legal frameworks to initiatives in the data profession&rsquo;. &nbsp;&nbsp;Read more: Microsoft Professional Program for Artificial Intelligence (Microsoft). &nbsp;&nbsp;Read more: Aiming to fill skill gaps in AI, Microsoft makes training courses available to the public (Microsoft blog).
Learning to deal with ambiguity:&hellip;Researchers take charge of problem of word ambiguity via a charge at including more context&hellip;Carnegie Mellon University researchers have tackled one of the harder problems in translation: dealing with &lsquo;homographs&rsquo; &ndash; words that are spelled the same but have different meanings in different contexts, like &lsquo;room&rsquo; and &lsquo;charges&rsquo;. They do this in the context of neural machine translation (NMT) systems, which use machine learning techniques to accomplish translation with orders of magnitude fewer hand-specified rules than prior systems.&nbsp; Existing NMT systems struggle with homographs, with performance of word-level translation degrading as the number of potential meanings of each word climbs, the researchers show. They try to alleviate this by adding a word context vector that can be used by the NMT systems to learn the different uses of the same word. Adding this &lsquo;context network&rsquo; into their NMT architecture leads to significantly improved BLEU scores of sentences translated by the system. &nbsp;&nbsp;Why it matters: It&rsquo;s noteworthy that the system used by the researchers to deal with the homograph problem is itself a learned system which, rather than using hand-written rules, seeks to instead ingest more context about each word and learn from that. This is illustrative of how AI-first software systems get built: if you identify a fault you typically write a program which learns to fix it, rather than learning to write a rule-based program that fixes it.&nbsp; Read more: Handling Homographs in Neural Machine Translation (Arxiv).
Chinese facial recognition company raises $600 million:&hellip;SenseTime plans to use funds for five supercomputers for its AI services&hellip;SenseTime, a homegrown computer vision startup that provides facial recognition tools at vast scales, has raised $600 million in funding. The Chinese company supplies facial recognition services to the public and private sectors and is now, according to a co-founder, profitable and looking to expand. The company is now &ldquo;developing a service code-named &ldquo;vipar&rdquo; to parse data from thousands of live camera feeds&rdquo;, according to Bloomberg News. &nbsp;&nbsp;Strategic compute: SenseTime will use money from the financing &ldquo;to build at least five supercomputers in top-tier cities over the coming year to drive Viper and other services. As envisioned, it streams thousands of live feeds into a single system that&rsquo;re automatically processed and tagged, via devices from office face-scanners to ATMs and traffic cameras (so long as the resolution is high enough). The ultimate goal is to juggle 100,000 feeds simultaneously,&rdquo; according to Bloomberg news.&nbsp; Read more: China Now Has the Most Valuable AI Startup in the World (Bloomberg).&hellip;Related: Chinese startup uses AI to spot jaywalkers and send them pictures of their face:&hellip;Computer vision @ China scale&hellip;Chinese startup Intellifusion is helping the local government in Shenzhen use facial recognition in combination with widely deployed urban cameras to text jaywalkers pictures of their faces along with personal information after they&rsquo;ve been caught. &nbsp;&nbsp;Read more: China is using facial recognition technology to send jaywalkers fines through text messages (Motherboard).
Think China&rsquo;s strategic technology initiatives are new? Think again:&hellip;wide-ranging post by former Asia-focused State Department employee puts Beijing&rsquo;s AI push in historical context&hellip;Here&rsquo;s an old (August 2017) but good post from the Paulson Institute at the University of Chicago about the history of Chinese technology policy in light of the government&rsquo;s recent public statements about developing a national AI strategy. China&rsquo;s longstanding worldview with regards to its technology strategy is that technology is a source of national power and China needs to develop more of an indigenous Chinese capability. &nbsp;&nbsp;Based on previous initiatives, it looks likely China will seek to attain frontier capabilities in AI then package those capabilities up as products and use that to fund further&nbsp;research. &ldquo;Chinese government, industry, and scientific leaders will continu…"

---

### Import AI: #89: Chinese facial recognition startup raises $600 million; why GPUs could alter AI progress; and using context to deal with language ambiguity

Beating Moore&rsquo;s Law with GPUs:&hellip;Could a rise in GPU and other novel AI-substrates help deal with the decline of Moore&rsquo;s Law?&hellip;CPU performance has been stagnating for several years as it has become harder to improve linear execution pipelines across whole chips in relation to the reduction in transistor sizes, and the related problems which come from having an increasingly large number of things needing to work in lock-step with one another at minute scales. Could GPUs give us a way around this performance impasse? That&rsquo;s the idea in a new blog from AI researcher Bharath Ramsundar who thinks that increases in GPU capabilities and the arrival of semiconductor substrates specialized for deep learning means that we can expect performance of AI applications to increase in coming years faster than typical computing jobs running on typical processors. He might be right &ndash; one of the weird things about deep learning is that its most essential elements, like big blocks of neural networks, can be scaled up to immense sizes without terrible scaling tradeoffs as their innards consist of relatively simple and parallel tasks like matrix multiplication, so new chips can easily be networked together to further boost base capabilities. Plus, standardization in a few software libraries, like NVIDIA&rsquo;s cuDNN and CUDA GPU-interfaces, or the rise of TensorFlow for AI programming, means that some applications are getting faster over time purely as a consequence of software updates to these other fundamental improvements.&nbsp; Why it matters: Much of the recent progress in AI has occurred because around the mid-2000s processors became capable enough to easily train large neural networks on chunks of data &ndash; this underlying hardware improvement unlocked breakthroughs like the 2012 &lsquo;AlexNet&rsquo; result for image recognition, related work in speech recognition, and subsequently significant innovations in research (AlphaGo) and application (large-scale sequence-to-sequence learning for &lsquo;Smart Reply&rsquo;, or the emergence of neural translation systems. If the arrival of things like GPUs and further software standardization and innovation has a good chance of further boosting performance, then researchers will be able to explore even larger or more complex models in the future, as well as run things like neural architecture search at a higher rate, which should combine to further drive progress. &nbsp;&nbsp;Read more: The Advent of Huang&rsquo;s Law (Bharath Ramsundar blog post).
Microsoft launches AI training course including &lsquo;Ethics&rsquo; segment:&hellip;New Professional Program for Artificial Intelligence sees Microsoft get into the AI certification business&hellip;Microsoft has followed other companies in making its internal training courses available externally via the Microsoft Professional Program in AI. This program is based on internal training initiatives the software company developed to ramp up their own professional skills.&nbsp;The Microsoft course is all fairly typical, teaching people about Python, statistics, the construction and deployment of deep learning and reinforcement learning projects, and deployment. It also includes a specific &ldquo;Ethics and Law in Data and Analytics&rdquo; course, which promises to teach developers how to &lsquo;apply ethical and legal frameworks to initiatives in the data profession&rsquo;. &nbsp;&nbsp;Read more: Microsoft Professional Program for Artificial Intelligence (Microsoft). &nbsp;&nbsp;Read more: Aiming to fill skill gaps in AI, Microsoft makes training courses available to the public (Microsoft blog).
Learning to deal with ambiguity:&hellip;Researchers take charge of problem of word ambiguity via a charge at including more context&hellip;Carnegie Mellon University researchers have tackled one of the harder problems in translation: dealing with &lsquo;homographs&rsquo; &ndash; words that are spelled the same but have different meanings in different contexts, like &lsquo;room&rsquo; and &lsquo;charges&rsquo;. They do this in the context of neural machine translation (NMT) systems, which use machine learning techniques to accomplish translation with orders of magnitude fewer hand-specified rules than prior systems.&nbsp; Existing NMT systems struggle with homographs, with performance of word-level translation degrading as the number of potential meanings of each word climbs, the researchers show. They try to alleviate this by adding a word context vector that can be used by the NMT systems to learn the different uses of the same word. Adding this &lsquo;context network&rsquo; into their NMT architecture leads to significantly improved BLEU scores of sentences translated by the system. &nbsp;&nbsp;Why it matters: It&rsquo;s noteworthy that the system used by the researchers to deal with the homograph problem is itself a learned system which, rather than using hand-written rules, seeks to instead ingest more context about each word and learn from that. This is illustrative of how AI-first software systems get built: if you identify a fault you typically write a program which learns to fix it, rather than learning to write a rule-based program that fixes it.&nbsp; Read more: Handling Homographs in Neural Machine Translation (Arxiv).
Chinese facial recognition company raises $600 million:&hellip;SenseTime plans to use funds for five supercomputers for its AI services&hellip;SenseTime, a homegrown computer vision startup that provides facial recognition tools at vast scales, has raised $600 million in funding. The Chinese company supplies facial recognition services to the public and private sectors and is now, according to a co-founder, profitable and looking to expand. The company is now &ldquo;developing a service code-named &ldquo;vipar&rdquo; to parse data from thousands of live camera feeds&rdquo;, according to Bloomberg News. &nbsp;&nbsp;Strategic compute: SenseTime will use money from the financing &ldquo;to build at least five supercomputers in top-tier cities over the coming year to drive Viper and other services. As envisioned, it streams thousands of live feeds into a single system that&rsquo;re automatically processed and tagged, via devices from office face-scanners to ATMs and traffic cameras (so long as the resolution is high enough). The ultimate goal is to juggle 100,000 feeds simultaneously,&rdquo; according to Bloomberg news.&nbsp; Read more: China Now Has the Most Valuable AI Startup in the World (Bloomberg).&hellip;Related: Chinese startup uses AI to spot jaywalkers and send them pictures of their face:&hellip;Computer vision @ China scale&hellip;Chinese startup Intellifusion is helping the local government in Shenzhen use facial recognition in combination with widely deployed urban cameras to text jaywalkers pictures of their faces along with personal information after they&rsquo;ve been caught. &nbsp;&nbsp;Read more: China is using facial recognition technology to send jaywalkers fines through text messages (Motherboard).
Think China&rsquo;s strategic technology initiatives are new? Think again:&hellip;wide-ranging post by former Asia-focused State Department employee puts Beijing&rsquo;s AI push in historical context&hellip;Here&rsquo;s an old (August 2017) but good post from the Paulson Institute at the University of Chicago about the history of Chinese technology policy in light of the government&rsquo;s recent public statements about developing a national AI strategy. China&rsquo;s longstanding worldview with regards to its technology strategy is that technology is a source of national power and China needs to develop more of an indigenous Chinese capability. &nbsp;&nbsp;Based on previous initiatives, it looks likely China will seek to attain frontier capabilities in AI then package those capabilities up as products and use that to fund further&nbsp;research. &ldquo;Chinese government, industry, and scientific leaders will continu…