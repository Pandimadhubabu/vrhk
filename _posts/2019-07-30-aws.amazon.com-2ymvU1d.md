---

layout: post
category: engineering
title: "Running Amazon Elastic Inference Workloads on Amazon ECS"
date: 2019-07-30 16:21:29
link: https://amzn.to/2ymvU1d
image: https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2019/07/30/TensorFlow-Inference-Cost-Efficiency-with-EI-v2.gif
domain: aws.amazon.com
author: "Amazon Web Services"
icon: http://a0.awsstatic.com/main/images/site/touch-icon-iphone-114-smile.png
excerpt: "Amazon Elastic Inference (EI) is a new service launched at re:Invent 2018. Elastic Inference reduces the cost of running deep learning inference by up to 75% compared to using standalone GPU instances. Elastic Inference lets you attach accelerators to any Amazon SageMaker or Amazon EC2 instance type and run inference on TensorFlow, Apache MXNet, and ONNX […]"

---

### Running Amazon Elastic Inference Workloads on Amazon ECS | Amazon Web Services

Amazon Elastic Inference (EI) is a new service launched at re:Invent 2018. Elastic Inference reduces the cost of running deep learning inference by up to 75% compared to using standalone GPU instances. Elastic Inference lets you attach accelerators to any Amazon SageMaker or Amazon EC2 instance type and run inference on TensorFlow, Apache MXNet, and ONNX […]