---

layout: post
category: product
title: "Import AI: 117: Surveillance search engines; harvesting real-world road data with hovering drones; and improving language with unsupervised pre-training"
date: 2018-10-22 14:27:05
link: https://vrhk.co/2CxjtD7
image: 
domain: jack-clark.net
author: "Jack Clark"
icon: https://s2.wp.com/i/webclip.png
excerpt: "Chinese researchers pursue state-of-the-art lip-reading with massive dataset:&hellip;What do I spy with my camera eyes? Lips moving! Now I can figure out what you are saying&hellip;Researchers with the Chinese Academic of Sciences and Huazhong University of Science and Technology have created a new dataset and benchmark for &ldquo;lip-reading in the wild&rdquo; for Mandarin. Lip-reading gives people a new sensory capability to imbue AI systems with. For instance, lip-reading systems can be used for &ldquo;aids for hearing-impaired persons, analysis of silent movies, liveness verification in video authentication systems, and so on&rdquo; the researchers write.&nbsp; Dataset details: The lipreading dataset contains 745,187 distinct samples from more than 2,000 speakers, grouped into 1,000 classes, where each class corresponds to the syllable of a Mandarin word composed of one or several Chinese characters. &ldquo;To the best of our knowledge, this database is currently the largest word-level lipreading dataset and the only public large-scale Mandarin lipreading dataset&rdquo;, the researchers write. The dataset has also been designed to be dverse so the footage in it consists of multiple different people taken from multiple different camera angles, along with perspectives taken from television broadcasts. This diversity makes the benchmark more closely approximate real world situations whereas previous work in this domain has involved stuff taken from a fixed perspective. They build the dataset by annotating Chinese television using a service provided by iFLYTEK, a Chinese speech recognition company.&nbsp; Baseline results: They train three baselines on this dataset &ndash; a fully 2D CNN, a fully 3D CNN (modeled on LipNet, research covered in ImportAI #NUMBER from RESEARCHERS) , and a model that mixes 2D and 3D convolutional layers. All of these approaches perform poorly on the new dataset, despite having obtained performances as high as 90% on other more restricted datasets. The researchers implement their models in PyTorch and train them on servers containing four Titan X GPUs with 12GB of memory. The resulting top-5 accuracy results for the baselines on the new Chinese dataset LRW-1000 are as follows:&ndash; LSTM-5: 48.74%&ndash; D3D: 59.80%&ndash; 3D+2D: 63.50%&nbsp; Why it matters: Systems for stuff like lipreading are going to have a significant impact on applications ranging from medicine to surveillance. One of the challenges posed by research like this is its inherently &lsquo;dual use&rsquo; nature; as the researchers allude to in the introduction of this paper, this work can be used both for healthcare uses as well for surveillance uses (see: &ldquo;analysis of silent movies&rdquo;). How society deals with the arrival of these general AI technologies will have a significant impact on the types of societal architectures that will be built and developed throughout the 21st Century. It is also notable to see the emergence of large-scale datasets built by Chinese researchers in Chinese language &ndash; perhaps one could measure the relative growth in certain language datasets to model AI interest in the associated countries?&nbsp; Read more: LRW-1000: A Naturally Distributed Large-Scale Benchmark for Lip Reading in the Wild (Arxiv).
Want to use AI to study the earth? Enter the PROBA-V Super Resolution competition:&hellip;European Space Agency challenges researchers to increase the usefulness of satellite-gathered images&hellip;The European Space Agency has launched the &lsquo;PROBA-V Super Resolution&rdquo; competition, which challenges researchers to take in a bunch of photos from a satellite of the same region of the Earth and stitch them together to create a higher-resolution composite.&nbsp; Data: The data contains multiple images taken in different spectral bands of 74 locations around the world at each point in time. Images are annotated with a &lsquo;quality map&rsquo; to indicate any parts of them that may be occluded or otherwise hard to process. &ldquo;Each data-point consists of exactly one 100m resolution image and several 300m resolution images from the same scene,&rdquo; they write.&nbsp; Why it matters: Competitions like this provide researchers with novel datasets to experiment with and have a chance of improving the overall usefulness of expensive capital equipment (such as satellites).&nbsp;&nbsp;Find out more about the competition here at the official website (PROBA-V Super Resolution challenge).
Google releases BERT, obtains state-of-the-art language understanding scores:&hellip;Language modeling enters its ImageNet-boom era&hellip;Google has released BERT, a natural language processing system that uses unsupervised pre-training and task fine-tuning to obtain state-of-the-art scores on a large number of distinct tasks.&nbsp; How it works: BERT, which stands for Bidirectional Encoder Representations from Transformers, builds on recent developments in language understanding ranging from techniques like ELMO to ULMFiT to recent work by OpenAI on unsupervised pre-training. BERT&rsquo;s major performance gains come from a specific structural modification (jointly conditioning on the left and right context in all layers), as well as some other minor tweaks, plus &ndash; as is the trend in deep learning these days &ndash; training on a larger model using more compute. The approach it is most similar to is OpenAI&rsquo;s work using unsupervised pre-training for language understanding, as well as work from  using similar approaches.&nbsp; Major tweak: BERT&rsquo;s use of joint conditioning likely leads to its most significant performance improvement. They implement this by adding in an additional pre-training objective called the &lsquo;masked language model&rsquo; which involves randomly masking input tokens, then asking the model to predict the contents of the masked token based on context &ndash; this constraint encourages the network to learn to use more context when completing task, which seems to lead to greater representational capacity and improved performance. They also use Next Sentence Prediction during pre-training to try to train a model that has a concept of relationships of concepts across different sentences. Later they conduct significant ablation studies of BERT and show that these two pre-training tweaks are likely responsible for the majority of the observed performance increase.&nbsp; Results: BERT obtains state-of-the-art performance on the multi-task GLUE benchmark, setting new state-of-the-art scores on a wide range of challenging tasks. It also sets a new state-of-the-art score on the &lsquo;SWAG&rsquo; dataset &ndash; significant, given that SWAG was released earlier this year and was expressly designed to challenge AI techniques, like DL, which may gather a significant amount of performance by deriving subtle statistical relationships within datasets.&nbsp; Scale: The researchers train two models, BERTBASE and BERTLARGE. BERTBASE was trained on 4 Cloud TPUs for approximately four days, and BERTLARGE was trained on 16 Cloud TPUs also for four days.&nbsp; Why it matters &ndash; Big Compute and AI Feudalism: Approaches like this show how powerful today&rsquo;s deep learning based systems are, especially when combined with large amounts of compute and data. There are legitimate arguments to be made that such approaches are bifurcating research into low-compute and high-compute domains &ndash; one of these main BERT models took 16 TPUs (so 64 TPU chips total) trained for four days, putting it out of reach of low-resource researchers. On the plus side, if Google releases things like the pre-trained model then people will be able to use the model themselves and merely pay the training cost to finetune for different domains. Whether we should be content with researchers getting the proverbial crumbs from rich organizations&rsquo; tables is another matter, though. Maybe 2018 is the year in which we start to see th…"

---

### Import AI: 117: Surveillance search engines; harvesting real-world road data with hovering drones; and improving language with unsupervised pre-training

Chinese researchers pursue state-of-the-art lip-reading with massive dataset:&hellip;What do I spy with my camera eyes? Lips moving! Now I can figure out what you are saying&hellip;Researchers with the Chinese Academic of Sciences and Huazhong University of Science and Technology have created a new dataset and benchmark for &ldquo;lip-reading in the wild&rdquo; for Mandarin. Lip-reading gives people a new sensory capability to imbue AI systems with. For instance, lip-reading systems can be used for &ldquo;aids for hearing-impaired persons, analysis of silent movies, liveness verification in video authentication systems, and so on&rdquo; the researchers write.&nbsp; Dataset details: The lipreading dataset contains 745,187 distinct samples from more than 2,000 speakers, grouped into 1,000 classes, where each class corresponds to the syllable of a Mandarin word composed of one or several Chinese characters. &ldquo;To the best of our knowledge, this database is currently the largest word-level lipreading dataset and the only public large-scale Mandarin lipreading dataset&rdquo;, the researchers write. The dataset has also been designed to be dverse so the footage in it consists of multiple different people taken from multiple different camera angles, along with perspectives taken from television broadcasts. This diversity makes the benchmark more closely approximate real world situations whereas previous work in this domain has involved stuff taken from a fixed perspective. They build the dataset by annotating Chinese television using a service provided by iFLYTEK, a Chinese speech recognition company.&nbsp; Baseline results: They train three baselines on this dataset &ndash; a fully 2D CNN, a fully 3D CNN (modeled on LipNet, research covered in ImportAI #NUMBER from RESEARCHERS) , and a model that mixes 2D and 3D convolutional layers. All of these approaches perform poorly on the new dataset, despite having obtained performances as high as 90% on other more restricted datasets. The researchers implement their models in PyTorch and train them on servers containing four Titan X GPUs with 12GB of memory. The resulting top-5 accuracy results for the baselines on the new Chinese dataset LRW-1000 are as follows:&ndash; LSTM-5: 48.74%&ndash; D3D: 59.80%&ndash; 3D+2D: 63.50%&nbsp; Why it matters: Systems for stuff like lipreading are going to have a significant impact on applications ranging from medicine to surveillance. One of the challenges posed by research like this is its inherently &lsquo;dual use&rsquo; nature; as the researchers allude to in the introduction of this paper, this work can be used both for healthcare uses as well for surveillance uses (see: &ldquo;analysis of silent movies&rdquo;). How society deals with the arrival of these general AI technologies will have a significant impact on the types of societal architectures that will be built and developed throughout the 21st Century. It is also notable to see the emergence of large-scale datasets built by Chinese researchers in Chinese language &ndash; perhaps one could measure the relative growth in certain language datasets to model AI interest in the associated countries?&nbsp; Read more: LRW-1000: A Naturally Distributed Large-Scale Benchmark for Lip Reading in the Wild (Arxiv).
Want to use AI to study the earth? Enter the PROBA-V Super Resolution competition:&hellip;European Space Agency challenges researchers to increase the usefulness of satellite-gathered images&hellip;The European Space Agency has launched the &lsquo;PROBA-V Super Resolution&rdquo; competition, which challenges researchers to take in a bunch of photos from a satellite of the same region of the Earth and stitch them together to create a higher-resolution composite.&nbsp; Data: The data contains multiple images taken in different spectral bands of 74 locations around the world at each point in time. Images are annotated with a &lsquo;quality map&rsquo; to indicate any parts of them that may be occluded or otherwise hard to process. &ldquo;Each data-point consists of exactly one 100m resolution image and several 300m resolution images from the same scene,&rdquo; they write.&nbsp; Why it matters: Competitions like this provide researchers with novel datasets to experiment with and have a chance of improving the overall usefulness of expensive capital equipment (such as satellites).&nbsp;&nbsp;Find out more about the competition here at the official website (PROBA-V Super Resolution challenge).
Google releases BERT, obtains state-of-the-art language understanding scores:&hellip;Language modeling enters its ImageNet-boom era&hellip;Google has released BERT, a natural language processing system that uses unsupervised pre-training and task fine-tuning to obtain state-of-the-art scores on a large number of distinct tasks.&nbsp; How it works: BERT, which stands for Bidirectional Encoder Representations from Transformers, builds on recent developments in language understanding ranging from techniques like ELMO to ULMFiT to recent work by OpenAI on unsupervised pre-training. BERT&rsquo;s major performance gains come from a specific structural modification (jointly conditioning on the left and right context in all layers), as well as some other minor tweaks, plus &ndash; as is the trend in deep learning these days &ndash; training on a larger model using more compute. The approach it is most similar to is OpenAI&rsquo;s work using unsupervised pre-training for language understanding, as well as work from  using similar approaches.&nbsp; Major tweak: BERT&rsquo;s use of joint conditioning likely leads to its most significant performance improvement. They implement this by adding in an additional pre-training objective called the &lsquo;masked language model&rsquo; which involves randomly masking input tokens, then asking the model to predict the contents of the masked token based on context &ndash; this constraint encourages the network to learn to use more context when completing task, which seems to lead to greater representational capacity and improved performance. They also use Next Sentence Prediction during pre-training to try to train a model that has a concept of relationships of concepts across different sentences. Later they conduct significant ablation studies of BERT and show that these two pre-training tweaks are likely responsible for the majority of the observed performance increase.&nbsp; Results: BERT obtains state-of-the-art performance on the multi-task GLUE benchmark, setting new state-of-the-art scores on a wide range of challenging tasks. It also sets a new state-of-the-art score on the &lsquo;SWAG&rsquo; dataset &ndash; significant, given that SWAG was released earlier this year and was expressly designed to challenge AI techniques, like DL, which may gather a significant amount of performance by deriving subtle statistical relationships within datasets.&nbsp; Scale: The researchers train two models, BERTBASE and BERTLARGE. BERTBASE was trained on 4 Cloud TPUs for approximately four days, and BERTLARGE was trained on 16 Cloud TPUs also for four days.&nbsp; Why it matters &ndash; Big Compute and AI Feudalism: Approaches like this show how powerful today&rsquo;s deep learning based systems are, especially when combined with large amounts of compute and data. There are legitimate arguments to be made that such approaches are bifurcating research into low-compute and high-compute domains &ndash; one of these main BERT models took 16 TPUs (so 64 TPU chips total) trained for four days, putting it out of reach of low-resource researchers. On the plus side, if Google releases things like the pre-trained model then people will be able to use the model themselves and merely pay the training cost to finetune for different domains. Whether we should be content with researchers getting the proverbial crumbs from rich organizations&rsquo; tables is another matter, though. Maybe 2018 is the year in which we start to see th…