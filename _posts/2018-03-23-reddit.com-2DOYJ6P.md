---

layout: post
category: threads
title: "[D] Do you normalize your L2-Loss Function to the number of features (as well as the number of samples)?"
date: 2018-03-23 13:23:06
link: https://vrhk.co/2DOYJ6P
image: https://www.redditstatic.com/icon.png
domain: reddit.com
author: "reddit"
icon: http://www.redditstatic.com/desktop2x/img/favicon/apple-icon-57x57.png
excerpt: "In batch learning, the loss is usually the mean over the samples from the batch (or mini-batch). If several features need to be predicted (or you..."

---

### [D] Do you normalize your L2-Loss Function to the number of features (as well as the number of samples)? â€¢ r/MachineLearning

In batch learning, the loss is usually the mean over the samples from the batch (or mini-batch). If several features need to be predicted (or you...