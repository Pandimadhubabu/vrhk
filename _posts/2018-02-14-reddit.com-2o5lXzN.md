---

layout: post
category: threads
title: "[D] Distillation using L1 Loss only?"
date: 2018-02-14 05:18:01
link: https://vrhk.co/2o5lXzN
image: https://www.redditstatic.com/icon.png
domain: reddit.com
author: "reddit"
icon: http://www.redditstatic.com/desktop2x/img/favicon/apple-icon-57x57.png
excerpt: "Is it _correct_ way to distil knowledge from pre-trained teacher network to student network by simply minimizing L1 Loss between their outputs? I..."

---

### [D] Distillation using L1 Loss only? â€¢ r/MachineLearning

Is it _correct_ way to distil knowledge from pre-trained teacher network to student network by simply minimizing L1 Loss between their outputs? I...