---

layout: post
category: product
title: "Import AI 132: Can your algorithm outsmart ‘The Obstacle Tower’?; cross-domain NLP with bioBERT; and training on FaceForensics to spot deepfakes"
date: 2019-02-04 18:41:56
link: https://vrhk.co/2GkUk0e
image: 
domain: jack-clark.net
author: "Jack Clark"
icon: https://s2.wp.com/i/webclip.png
excerpt: "Think your algorithm is good at exploration? Enter &lsquo;The Obstacle Tower&rsquo;:&hellip;Now that Montezuma has been solved, we need to move on. Could &lsquo;The Obstacle Tower&rsquo; be the next challenge for people to grind their teeth over?&hellip;The Atari game Montezuma&rsquo;s Revenge loomed large in AI research for many years, challenging developers to come up with systems capable of unparallelled autonomous exploration and exploitation of simulated environments. But in 2018 multiple groups provided algorithms that were able to obtain human performance on the game (for instance: OpenAI via Random Network Distillation, and Uber via Go-Explore). Now, Unity Technologies has released a successor to Montezuma&rsquo;s Revenge called The Obstacle Tower, which is designed to be &ldquo;a broad and deep challenge, the solving of which would imply a major advancement in reinforcement learning&rdquo;, according to Unity. &nbsp;&nbsp;Enter&hellip;The Obstacle Tower! The game&rsquo;s features include: physics-driven interactions, high-quality graphics, procedural generation of levels, and variable textures. These traits create an environment that will probably demand agents develop sophisticated visuo-control policies combined with planning. &nbsp;&nbsp;Baseline results: Humans are able to &ndash; on average &ndash; reach the 15th floor of the game in two variants of the game, and reach the 9th floor in a hard variant called &ldquo;strong generalization&rdquo; (where the training occurs on separate environment seeds with separate visual themes). PPO and Rainbow &ndash; two contemporary powerful RL algorithms &ndash; do very badly on the game: PPO and Rainbow make it as far as floor 0.6 and 1.6 respectively in the &ldquo;strong generalization&rdquo; regime. In the easier regime, both algorithms only get as far as the fifth floor on average. &nbsp;&nbsp;Challenge: Unity and Google are challenging developers to program systems capable of climbing Obstacle Tower. The challenge commences on Monday February 11, 2019. &ldquo;The first-place entry will be awarded $10,000 in cash, up to $2,500 in credits towards travel to an AI/ML-focused conference, and credits redeemable at the Google Cloud Platform,&rdquo; according to the competition website.&nbsp; Why it matters: In AI research, benchmarks have typically motivated research progress. The Obstacle Tower looks to be hard enough to motivate the development of more capable algorithms, but is tractable enough that developers can get some signs of life by using today&rsquo;s systems. &nbsp;&nbsp;Read more about the challenge: Do you dare to challenge the Obstacle Tower? (Unity).&nbsp; &nbsp;Get the code for Obstacle Tower here (GitHub).&nbsp; &nbsp;Read the paper: The Obstacle Tower: A Generalization Challenge in Vision, Control, and Planning (research paper PDF hosted on Google Cloud Storage).
What big language models like BERT have to do with the future of AI:&hellip;BERT + specific subject (in this case, biomedical data) = high-performance, domain specific language-driven AI capabilities&hellip;Researchers with Korea University and startup Clova AI Research have taken BERT, a general purpose Transformer-based language model developed by Google, and trained it against specific datasets in the biomedical field. The result is a NLP model customized for biomedical tasks that the researchers finetune for Named Entity Recognition, Relation Extraction, and Question Answering.&nbsp;&nbsp;Large-scale pre-training: The original BERT system was pre-trained on Wikipedia (2.5 billion words) and BooksCorpus (0.8 billion words); BioBERT is pre-trained on these along with the PubMed and PMC corpora (4.5 billion words and 13.5 billion words, respectively). &nbsp;&nbsp;Results: BioBERT gets state-of-the-art scores in entity recognition against major datasets dealing with diseases, chemicals, genes and proteins. It also obtains state-of-the-art scores against three question answering tasks. Performance isn&rsquo;t universally good &ndash; BioBERT does significantly worse at a relation extraction task, among others tasks. &nbsp;&nbsp;Expensive: Training models at this scale isn&rsquo;t cheap: BioBERT &ldquo;trained for over 20 days with 8 V100 GPUs&rdquo;. And the researchers also lacked the compute resources to use the largest version of BERT for pre-training, they wrote. &nbsp;&nbsp;&hellip;But finetuning can be cheap: The researchers report that finetuning can take as little as an hour using a single NVIDIA Titan X card &ndash; this is due to the small size of the dataset, and the significant representational capacity of BioBERT as a consequence of large-scale pre-training. &nbsp;&nbsp;Why this matters: BioBERT represents a trend in research we&rsquo;re going to see repeated in 2019 and beyond: big company releases a computationally intensive model, other researchers customize this model against a specific context (typically via data augmentation and/or fine-tuning), then apply that model and obtain state-of-the-art scores in their domain. If you step back and consider the implicit power structure baked into this it can get a bit disturbing: this trend means an increasing chunk of research is dependent on the computational dividends of private AI developers. &nbsp;&nbsp;Read more: BioBERT: pre-trained biomedical language representation model for biomedical text mining (Arxiv).
FaceForensics: A dataset to distinguish between real and synthetic faces:&hellip;When is a human face not a human face? When it has been synthetically generated by an AI system&hellip;We&rsquo;re soon going to lose all trust in digital images and video as people use AI techniques to create synthetic people, or to fake existing people doing unusual things. Now, researchers with the Technical University of Munich, the University Federico II of Naples, and the University of Erlangen-Nuremberg have sought to save us from this info-apocalypse by releasing FaceForensics, &ldquo;a database of facial forgeries that enables researchers to train deep-learning-based approaches in a supervised fashion&rdquo;. &nbsp;&nbsp;FaceForensics dataset: The dataset contains 1,000 video sequences taken from YouTube videos of news or interview or video blog content. Each of these videos has three contemporary manipulation methods applied to it &ndash; Face2Face, FaceSwap, and Deepfakes. This quadruples the size of the dataset, creating three sets of 1,000 doctored sequences, as well as the raw ones. The sequences can be further split up into single images, yielding approximately ~500,000 un-modified and ~500,000 modified images.&nbsp; How good at humans are spotting doctored videos? In tests of 143 people, the researchers found that a human can tell real from fake 71% of the time when looking at high quality videos and 61% when studying low quality videos. &nbsp;&nbsp;Can AI detect fake AI? FaceForensics can be used to train systems to detect forged and non-forged images. &ldquo;Domain-specific information in combination with a XceptionNet classifier shows the best performance in each test,&rdquo; they write, after evaluating five potential fake-spotting techniques.&nbsp;&nbsp;Why this matters: It remains an open question as to whether fake imagery will be &lsquo;defense dominant&rsquo; or &lsquo;offense dominant&rsquo; in terms of who has the advantage (people creating these images, or those trying to spot them); research like this will help scientists better understand this dynamic, which can let them recommend more effective policies to governments to potentially regulate the malicious uses of this technology. &nbsp;&nbsp;Read more: FaceForensics++: Learning to Detect Manipulated Facial Images (Arxiv).
Google researches evolve the next version of the Transformer:&hellip;Using vast amounts of compute to create fundamental deep learning components provides further evidence AI research is splitting into small-compute and big-compute domains&hellip;How do you create a better dee…"

---

### Import AI 132: Can your algorithm outsmart ‘The Obstacle Tower’?; cross-domain NLP with bioBERT; and training on FaceForensics to spot deepfakes

Think your algorithm is good at exploration? Enter &lsquo;The Obstacle Tower&rsquo;:&hellip;Now that Montezuma has been solved, we need to move on. Could &lsquo;The Obstacle Tower&rsquo; be the next challenge for people to grind their teeth over?&hellip;The Atari game Montezuma&rsquo;s Revenge loomed large in AI research for many years, challenging developers to come up with systems capable of unparallelled autonomous exploration and exploitation of simulated environments. But in 2018 multiple groups provided algorithms that were able to obtain human performance on the game (for instance: OpenAI via Random Network Distillation, and Uber via Go-Explore). Now, Unity Technologies has released a successor to Montezuma&rsquo;s Revenge called The Obstacle Tower, which is designed to be &ldquo;a broad and deep challenge, the solving of which would imply a major advancement in reinforcement learning&rdquo;, according to Unity. &nbsp;&nbsp;Enter&hellip;The Obstacle Tower! The game&rsquo;s features include: physics-driven interactions, high-quality graphics, procedural generation of levels, and variable textures. These traits create an environment that will probably demand agents develop sophisticated visuo-control policies combined with planning. &nbsp;&nbsp;Baseline results: Humans are able to &ndash; on average &ndash; reach the 15th floor of the game in two variants of the game, and reach the 9th floor in a hard variant called &ldquo;strong generalization&rdquo; (where the training occurs on separate environment seeds with separate visual themes). PPO and Rainbow &ndash; two contemporary powerful RL algorithms &ndash; do very badly on the game: PPO and Rainbow make it as far as floor 0.6 and 1.6 respectively in the &ldquo;strong generalization&rdquo; regime. In the easier regime, both algorithms only get as far as the fifth floor on average. &nbsp;&nbsp;Challenge: Unity and Google are challenging developers to program systems capable of climbing Obstacle Tower. The challenge commences on Monday February 11, 2019. &ldquo;The first-place entry will be awarded $10,000 in cash, up to $2,500 in credits towards travel to an AI/ML-focused conference, and credits redeemable at the Google Cloud Platform,&rdquo; according to the competition website.&nbsp; Why it matters: In AI research, benchmarks have typically motivated research progress. The Obstacle Tower looks to be hard enough to motivate the development of more capable algorithms, but is tractable enough that developers can get some signs of life by using today&rsquo;s systems. &nbsp;&nbsp;Read more about the challenge: Do you dare to challenge the Obstacle Tower? (Unity).&nbsp; &nbsp;Get the code for Obstacle Tower here (GitHub).&nbsp; &nbsp;Read the paper: The Obstacle Tower: A Generalization Challenge in Vision, Control, and Planning (research paper PDF hosted on Google Cloud Storage).
What big language models like BERT have to do with the future of AI:&hellip;BERT + specific subject (in this case, biomedical data) = high-performance, domain specific language-driven AI capabilities&hellip;Researchers with Korea University and startup Clova AI Research have taken BERT, a general purpose Transformer-based language model developed by Google, and trained it against specific datasets in the biomedical field. The result is a NLP model customized for biomedical tasks that the researchers finetune for Named Entity Recognition, Relation Extraction, and Question Answering.&nbsp;&nbsp;Large-scale pre-training: The original BERT system was pre-trained on Wikipedia (2.5 billion words) and BooksCorpus (0.8 billion words); BioBERT is pre-trained on these along with the PubMed and PMC corpora (4.5 billion words and 13.5 billion words, respectively). &nbsp;&nbsp;Results: BioBERT gets state-of-the-art scores in entity recognition against major datasets dealing with diseases, chemicals, genes and proteins. It also obtains state-of-the-art scores against three question answering tasks. Performance isn&rsquo;t universally good &ndash; BioBERT does significantly worse at a relation extraction task, among others tasks. &nbsp;&nbsp;Expensive: Training models at this scale isn&rsquo;t cheap: BioBERT &ldquo;trained for over 20 days with 8 V100 GPUs&rdquo;. And the researchers also lacked the compute resources to use the largest version of BERT for pre-training, they wrote. &nbsp;&nbsp;&hellip;But finetuning can be cheap: The researchers report that finetuning can take as little as an hour using a single NVIDIA Titan X card &ndash; this is due to the small size of the dataset, and the significant representational capacity of BioBERT as a consequence of large-scale pre-training. &nbsp;&nbsp;Why this matters: BioBERT represents a trend in research we&rsquo;re going to see repeated in 2019 and beyond: big company releases a computationally intensive model, other researchers customize this model against a specific context (typically via data augmentation and/or fine-tuning), then apply that model and obtain state-of-the-art scores in their domain. If you step back and consider the implicit power structure baked into this it can get a bit disturbing: this trend means an increasing chunk of research is dependent on the computational dividends of private AI developers. &nbsp;&nbsp;Read more: BioBERT: pre-trained biomedical language representation model for biomedical text mining (Arxiv).
FaceForensics: A dataset to distinguish between real and synthetic faces:&hellip;When is a human face not a human face? When it has been synthetically generated by an AI system&hellip;We&rsquo;re soon going to lose all trust in digital images and video as people use AI techniques to create synthetic people, or to fake existing people doing unusual things. Now, researchers with the Technical University of Munich, the University Federico II of Naples, and the University of Erlangen-Nuremberg have sought to save us from this info-apocalypse by releasing FaceForensics, &ldquo;a database of facial forgeries that enables researchers to train deep-learning-based approaches in a supervised fashion&rdquo;. &nbsp;&nbsp;FaceForensics dataset: The dataset contains 1,000 video sequences taken from YouTube videos of news or interview or video blog content. Each of these videos has three contemporary manipulation methods applied to it &ndash; Face2Face, FaceSwap, and Deepfakes. This quadruples the size of the dataset, creating three sets of 1,000 doctored sequences, as well as the raw ones. The sequences can be further split up into single images, yielding approximately ~500,000 un-modified and ~500,000 modified images.&nbsp; How good at humans are spotting doctored videos? In tests of 143 people, the researchers found that a human can tell real from fake 71% of the time when looking at high quality videos and 61% when studying low quality videos. &nbsp;&nbsp;Can AI detect fake AI? FaceForensics can be used to train systems to detect forged and non-forged images. &ldquo;Domain-specific information in combination with a XceptionNet classifier shows the best performance in each test,&rdquo; they write, after evaluating five potential fake-spotting techniques.&nbsp;&nbsp;Why this matters: It remains an open question as to whether fake imagery will be &lsquo;defense dominant&rsquo; or &lsquo;offense dominant&rsquo; in terms of who has the advantage (people creating these images, or those trying to spot them); research like this will help scientists better understand this dynamic, which can let them recommend more effective policies to governments to potentially regulate the malicious uses of this technology. &nbsp;&nbsp;Read more: FaceForensics++: Learning to Detect Manipulated Facial Images (Arxiv).
Google researches evolve the next version of the Transformer:&hellip;Using vast amounts of compute to create fundamental deep learning components provides further evidence AI research is splitting into small-compute and big-compute domains&hellip;How do you create a better dee…