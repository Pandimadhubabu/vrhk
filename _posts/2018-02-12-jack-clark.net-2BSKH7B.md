---

layout: post
category: product
title: "Import AI: #81: Trading cryptocurrency with deep learning; Google shows why evolutionary methods beat RL (for now); and using iWatch telemetry for AI health diagnosis"
date: 2018-02-12 16:22:03
link: https://vrhk.co/2BSKH7B
image: 
domain: jack-clark.net
author: "Jack Clark"
icon: https://s0.wp.com/i/webclip.png
excerpt: "DeepMind&rsquo;s IMPALA tells us that transfer learning is starting to work:&hellip;Single reinforcement learning agent with same parameters solves a multitude of tasks, with the aid of a bunch of computers&hellip;DeepMind has published details on IMPALA, a single reinforcement learning agent that can master a suite of 30 3D-world tasks in &lsquo;DeepMind Lab&rsquo; as well as all 57 Atari games. The agent displays some competency at transfer learning, which means it&rsquo;s able to use knowledge gleaned from solving one task to solve another, increasing the sample efficiency of the algorithm.&nbsp;&nbsp;The technique: The Importance Weighted Actor-Learner Architecture (IMPALA) scales to multitudes of sub-agents (actors) deployed on thousands of machines which beam their experiences (sequences of states, actions, and rewards) back to a centralized learner, which uses GPUs to derive insights which are fed back to the agents. In the background it does some clever things with normalizing the learning of individual agents and the meta-agent to avoid temporal decoherence via a new off-policy actor-critic algorithm called V-trace. The outcome is an algorithm that can be far more sample efficient and performant than traditional RL algorithms like A2C.&nbsp; Datacenter-scale AI training: If you didn&rsquo;t think compute was the strategic determiner of AI research, then read this paper and consider your assumptions: IMPALA can achieve throughput rates of 250,000 frames per second via its large-scale, distributed implementation which involves 500 CPUS and 1 GPU assigned to each IMPALA agent. Such systems can achieve a throughput of 21 billion frames a day, DeepMind notes.Transfer learning: IMPALA agents can be trained on multiple tasks in parallel, attaining median scores on the full Atari-57 dataset of as high as 59.7% of human performance, roughly comparable to the performance of single-game trained simple A3C agents. There&rsquo;s obviously a ways to go before IMPALA transfer learning approaches are able to rival fine-tuned single environment implementations (which regularly far exceed human performance), but the indications are encouraging. Similarly competitive transfer-learning traits show up when they test it on a suite of 30 environments implemented in DeepMind Lab, the company&rsquo;s Quake-based 3D testing platform.Why it matters: Big computers are analogous to large telescopes with very fast turn rates, letting researchers probe the outer limits of certain testing regiments while being able to pivot across the entire scientific field of enquiry very rapidly. IMPALA is the sort of algorithm that organizations can design when they&rsquo;re able to tap into large fields of computation during research. &ldquo;The ability to train agents at this scale directly translates to very quick turnaround for investigating new ideas and opens up unexplored opportunities,&rdquo; DeepMind writes.Read more: IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures (Arxiv).
Dawn of the cryptocurrency AI agents: research paths for trading crypto via reinforcement learning:&hellip;Why crypto could be the ultimate testing ground for RL-based trading systems, and why this will require numerous fundamental research breakthroughs to succeed&hellip;AI chap Denny Britz has spent the past few months wondering what sorts of AI techniques could be applied to learning to profitably trade cryptocurrencies. &ldquo;It is quite similar to training agents for multiplayer games such as DotA, and many of the same research problems carry over. Knowing virtually nothing about trading, I have spent the past few months working on a project in this field,&rdquo; he writes.&nbsp;&nbsp;The face-ripping problems of trading: Many years ago I spent a few years working around one of the main financial trading centers of Europe: Canary Wharf in London, UK. A phrase I&rsquo;d often hear in the bars after work would be one trader remarking to another something to the nature of: &ldquo;I got my face ripped off today&rdquo;. Were these traders secretly involved in some kind of fantastically violent bloodsport, known only to them, my youthful self wondered? Not quite! What that phrase really means is that the financial markets are cruel, changeable, and, even when you have a good hunch or prediction, they can still betray you and destroy your trading book, despite you doing everything &lsquo;right&rsquo;. In this post former Google Brain chap Denny Britz does a good job of cautioning the would-be AI trader that cryptocurrencies are the same: even if you have the correct prediction, exogenous shocks beyond your control (trading latency, liquidity, etc), can destroy you in an instant. &ldquo;What is the lesson here? In order to make money from a simple price prediction strategy, we must predict relatively large price movements over longer periods of time, or be very smart about our fees and order management. And that&rsquo;s a very difficult prediction problem,&rdquo; he writes. So why not invent more complex strategies using AI tools, he suggests.Deep reinforcement learning for trading: Britz is keen on the idea of using deep reinforcement learning for trading because it can further remove the human from needing to design many of the precise trading strategies needed to profit in this kind of market. Additionally, it has the promise of being able to operate at shorter timescales than those which humans can take actions in. The catch is that you&rsquo;ll need to be able to build a simulator of the market you&rsquo;re trading in and try to make this simulator have the same sorts of patterns of data found in the real world, then you&rsquo;ll need to transfer your learned policy into a real market and hope that you haven&rsquo;t overfit. This is non-trivial. You&rsquo;ll also need to develop agents that can model other market participants and factor predictions about their actions into decision-making: another non-trivial problem.&nbsp; Read more here: Introduction to Learning to Trade with Reinforcement Learning.
Google researchers: In the battle between evolution and RL, evolution wins: fow now:&hellip;It takes a whole datacenter to raise a model&hellip;Last year, Google researchers caused a stir when they showed that you could use reinforcement learning to get computers to learn how to design better versions of image classifiers. At around the same time, other researchers showed you could use strategies based around evolutionary algorithms to do the same thing. But which is better? Google researchers have used their gigantic compute resources as the equivalent of a big telescope and found us the answer, lurking out there at vast compute scales.&nbsp; The result: Regularized evolutionary approaches (nicknamed: &lsquo;AmoebaNet&rsquo;) yield a new state-of-the-art on image classification on CIFAR-10, parity with RL approaches on ImageNet, and marginally higher performance on the mobile (aka lightweight) ImageNet. Evolution &ldquo;is either better than or equal to RL, with statistical significance &ldquo;when tested on &ldquo;small-scale&rdquo; aka single-CPU experiments. Evolution also increases its accuracy far more rapidly than RL during the initial stages of training. For large-scale experiments (450 GPUs (!!!) per experiment) they found that Evolution and RL do about the same, with evolution approaching higher accuracies at a faster rate than reinforcement learning systems. Additionally, evolved models make a drastically more efficient use of compute than their RL variants and obtain ever-so-slightly higher accuracies.&nbsp; The method: The researchers test RL and evolutionary approaches on designing a network composed of two fundamental modules: a normal cell and a reduction cell, which are stacked in feed-forward patterns to form an image classifier. They test two variants of evolution: non-regularized (kill the worst-performing network at each time period) and regul…"

---

### Import AI: #81: Trading cryptocurrency with deep learning; Google shows why evolutionary methods beat RL (for now); and using iWatch telemetry for AI health diagnosis

DeepMind&rsquo;s IMPALA tells us that transfer learning is starting to work:&hellip;Single reinforcement learning agent with same parameters solves a multitude of tasks, with the aid of a bunch of computers&hellip;DeepMind has published details on IMPALA, a single reinforcement learning agent that can master a suite of 30 3D-world tasks in &lsquo;DeepMind Lab&rsquo; as well as all 57 Atari games. The agent displays some competency at transfer learning, which means it&rsquo;s able to use knowledge gleaned from solving one task to solve another, increasing the sample efficiency of the algorithm.&nbsp;&nbsp;The technique: The Importance Weighted Actor-Learner Architecture (IMPALA) scales to multitudes of sub-agents (actors) deployed on thousands of machines which beam their experiences (sequences of states, actions, and rewards) back to a centralized learner, which uses GPUs to derive insights which are fed back to the agents. In the background it does some clever things with normalizing the learning of individual agents and the meta-agent to avoid temporal decoherence via a new off-policy actor-critic algorithm called V-trace. The outcome is an algorithm that can be far more sample efficient and performant than traditional RL algorithms like A2C.&nbsp; Datacenter-scale AI training: If you didn&rsquo;t think compute was the strategic determiner of AI research, then read this paper and consider your assumptions: IMPALA can achieve throughput rates of 250,000 frames per second via its large-scale, distributed implementation which involves 500 CPUS and 1 GPU assigned to each IMPALA agent. Such systems can achieve a throughput of 21 billion frames a day, DeepMind notes.Transfer learning: IMPALA agents can be trained on multiple tasks in parallel, attaining median scores on the full Atari-57 dataset of as high as 59.7% of human performance, roughly comparable to the performance of single-game trained simple A3C agents. There&rsquo;s obviously a ways to go before IMPALA transfer learning approaches are able to rival fine-tuned single environment implementations (which regularly far exceed human performance), but the indications are encouraging. Similarly competitive transfer-learning traits show up when they test it on a suite of 30 environments implemented in DeepMind Lab, the company&rsquo;s Quake-based 3D testing platform.Why it matters: Big computers are analogous to large telescopes with very fast turn rates, letting researchers probe the outer limits of certain testing regiments while being able to pivot across the entire scientific field of enquiry very rapidly. IMPALA is the sort of algorithm that organizations can design when they&rsquo;re able to tap into large fields of computation during research. &ldquo;The ability to train agents at this scale directly translates to very quick turnaround for investigating new ideas and opens up unexplored opportunities,&rdquo; DeepMind writes.Read more: IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures (Arxiv).
Dawn of the cryptocurrency AI agents: research paths for trading crypto via reinforcement learning:&hellip;Why crypto could be the ultimate testing ground for RL-based trading systems, and why this will require numerous fundamental research breakthroughs to succeed&hellip;AI chap Denny Britz has spent the past few months wondering what sorts of AI techniques could be applied to learning to profitably trade cryptocurrencies. &ldquo;It is quite similar to training agents for multiplayer games such as DotA, and many of the same research problems carry over. Knowing virtually nothing about trading, I have spent the past few months working on a project in this field,&rdquo; he writes.&nbsp;&nbsp;The face-ripping problems of trading: Many years ago I spent a few years working around one of the main financial trading centers of Europe: Canary Wharf in London, UK. A phrase I&rsquo;d often hear in the bars after work would be one trader remarking to another something to the nature of: &ldquo;I got my face ripped off today&rdquo;. Were these traders secretly involved in some kind of fantastically violent bloodsport, known only to them, my youthful self wondered? Not quite! What that phrase really means is that the financial markets are cruel, changeable, and, even when you have a good hunch or prediction, they can still betray you and destroy your trading book, despite you doing everything &lsquo;right&rsquo;. In this post former Google Brain chap Denny Britz does a good job of cautioning the would-be AI trader that cryptocurrencies are the same: even if you have the correct prediction, exogenous shocks beyond your control (trading latency, liquidity, etc), can destroy you in an instant. &ldquo;What is the lesson here? In order to make money from a simple price prediction strategy, we must predict relatively large price movements over longer periods of time, or be very smart about our fees and order management. And that&rsquo;s a very difficult prediction problem,&rdquo; he writes. So why not invent more complex strategies using AI tools, he suggests.Deep reinforcement learning for trading: Britz is keen on the idea of using deep reinforcement learning for trading because it can further remove the human from needing to design many of the precise trading strategies needed to profit in this kind of market. Additionally, it has the promise of being able to operate at shorter timescales than those which humans can take actions in. The catch is that you&rsquo;ll need to be able to build a simulator of the market you&rsquo;re trading in and try to make this simulator have the same sorts of patterns of data found in the real world, then you&rsquo;ll need to transfer your learned policy into a real market and hope that you haven&rsquo;t overfit. This is non-trivial. You&rsquo;ll also need to develop agents that can model other market participants and factor predictions about their actions into decision-making: another non-trivial problem.&nbsp; Read more here: Introduction to Learning to Trade with Reinforcement Learning.
Google researchers: In the battle between evolution and RL, evolution wins: fow now:&hellip;It takes a whole datacenter to raise a model&hellip;Last year, Google researchers caused a stir when they showed that you could use reinforcement learning to get computers to learn how to design better versions of image classifiers. At around the same time, other researchers showed you could use strategies based around evolutionary algorithms to do the same thing. But which is better? Google researchers have used their gigantic compute resources as the equivalent of a big telescope and found us the answer, lurking out there at vast compute scales.&nbsp; The result: Regularized evolutionary approaches (nicknamed: &lsquo;AmoebaNet&rsquo;) yield a new state-of-the-art on image classification on CIFAR-10, parity with RL approaches on ImageNet, and marginally higher performance on the mobile (aka lightweight) ImageNet. Evolution &ldquo;is either better than or equal to RL, with statistical significance &ldquo;when tested on &ldquo;small-scale&rdquo; aka single-CPU experiments. Evolution also increases its accuracy far more rapidly than RL during the initial stages of training. For large-scale experiments (450 GPUs (!!!) per experiment) they found that Evolution and RL do about the same, with evolution approaching higher accuracies at a faster rate than reinforcement learning systems. Additionally, evolved models make a drastically more efficient use of compute than their RL variants and obtain ever-so-slightly higher accuracies.&nbsp; The method: The researchers test RL and evolutionary approaches on designing a network composed of two fundamental modules: a normal cell and a reduction cell, which are stacked in feed-forward patterns to form an image classifier. They test two variants of evolution: non-regularized (kill the worst-performing network at each time period) and regul…