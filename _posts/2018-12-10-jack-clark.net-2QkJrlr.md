---

layout: post
category: product
title: "Import AI: 124: Google researchers produce metric that could help us track the evolution of fake video news; $4000 grants for people to teach deep learning; creating aggressive self-driving cars."
date: 2018-12-10 18:36:53
link: https://vrhk.co/2QkJrlr
image: 
domain: jack-clark.net
author: "Jack Clark"
icon: https://s2.wp.com/i/webclip.png
excerpt: "Using AI to learn to design networks with multiple constraints:&hellip;InstaNAS lets people add multiple specifications to neural architecture search&hellip;In the past couple of years researchers have started to use various AI techniques such as reinforcement learning and evolution to use AI to design neural network architectures. This has already yielded numerous systems that display state-of-the-art performance on challenging tasks like image recognition, outperforming systems designed specifically by humans.
More recently, we&rsquo;ve seen a further push to make such so-called &lsquo;neural architecture search&rsquo; (NAS) systems efficient, and approaches like ENAS (Import AI #82)&nbsp; and SMASH (Import AI #56) have shown how to take systems that previously required hundreds of GPUs and fit them onto one or two GPUs.&nbsp; Now, researchers are beginning to explore along another-axis of the NAS space: developing techniques that let them provide multiple objectives to the NAS system, letting them specify networks against different constraints. New research from National Tsing-Hua University in Taiwan and Google Research introduces InstaNAS, a system that lets people specify two categories of objectives as search targets, task-dependent objectives (eg, the accuracy in a given classification task) and architecture-level objectives (eg, latency/computational costs). &nbsp;&nbsp;How it works: Training InstaNAS systems involves three phases of work: pre-training a one-shot model, then introducing a controller which learns to select architectures from the one-shot model with respect to each input instance (during this stage, &ldquo;the controller and the one-shot model are being trained alternatively, which enforces the one-shot model to adapt to the distribution change of the controller&rdquo;, the researchers write), and a final stage in which the system picks the controller which best satisfies the constraints, then the one-shot model is re-trained with that high-performing controller.&nbsp; Results: Systems trained with InstaNAS achieve 48.9% and 40.2% average latency reduction on CIFAR-10 and CIFAR-100 against MobileNetV2 with comparable accuracy scores. Accuracies do take a slight hit (eg, the best accuracy on an InstaNAS system is approximately 95.7%, compared to 96.6% for a NAS-trained system.) &nbsp;&nbsp;Why it matters: As we industrialize artificial intelligence we&rsquo;re going to be offloading increasingly large chunks of AI development to AI systems themselves. The development and extension of NAS approaches will be crucial to this. Though we should bear in mind that there&rsquo;s an implicit electricityhuman brain tradeoff we&rsquo;re making here, and my intuition is that for some very large-scale NAS systems we could end up creating some hugely energy-hungry systems, which carry their own implicit (un-recognized) environmental externality.&nbsp; Read more: InstaNAS: Instance-aware Neural Architecture Search (Arxiv).
New metrics to let us work out when Fake Video News is going to become real:&hellip;With a little help from StarCraft 2!&hellip;Google Brain researchers have proposed a new metric to give researchers a better way to assess the quality of synthetically generated videos. The motivation for this research is that today we lack effective ways to assess and quantify improvements in synthetic video generation, and the history of the deep learning subfield within AI has tended to show the progress in a domain improves once the research community settles on a standard metric and/or dataset to use to assess progress. (My pet theory for why this is: There are so many AI papers published these days that researchers need simple heuristics to tell them whether to invest time in reading something, and progress against a generally agreed upon shared dataset can be a good input for this &ndash; eg, ImageNet (Image Rec), Penn Treebank (NLU), Switchboard Hub 500 (Speech Rec). ) &nbsp;&nbsp;The metric: Frechet Video Distance (FVD): FVD has been designed to give scores that reflect not only the quality of the video, but also its temporal coherence &ndash; aka, the way things transition from frame to frame. FVD is built around what the researchers call a &lsquo;Inflated 3D Convnet&rsquo;, which has been used to solve tasks in other challenging video domains. Because this network is trained to spot actions in videos it contains useful feature relations that correspond to sequences of movements over time. FVD uses an Inflated 3D Convnet, trained on the Kinetics data set of human-centered YouTube videos, to let FVD characterize the difference between the temporal transitions seen in the synthetic videos, and between its own feature representations of physical movements derived from the real world.&nbsp; The datasets: In tandem with FVD, the researchers introduce a new dataset based around StarCraft 2, a top-down real-time strategy game with lush, colorful graphics &ldquo;to serve as an intermediate step towards real world video data sets.&rdquo; These videos contain various different tasks in StarCraft 2 which are fairly self-explanatory &ndash; move unit to border; collect mineral shards; brawl; and road trip with medivac. The researchers provide 14,000 videos for each scenario. &nbsp;&nbsp;Results: FVD seems to be a metric that more closely tracks the scores humans give when performing a qualitative evaluation of synthetic videos. &ldquo;It is clear that FVD is better equipped to rank models according to human perception of quality&rdquo;.&nbsp; Why it matters: Synthetic videos are likely going to cause a large number of profound challenges in AI policy, as progression in this research domain yields immediate applications in the creation of automated propaganda. One of the most challenging things about this area &ndash; until now &ndash; has been the lack of available metrics to use to track progression here and thereby estimate when synthetic videos are likely going to become something &lsquo;good enough&rsquo; for people to worry about in domains outside of AI research. &ldquo;We believe that FVD and SCV will greatly benefit research in generative models of video in providing a well tailored, objective measure of progress,&rdquo; they write.&nbsp; &nbsp; &nbsp;Read more: Towards Accurate Generative Models of Video: A New Metric &amp; Challenges (Arxiv).&nbsp; &nbsp;Get the datasets from here.
Teaching self-driving cars how to drive aggressively:&hellip;Fusing deep learning and model predictive control for aggressive robot cars&hellip;Researchers with the Georgia Institute of Technology have created a self-driving car system that can successfully navigate a 1:5-scale &lsquo;AutoRally&rsquo; vehicle along a dirt track at high speeds. This type of work paves the way for a future where self-driving cars can go off-road, and gives us indications for how militaries might be developing their own stealthy unmanned ground vehicles (UGVs). &nbsp;&nbsp;How it works: Fusing deep learning and model predictive control: To create the system, the researchers feed visual inputs from a monocular camera into either a static supervised classifier or a recurrent LSTM (they switch between the two according to the difficulty of the particular section of the map the vehicle is on) which use this information to predict where the vehicle is against a pre-downloaded map schematic. They then feed this prediction into a GPU-based particle filter which incorporates data from the vehicle IMU and wheel speeds to further predict where the vehicle is on the map.&nbsp; Superhuman Wacky Races: The researchers test their system out on a complex dirt track on at the Georgia Tech Autonomous Racing Facility. This track &ldquo;includes turns of varying radius including a 180 degree hairpin and S curve, and a long straight section&rdquo;. The AutoRally car is able to &ldquo;repeatedly beat the best single lap performed by an experienced human test driver who provided all of the syste…"

---

### Import AI: 124: Google researchers produce metric that could help us track the evolution of fake video news; $4000 grants for people to teach deep learning; creating aggressive self-driving cars.

Using AI to learn to design networks with multiple constraints:&hellip;InstaNAS lets people add multiple specifications to neural architecture search&hellip;In the past couple of years researchers have started to use various AI techniques such as reinforcement learning and evolution to use AI to design neural network architectures. This has already yielded numerous systems that display state-of-the-art performance on challenging tasks like image recognition, outperforming systems designed specifically by humans.
More recently, we&rsquo;ve seen a further push to make such so-called &lsquo;neural architecture search&rsquo; (NAS) systems efficient, and approaches like ENAS (Import AI #82)&nbsp; and SMASH (Import AI #56) have shown how to take systems that previously required hundreds of GPUs and fit them onto one or two GPUs.&nbsp; Now, researchers are beginning to explore along another-axis of the NAS space: developing techniques that let them provide multiple objectives to the NAS system, letting them specify networks against different constraints. New research from National Tsing-Hua University in Taiwan and Google Research introduces InstaNAS, a system that lets people specify two categories of objectives as search targets, task-dependent objectives (eg, the accuracy in a given classification task) and architecture-level objectives (eg, latency/computational costs). &nbsp;&nbsp;How it works: Training InstaNAS systems involves three phases of work: pre-training a one-shot model, then introducing a controller which learns to select architectures from the one-shot model with respect to each input instance (during this stage, &ldquo;the controller and the one-shot model are being trained alternatively, which enforces the one-shot model to adapt to the distribution change of the controller&rdquo;, the researchers write), and a final stage in which the system picks the controller which best satisfies the constraints, then the one-shot model is re-trained with that high-performing controller.&nbsp; Results: Systems trained with InstaNAS achieve 48.9% and 40.2% average latency reduction on CIFAR-10 and CIFAR-100 against MobileNetV2 with comparable accuracy scores. Accuracies do take a slight hit (eg, the best accuracy on an InstaNAS system is approximately 95.7%, compared to 96.6% for a NAS-trained system.) &nbsp;&nbsp;Why it matters: As we industrialize artificial intelligence we&rsquo;re going to be offloading increasingly large chunks of AI development to AI systems themselves. The development and extension of NAS approaches will be crucial to this. Though we should bear in mind that there&rsquo;s an implicit electricityhuman brain tradeoff we&rsquo;re making here, and my intuition is that for some very large-scale NAS systems we could end up creating some hugely energy-hungry systems, which carry their own implicit (un-recognized) environmental externality.&nbsp; Read more: InstaNAS: Instance-aware Neural Architecture Search (Arxiv).
New metrics to let us work out when Fake Video News is going to become real:&hellip;With a little help from StarCraft 2!&hellip;Google Brain researchers have proposed a new metric to give researchers a better way to assess the quality of synthetically generated videos. The motivation for this research is that today we lack effective ways to assess and quantify improvements in synthetic video generation, and the history of the deep learning subfield within AI has tended to show the progress in a domain improves once the research community settles on a standard metric and/or dataset to use to assess progress. (My pet theory for why this is: There are so many AI papers published these days that researchers need simple heuristics to tell them whether to invest time in reading something, and progress against a generally agreed upon shared dataset can be a good input for this &ndash; eg, ImageNet (Image Rec), Penn Treebank (NLU), Switchboard Hub 500 (Speech Rec). ) &nbsp;&nbsp;The metric: Frechet Video Distance (FVD): FVD has been designed to give scores that reflect not only the quality of the video, but also its temporal coherence &ndash; aka, the way things transition from frame to frame. FVD is built around what the researchers call a &lsquo;Inflated 3D Convnet&rsquo;, which has been used to solve tasks in other challenging video domains. Because this network is trained to spot actions in videos it contains useful feature relations that correspond to sequences of movements over time. FVD uses an Inflated 3D Convnet, trained on the Kinetics data set of human-centered YouTube videos, to let FVD characterize the difference between the temporal transitions seen in the synthetic videos, and between its own feature representations of physical movements derived from the real world.&nbsp; The datasets: In tandem with FVD, the researchers introduce a new dataset based around StarCraft 2, a top-down real-time strategy game with lush, colorful graphics &ldquo;to serve as an intermediate step towards real world video data sets.&rdquo; These videos contain various different tasks in StarCraft 2 which are fairly self-explanatory &ndash; move unit to border; collect mineral shards; brawl; and road trip with medivac. The researchers provide 14,000 videos for each scenario. &nbsp;&nbsp;Results: FVD seems to be a metric that more closely tracks the scores humans give when performing a qualitative evaluation of synthetic videos. &ldquo;It is clear that FVD is better equipped to rank models according to human perception of quality&rdquo;.&nbsp; Why it matters: Synthetic videos are likely going to cause a large number of profound challenges in AI policy, as progression in this research domain yields immediate applications in the creation of automated propaganda. One of the most challenging things about this area &ndash; until now &ndash; has been the lack of available metrics to use to track progression here and thereby estimate when synthetic videos are likely going to become something &lsquo;good enough&rsquo; for people to worry about in domains outside of AI research. &ldquo;We believe that FVD and SCV will greatly benefit research in generative models of video in providing a well tailored, objective measure of progress,&rdquo; they write.&nbsp; &nbsp; &nbsp;Read more: Towards Accurate Generative Models of Video: A New Metric &amp; Challenges (Arxiv).&nbsp; &nbsp;Get the datasets from here.
Teaching self-driving cars how to drive aggressively:&hellip;Fusing deep learning and model predictive control for aggressive robot cars&hellip;Researchers with the Georgia Institute of Technology have created a self-driving car system that can successfully navigate a 1:5-scale &lsquo;AutoRally&rsquo; vehicle along a dirt track at high speeds. This type of work paves the way for a future where self-driving cars can go off-road, and gives us indications for how militaries might be developing their own stealthy unmanned ground vehicles (UGVs). &nbsp;&nbsp;How it works: Fusing deep learning and model predictive control: To create the system, the researchers feed visual inputs from a monocular camera into either a static supervised classifier or a recurrent LSTM (they switch between the two according to the difficulty of the particular section of the map the vehicle is on) which use this information to predict where the vehicle is against a pre-downloaded map schematic. They then feed this prediction into a GPU-based particle filter which incorporates data from the vehicle IMU and wheel speeds to further predict where the vehicle is on the map.&nbsp; Superhuman Wacky Races: The researchers test their system out on a complex dirt track on at the Georgia Tech Autonomous Racing Facility. This track &ldquo;includes turns of varying radius including a 180 degree hairpin and S curve, and a long straight section&rdquo;. The AutoRally car is able to &ldquo;repeatedly beat the best single lap performed by an experienced human test driver who provided all of the syste…