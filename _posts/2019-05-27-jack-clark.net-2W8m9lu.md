---

layout: post
category: product
title: "Import AI 148: Standardizing robotics research with Berkeley’s REPLAB; cheaper neural architecture search; and what a drone-racing benchmark says about dual use"
date: 2019-05-27 16:31:34
link: https://vrhk.co/2W8m9lu
image: 
domain: jack-clark.net
author: "Jack Clark"
icon: https://s2.wp.com/i/webclip.png
excerpt: "Standardizing physical robot testing with the Berkeley REPLAB:&hellip;What could help industrialize robotics+AI? An arm in a box, plus standardized software and testing!&hellip;Berkeley researchers have built REPLAB, a &ldquo;standardized and easily replicable hardware platform&rdquo; for benchmarking real-world robot performance. Something like REPLAB could be useful because it can bring standardization to how we test the increasingly advanced capabilities of robots equipped with AI. Today, if I want to get a sense for robot capabilities, I can go and read innumerable research papers that give me a sense of progress in simulated environments including simulated robots. What I can&rsquo;t do is go and read about performance of multiple real robots in real environments performing the same task &ndash; that&rsquo;s because of a lack of standardization of hardware, tasks, and testing regimes.
Introducing REPLAB: REPLAB consists of a module for real-world robot testing that contains a cheap robotic arm (specifically, a WidowX arm from Interbotix Labs) along with an RGB-D camera. The REPLAB is compact, with the researchers estimating you can fit up two 20 of the arm-containing cells in the same floor space as you&rsquo;d use for a single &lsquo;Baxter&rsquo; robotic arm. Each REPLAB costs about $2000 ($3000 if you buy some extra servos for the arm, to replace in case of equipment failures).
Reliability: During REPLAB development and testing, the researchers &ldquo;encountered no major breakages over more than 100,000 grasp attempts. No servos needed to be replaced. Repair maintenance work was largely limited to occasional tightening of screws and replacing frayed cables&rdquo;. Each cell was able to perform about 2,500 grasps per day &ldquo;with fewer than two interventions per cell per day on average&rdquo;.
Grasping benchmark: The testing platform is accompanied by a benchmark built around robotic grasping, and a dataset &ldquo;that can be used together with REPLAB to evaluate learning algorithms for robotic grasping&rdquo;. The dataset consists of ~92,000 randomly sampled grasps accompanied by labels connoting success or failure.
Why this matters: One indicator of the industrialization of AI is the proliferation of shared benchmarks and standardized testing means &ndash; I think of this as equivalent to how in the past we saw oil companies converge on similar infrastructures for labeling, analyzing, and shipping oil and oil information around the world. The fact we&rsquo;re now at the stage of researchers trying to create cheap, standardized testing platforms (see also: Berkeley&rsquo;s designed-for-mass-production &lsquo;BLUE&rsquo; robot, covered in Import AI #142.) is a further indication that robotics+AI is industrializing. &nbsp;&nbsp;Read more: REPLAB: A Reproducible Low-Cost Arm Benchmark Platform for Robotic Learning (Arxiv).
#####################################################
Chinese researchers fuse knowledge bases with big language models:&hellip;What comes after BERT? Tsinghua University thinks the answer might be &lsquo;ERNIE&rsquo;&hellip;Researchers with Tsinghua University and Huawei&rsquo;s Noah&rsquo;s Ark Lab have combined structured pools of knowledge with big, learned language models. Their system, called ERNIE (Enhanced Language RepresentatioN with Informative Entities), trains a Transformer-based language model so that, during training, it regularly tries to tie things it reads to entities stored in a structured knowledge graph. Pre-training with a big knowledge graph: To integrate external data sources, the researchers create an additional pre-training objective, which encourages the system to learn correspondences between various strings of tokens (eg Bob Dylan wrote Blowin&rsquo; in the Wind in 1962) and their entities (Bob Dylan, Blowin&rsquo; in the Wind). &ldquo;We design a new pre-training objective by randomly masking some of the named entity alignments in the input text and asking the model to select appropriate entities from KGs to complete the alignments,&rdquo; they write.
Data: During training, they pair text from Wikipedia with knowledge embeddings trained on Wikidata, which are used to identify the entities used within the knowledge graph.
Results: ERNIE obtains higher scores at entity-recognition tasks than BERT, chiefly due to less frequently learning incorrect labels compared to BERT (which helps it avoid over-fitting on wrong answers) &ndash; you&rsquo;d expect this, given the use of a structured dataset of entity names during training (though they also conduct an ablation study that confirms this as well &ndash; versions of ERnIE trained without an external dataset see their performance noticeably diminish). The system also does well on classifying the relationships between different entities, and in this domain continues to outperform BERT models.
Why this matters: NLP is going through a renaissance as researchers adapt semi-supervised learning techniques from other modalities, like images and audio, for text. The result has been the creation of multiple large-scale, general purpose language models (eg: ULMFiT, GPT-2, BERT) which display powerful capabilities as a consequence of being pre-trained on very large corpuses of text. But a problem with these models is that it&rsquo;s currently unclear how you get them to reliably learn certain things. One way to solve this is by stapling a module of facts into the system and forcing it, during pre-training, to try and map facts to entities it learns about &ndash; that&rsquo;s essentially what the researchers have done here, and it&rsquo;ll be interesting to see whether the approach of language model + knowledge base is successful in the long run, or if we&rsquo;ll just train sufficiently large language models that they&rsquo;ll autonomously create their own knowledge bases during training.&nbsp;&nbsp;Read more: ERNIE: Enhanced Language Representation with Informative Entities (Arxiv).
#####################################################
What happens if neural architecture search gets really, really cheap?&hellip;Chinese researchers seek to make neural architecture search more efficient&hellip;Researchers with the Chinese Academy of Sciences have trained an AI system to design a better AI system. Their work, Efficient Evolution of Neural Architecture (EENA), fits within the general area of neural architecture search. NAS is a sub-field within AI that has seen a lot of activity in recent years, following companies like Google showing that you can use techniques like reinforcement learning or evolutionary search to learn neural architectures that outperform those designed by humans. One problem with NAS approaches, though, is that they&rsquo;re typically very expensive &ndash; a neural architecture search paper from 2016 used 1800 GPU-days of computation to train a near-state-of-the-art CIFAR-10 image recognition model. EENA is one of a new crop of techniques (along with work by Google on Efficient Neural Architecture Search, or ENAS &ndash; see Import AI #124), meant to make such approaches far more computationally efficient. 
What&rsquo;s special about EENA: EENA isn&rsquo;t particularly special and the authors acknowledge this, noting that much of their work here has come from curating past techniques and figuring out the right cocktail of things to get the AI to learn. &ldquo;We absorb more blocks of classical networks such as dense block, add some effective changes such as noises for new parameters and discard several ineffective operations such as kernel widening in our method,&rdquo; they write. What&rsquo;s more significant is the general trend this implies &ndash; sophisticated AI developers seem to put enough value in NAS-based approaches that they&rsquo;re all working to make them cheaper to use.
Results: Their best-performing system obtains a 2.56% error rate when tested for how well it can classify images in the mid-size &lsquo;CIFAR-10&rsquo; datase…"

---

### Import AI 148: Standardizing robotics research with Berkeley’s REPLAB; cheaper neural architecture search; and what a drone-racing benchmark says about dual use

Standardizing physical robot testing with the Berkeley REPLAB:&hellip;What could help industrialize robotics+AI? An arm in a box, plus standardized software and testing!&hellip;Berkeley researchers have built REPLAB, a &ldquo;standardized and easily replicable hardware platform&rdquo; for benchmarking real-world robot performance. Something like REPLAB could be useful because it can bring standardization to how we test the increasingly advanced capabilities of robots equipped with AI. Today, if I want to get a sense for robot capabilities, I can go and read innumerable research papers that give me a sense of progress in simulated environments including simulated robots. What I can&rsquo;t do is go and read about performance of multiple real robots in real environments performing the same task &ndash; that&rsquo;s because of a lack of standardization of hardware, tasks, and testing regimes.
Introducing REPLAB: REPLAB consists of a module for real-world robot testing that contains a cheap robotic arm (specifically, a WidowX arm from Interbotix Labs) along with an RGB-D camera. The REPLAB is compact, with the researchers estimating you can fit up two 20 of the arm-containing cells in the same floor space as you&rsquo;d use for a single &lsquo;Baxter&rsquo; robotic arm. Each REPLAB costs about $2000 ($3000 if you buy some extra servos for the arm, to replace in case of equipment failures).
Reliability: During REPLAB development and testing, the researchers &ldquo;encountered no major breakages over more than 100,000 grasp attempts. No servos needed to be replaced. Repair maintenance work was largely limited to occasional tightening of screws and replacing frayed cables&rdquo;. Each cell was able to perform about 2,500 grasps per day &ldquo;with fewer than two interventions per cell per day on average&rdquo;.
Grasping benchmark: The testing platform is accompanied by a benchmark built around robotic grasping, and a dataset &ldquo;that can be used together with REPLAB to evaluate learning algorithms for robotic grasping&rdquo;. The dataset consists of ~92,000 randomly sampled grasps accompanied by labels connoting success or failure.
Why this matters: One indicator of the industrialization of AI is the proliferation of shared benchmarks and standardized testing means &ndash; I think of this as equivalent to how in the past we saw oil companies converge on similar infrastructures for labeling, analyzing, and shipping oil and oil information around the world. The fact we&rsquo;re now at the stage of researchers trying to create cheap, standardized testing platforms (see also: Berkeley&rsquo;s designed-for-mass-production &lsquo;BLUE&rsquo; robot, covered in Import AI #142.) is a further indication that robotics+AI is industrializing. &nbsp;&nbsp;Read more: REPLAB: A Reproducible Low-Cost Arm Benchmark Platform for Robotic Learning (Arxiv).
#####################################################
Chinese researchers fuse knowledge bases with big language models:&hellip;What comes after BERT? Tsinghua University thinks the answer might be &lsquo;ERNIE&rsquo;&hellip;Researchers with Tsinghua University and Huawei&rsquo;s Noah&rsquo;s Ark Lab have combined structured pools of knowledge with big, learned language models. Their system, called ERNIE (Enhanced Language RepresentatioN with Informative Entities), trains a Transformer-based language model so that, during training, it regularly tries to tie things it reads to entities stored in a structured knowledge graph. Pre-training with a big knowledge graph: To integrate external data sources, the researchers create an additional pre-training objective, which encourages the system to learn correspondences between various strings of tokens (eg Bob Dylan wrote Blowin&rsquo; in the Wind in 1962) and their entities (Bob Dylan, Blowin&rsquo; in the Wind). &ldquo;We design a new pre-training objective by randomly masking some of the named entity alignments in the input text and asking the model to select appropriate entities from KGs to complete the alignments,&rdquo; they write.
Data: During training, they pair text from Wikipedia with knowledge embeddings trained on Wikidata, which are used to identify the entities used within the knowledge graph.
Results: ERNIE obtains higher scores at entity-recognition tasks than BERT, chiefly due to less frequently learning incorrect labels compared to BERT (which helps it avoid over-fitting on wrong answers) &ndash; you&rsquo;d expect this, given the use of a structured dataset of entity names during training (though they also conduct an ablation study that confirms this as well &ndash; versions of ERnIE trained without an external dataset see their performance noticeably diminish). The system also does well on classifying the relationships between different entities, and in this domain continues to outperform BERT models.
Why this matters: NLP is going through a renaissance as researchers adapt semi-supervised learning techniques from other modalities, like images and audio, for text. The result has been the creation of multiple large-scale, general purpose language models (eg: ULMFiT, GPT-2, BERT) which display powerful capabilities as a consequence of being pre-trained on very large corpuses of text. But a problem with these models is that it&rsquo;s currently unclear how you get them to reliably learn certain things. One way to solve this is by stapling a module of facts into the system and forcing it, during pre-training, to try and map facts to entities it learns about &ndash; that&rsquo;s essentially what the researchers have done here, and it&rsquo;ll be interesting to see whether the approach of language model + knowledge base is successful in the long run, or if we&rsquo;ll just train sufficiently large language models that they&rsquo;ll autonomously create their own knowledge bases during training.&nbsp;&nbsp;Read more: ERNIE: Enhanced Language Representation with Informative Entities (Arxiv).
#####################################################
What happens if neural architecture search gets really, really cheap?&hellip;Chinese researchers seek to make neural architecture search more efficient&hellip;Researchers with the Chinese Academy of Sciences have trained an AI system to design a better AI system. Their work, Efficient Evolution of Neural Architecture (EENA), fits within the general area of neural architecture search. NAS is a sub-field within AI that has seen a lot of activity in recent years, following companies like Google showing that you can use techniques like reinforcement learning or evolutionary search to learn neural architectures that outperform those designed by humans. One problem with NAS approaches, though, is that they&rsquo;re typically very expensive &ndash; a neural architecture search paper from 2016 used 1800 GPU-days of computation to train a near-state-of-the-art CIFAR-10 image recognition model. EENA is one of a new crop of techniques (along with work by Google on Efficient Neural Architecture Search, or ENAS &ndash; see Import AI #124), meant to make such approaches far more computationally efficient. 
What&rsquo;s special about EENA: EENA isn&rsquo;t particularly special and the authors acknowledge this, noting that much of their work here has come from curating past techniques and figuring out the right cocktail of things to get the AI to learn. &ldquo;We absorb more blocks of classical networks such as dense block, add some effective changes such as noises for new parameters and discard several ineffective operations such as kernel widening in our method,&rdquo; they write. What&rsquo;s more significant is the general trend this implies &ndash; sophisticated AI developers seem to put enough value in NAS-based approaches that they&rsquo;re all working to make them cheaper to use.
Results: Their best-performing system obtains a 2.56% error rate when tested for how well it can classify images in the mid-size &lsquo;CIFAR-10&rsquo; datase…