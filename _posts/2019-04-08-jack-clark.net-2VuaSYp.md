---

layout: post
category: product
title: "Import AI 141: AIs play doom at thousands of frames per second; NeurIPS wants reproducible research; and Google creates&amp;scraps AI ethics council."
date: 2019-04-08 10:26:27
link: https://vrhk.co/2VuaSYp
image: 
domain: jack-clark.net
author: "Jack Clark"
icon: https://s2.wp.com/i/webclip.png
excerpt: "75 seconds: How long it takes to train a network against ImageNet:&hellip;Fujitsu Research claims state-of-the-art ImageNet training scheme&hellip;Researchers with Fujitsu Laboratories in Japan have further reduced the time it takes to train large-scale, supervised learning AI models; their approach lets them train a residual network to around 75% accuracy on the ImageNet dataset after 74.7 seconds of training time. This is a big leap from where we were in 2017 (an hour), and is impressive relative to late-2018 performance (around 4 minutes: see issue #121).
How they did it: The researchers trained their system across 2,048 Tesla V100 GPUs via the Amazon-developed MXNet deep learning framework. They used a large mini-batch size of 81,920, and also implemented layer-wise adaptive scaling (LARS) and a &lsquo;warming up&rsquo; period to increase learning efficiency.
Why it matters: Training large models on distributed infrastructure is a key component of modern AI research, and the reduction in time we&rsquo;ve seen on ImageNet training is striking &ndash; I think this is emblematic of the industrialization of AI, as people seek to create systematic approaches to efficiently training models across large amounts of computers. This trend ultimately leads to a speedup in the rate of research reliant on large-scale experimentation, and can unlock new paths of research. &nbsp;&nbsp;Read more: Yet Another Accelerated SGD: ResNet-50 Training on ImageNet in 74.7 seconds (Arxiv).
#####################################################
Ian &lsquo;GANfather&rsquo; Goodfellow heads to Apple:&hellip;Machine learning researcher swaps Google for Apple&hellip;Ian Goodfellow, a machine learning researcher who developed an AI approach called generative adversarial networks (GANs), is leaving Google for Apple.
Apple&rsquo;s deep learning training period: For the past few years, Apple has been trying to fill its ranks with more prominent people working on its AI projects. In 2016 it hired Russ Salakhutdinov, a researcher from CMU who had formerly studied under Geoffrey Hinton in Toronto, to direct its AI research efforts. Russ helped build up more of a traditional academic ML group at Apple, and Apple lifted its customary veil of secrecy a bit with the&nbsp;Apple Machine Learning Journal, a blog that details some of the research done by the secretive organization. Most recently, Apple hired John Giannandrea from Google to help lead its AI strategy. I hope Ian can push Apple towards being more discursive and open about aspects of its research, and I&rsquo;m curious to see what happens next.
Why this matters: Two of Ian&rsquo;s research interests &ndash; GANs and adversarial examples (manipulations made to data structures that cause neural networks to misclassify things) &ndash; have significant roles in AI policy, and I&rsquo;m wondering if Apple might explore this more through proactive work (making things safer and better) along with policy advocacy.&nbsp; Read more: One of Google&rsquo;s top A.I. people has joined Apple (CNBC).
#####################################################
World&rsquo;s most significant AI conference wants more reproducible research:&hellip;NeurIPS 2019 policy will have knock-on effect across wider AI ecosystem&hellip;The organizing committee for the Neural Information Processing Systems Conference (NeurIPS, formerly NIPS), has made two changes to submissions for the AI conference: A &ldquo;mandatory Reproducibility Checklist&rdquo;, along with &ldquo;a formal statement of expectations regarding the submission of code through a new Code Submission Policy&rdquo;.
Reproducibility checklist: Those submitting papers to NeurIPS will fill out a reproducibility checklist, originally developed by researcher Joelle Pineau. &ldquo;The answers will be available to reviewers and area chairs, who may use this information to help them assess the clarity and potential impact of submissions&rdquo;.
Code submissions: People will be expected (though not forced &ndash; yet) to submit code along with their papers, if they involve experiments that relate to a new algorithm or a modification of an existing one. &ldquo;It has become clear that this topic requires we move at a careful pace, as we learn where our &ldquo;comfort zone&rdquo; is as a community,&rdquo; the organizers write.
 &nbsp;&nbsp;Non-executable: Code submitted to NeurIPS won&rsquo;t need to be executable &ndash; this helps researchers whose work depends either on proprietary code (for instance, it plugs into a large-scale, proprietary training system, like those used by large technology companies), or who depend on proprietary datasets.
Why this matters: Reproducibility touches on many of the anxieties of current AI research relating to the difference in resources between academic researchers and those at corporate labs. Having more initiatives around reproducibility may help to close this divide, especially done in a (seemingly quite thoughtful) way that lets corporate researchers do things like publishing code without needing to worry about leaking information about internal proprietary infrastructure. &nbsp;&nbsp;Read more: Call for Papers (NeurIPS Medium page). &nbsp;&nbsp;Check out the code submission policy here (Google Doc).
#####################################################
Making RL research cheaper by using more efficient environments:&hellip;Want to train agents on a budget? Comfortable with your agents learning within a retro hell? Then ViZDoom might be the right choice for you&hellip;A team of researchers from INRIA in France have developed a set of tasks that demand &ldquo;complex reasoning and exploration&rdquo;, which can be run within the ViZDoom simulator at around 10,000 environment interactions per second; the goal of the project is to make it easier for people to do reinforcement learning research without spending massive amounts of compute.
Extending ViZDoom: ViZDoom is an implementation of the ancient first-person shooter game, Doom. However, one drawback is that it ships with only eight different scenarios to train agents in. To extend this, the researchers have developed four new scenarios designed to &ldquo;test navigation, reasoning, and memorization&rdquo;, variants of which can be procedurally generated.
Scenarios for creating thinking machines: These four scenarios include a navigation task called Labyrinth; Find and return, where the agents needs to find an object in the maze then return to its starting point; Ordered k-item, where the agent needs to collect a few different items in a predefined order; and Two color correlation, where an agent needs to explore a maze to find a column at its center, then pick up objects which are the same color as the column.
Spatial reasoning is&hellip; reassuringly difficult: &ldquo;The experiments on our proposed suite of benchmarks indicate that current state-of-the-art models and algorithms still struggle to learn complex tasks, involving several different objects in different places, and whose appearance and relationships to the task itself need to be learned from reward&rdquo;.&nbsp; Read more: Deep Reinforcement Learning on a Budget: 3D Control and Reasoning Without a Supercomputer (Arxiv).
######################################################Facebook wants to make smart robots, so it built them a habitat:&hellip;New open source research platform can conduct large-scale experiments, running 3D world simulators at thousands of frames per second&hellip;A team from Facebook, Georgia Institute of Technology, Simon Fraser University, Intel Labs, and Berkeley, have released Habitat, &ldquo;a platform for embodied AI research&rdquo;. The open source software is designed to help train agents for navigation and interaction tasks in a variety of domains, ranging from 3D environment simulators like Stanford&rsquo;s &lsquo;Gibson&rsquo; system or Matterport 3D to fully synthetic datasets like SUNCG. &nbsp;&nbsp;&ldquo;Our goal is to unify…"

---

### Import AI 141: AIs play doom at thousands of frames per second; NeurIPS wants reproducible research; and Google creates&amp;scraps AI ethics council.

75 seconds: How long it takes to train a network against ImageNet:&hellip;Fujitsu Research claims state-of-the-art ImageNet training scheme&hellip;Researchers with Fujitsu Laboratories in Japan have further reduced the time it takes to train large-scale, supervised learning AI models; their approach lets them train a residual network to around 75% accuracy on the ImageNet dataset after 74.7 seconds of training time. This is a big leap from where we were in 2017 (an hour), and is impressive relative to late-2018 performance (around 4 minutes: see issue #121).
How they did it: The researchers trained their system across 2,048 Tesla V100 GPUs via the Amazon-developed MXNet deep learning framework. They used a large mini-batch size of 81,920, and also implemented layer-wise adaptive scaling (LARS) and a &lsquo;warming up&rsquo; period to increase learning efficiency.
Why it matters: Training large models on distributed infrastructure is a key component of modern AI research, and the reduction in time we&rsquo;ve seen on ImageNet training is striking &ndash; I think this is emblematic of the industrialization of AI, as people seek to create systematic approaches to efficiently training models across large amounts of computers. This trend ultimately leads to a speedup in the rate of research reliant on large-scale experimentation, and can unlock new paths of research. &nbsp;&nbsp;Read more: Yet Another Accelerated SGD: ResNet-50 Training on ImageNet in 74.7 seconds (Arxiv).
#####################################################
Ian &lsquo;GANfather&rsquo; Goodfellow heads to Apple:&hellip;Machine learning researcher swaps Google for Apple&hellip;Ian Goodfellow, a machine learning researcher who developed an AI approach called generative adversarial networks (GANs), is leaving Google for Apple.
Apple&rsquo;s deep learning training period: For the past few years, Apple has been trying to fill its ranks with more prominent people working on its AI projects. In 2016 it hired Russ Salakhutdinov, a researcher from CMU who had formerly studied under Geoffrey Hinton in Toronto, to direct its AI research efforts. Russ helped build up more of a traditional academic ML group at Apple, and Apple lifted its customary veil of secrecy a bit with the&nbsp;Apple Machine Learning Journal, a blog that details some of the research done by the secretive organization. Most recently, Apple hired John Giannandrea from Google to help lead its AI strategy. I hope Ian can push Apple towards being more discursive and open about aspects of its research, and I&rsquo;m curious to see what happens next.
Why this matters: Two of Ian&rsquo;s research interests &ndash; GANs and adversarial examples (manipulations made to data structures that cause neural networks to misclassify things) &ndash; have significant roles in AI policy, and I&rsquo;m wondering if Apple might explore this more through proactive work (making things safer and better) along with policy advocacy.&nbsp; Read more: One of Google&rsquo;s top A.I. people has joined Apple (CNBC).
#####################################################
World&rsquo;s most significant AI conference wants more reproducible research:&hellip;NeurIPS 2019 policy will have knock-on effect across wider AI ecosystem&hellip;The organizing committee for the Neural Information Processing Systems Conference (NeurIPS, formerly NIPS), has made two changes to submissions for the AI conference: A &ldquo;mandatory Reproducibility Checklist&rdquo;, along with &ldquo;a formal statement of expectations regarding the submission of code through a new Code Submission Policy&rdquo;.
Reproducibility checklist: Those submitting papers to NeurIPS will fill out a reproducibility checklist, originally developed by researcher Joelle Pineau. &ldquo;The answers will be available to reviewers and area chairs, who may use this information to help them assess the clarity and potential impact of submissions&rdquo;.
Code submissions: People will be expected (though not forced &ndash; yet) to submit code along with their papers, if they involve experiments that relate to a new algorithm or a modification of an existing one. &ldquo;It has become clear that this topic requires we move at a careful pace, as we learn where our &ldquo;comfort zone&rdquo; is as a community,&rdquo; the organizers write.
 &nbsp;&nbsp;Non-executable: Code submitted to NeurIPS won&rsquo;t need to be executable &ndash; this helps researchers whose work depends either on proprietary code (for instance, it plugs into a large-scale, proprietary training system, like those used by large technology companies), or who depend on proprietary datasets.
Why this matters: Reproducibility touches on many of the anxieties of current AI research relating to the difference in resources between academic researchers and those at corporate labs. Having more initiatives around reproducibility may help to close this divide, especially done in a (seemingly quite thoughtful) way that lets corporate researchers do things like publishing code without needing to worry about leaking information about internal proprietary infrastructure. &nbsp;&nbsp;Read more: Call for Papers (NeurIPS Medium page). &nbsp;&nbsp;Check out the code submission policy here (Google Doc).
#####################################################
Making RL research cheaper by using more efficient environments:&hellip;Want to train agents on a budget? Comfortable with your agents learning within a retro hell? Then ViZDoom might be the right choice for you&hellip;A team of researchers from INRIA in France have developed a set of tasks that demand &ldquo;complex reasoning and exploration&rdquo;, which can be run within the ViZDoom simulator at around 10,000 environment interactions per second; the goal of the project is to make it easier for people to do reinforcement learning research without spending massive amounts of compute.
Extending ViZDoom: ViZDoom is an implementation of the ancient first-person shooter game, Doom. However, one drawback is that it ships with only eight different scenarios to train agents in. To extend this, the researchers have developed four new scenarios designed to &ldquo;test navigation, reasoning, and memorization&rdquo;, variants of which can be procedurally generated.
Scenarios for creating thinking machines: These four scenarios include a navigation task called Labyrinth; Find and return, where the agents needs to find an object in the maze then return to its starting point; Ordered k-item, where the agent needs to collect a few different items in a predefined order; and Two color correlation, where an agent needs to explore a maze to find a column at its center, then pick up objects which are the same color as the column.
Spatial reasoning is&hellip; reassuringly difficult: &ldquo;The experiments on our proposed suite of benchmarks indicate that current state-of-the-art models and algorithms still struggle to learn complex tasks, involving several different objects in different places, and whose appearance and relationships to the task itself need to be learned from reward&rdquo;.&nbsp; Read more: Deep Reinforcement Learning on a Budget: 3D Control and Reasoning Without a Supercomputer (Arxiv).
######################################################Facebook wants to make smart robots, so it built them a habitat:&hellip;New open source research platform can conduct large-scale experiments, running 3D world simulators at thousands of frames per second&hellip;A team from Facebook, Georgia Institute of Technology, Simon Fraser University, Intel Labs, and Berkeley, have released Habitat, &ldquo;a platform for embodied AI research&rdquo;. The open source software is designed to help train agents for navigation and interaction tasks in a variety of domains, ranging from 3D environment simulators like Stanford&rsquo;s &lsquo;Gibson&rsquo; system or Matterport 3D to fully synthetic datasets like SUNCG. &nbsp;&nbsp;&ldquo;Our goal is to unify…