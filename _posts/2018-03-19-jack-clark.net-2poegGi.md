---

layout: post
category: product
title: "Import AI: #86: Baidu releases a massive self-driving car dataset; DeepMind boosts AI capabilities via neural teachers; and what happens when AIs evolve to do dangerous, subversive things."
date: 2018-03-19 16:42:11
link: https://vrhk.co/2poegGi
image: 
domain: jack-clark.net
author: "Jack Clark"
icon: https://s0.wp.com/i/webclip.png
excerpt: "Boosting AI capabilities with neural teachers:&hellip;AKA, why my small student with multiple expert teachers beats your larger more well-resourced teacherless-student&hellip;Research from DeepMind shows how to boost the performance of a given agent on a task by transferring knowledge from a pre-trained &lsquo;teacher&rsquo; agent. The technique yields a significant speedup in training AI agents, and there&rsquo;s some evidence that agents that are taught attain higher performance than non-taught ones. The technique comes in two flavors: single teacher and multi-teacher; agents pretrained via multiple specialized teachers do better than ones trained by a single entity, as expected.&nbsp; Strange and subtle: The approach has a few traits that seem helpful for the development of more sophisticated AI agents: in one task DeepMind tests it on the agent needs to figure out how to use a short-term memory to be able to attain a high score. &lsquo;Small&rsquo; agents (which only have two convolutional layers) typically fail to learn to use a memory and therefore cannot achieve scores above a certain threshold, but by training a &lsquo;small&rsquo; agent with multiple specialized teachers the researchers create one that can succeed at the task. &ldquo;This is perhaps surprising because the kickstarting mechanism only guides the student agent in which action to take: it puts no constraint on how the student structures its internal memory state. However, the student can only predict the teacher&rsquo;s behaviour by remembering information from before the respawn, which seems to be enough supervisory signal to drive short-term memory formation. We find this a wonderful parallel with how the best human educators teach: not telling the student what to think, but simply putting the student in a fruitful position to learn for themselves,&rdquo; the researchers write.&nbsp; Why it matters: Trends like this suggest that scientists can speed their own research by using such pre-trained techniques to better evaluate new agents. This adds further credence to the notion that a key input to (some types of) AI research will shift to being compute from pre-labelled static datasets. Though it should be noted that data here is implicit in the form of a procedural, modifiable simulator that researchers can access). More speculatively, this means it may be possible to use mixtures of teachers to train complex agents that far exceed in capabilities any of their forebears &ndash; perhaps an area where the sum really will be greater than its parts.&nbsp;&nbsp;Read more: Kickstarting Deep Reinforcement Learning (Arxiv).
100,000+ developer survey shows AI concerns:&hellip;What developers think is dangerous and exciting, and who they think is responsible&hellip;Developer community StackOverflow has published the results of its annual survey of its community; this year it asked about AI:&ndash; What developers think is &ldquo;dangerous&rdquo; re AI: Increasing automation of jobs (40.8%)&ndash; What developers think is &ldquo;exciting&rdquo; re AI: AI surpassing human intelligence, aka the singularity (28%)&ndash; Who is responsible for considering the ramifications of AI:&nbsp; &nbsp;&ndash; The developers or the people creating the AI: 47.8%&nbsp; &nbsp;&ndash; A governmental or other regulatory body: 27.9%&ndash; Different roles = different concerns: People that identified as technical specialists tended to say they were more concerned about issues of fairness than the singularity, whereas designers and mobile developers tended to be more concerned about the singularity.&nbsp; Read more: Developer Survey Results 2018 (StackOverFlow).
Baidu &amp; Toyota and Berkeley researchers organize self-driving car challenge backed by new self-driving car dataset from Baidu:\&hellip;&rdquo;ApolloScape&rdquo; adds Chinese data for self-driving car researchers, plus Baidu says it has joined Berkeley&rsquo;s &ldquo;DeepDrive&rdquo; self-driving car AI coalition&hellip;A new competition and dataset may give researchers a better way to measure the capabilities and progression of autonomous car AI.&nbsp; The dataset: The &lsquo;ApolloScape&rsquo; dataset from Baidu contains ~200,000 RGB images with corresponding pixel-by-pixel semantic annotation. Each frame is labeled from a set of 25 semantic classes that include: cars, motorcycles, sidewalks, traffic cones, trash cans, vegetation, and so on. Each of the images has a resolution of 3384 x 2710, and each frame is separated by a meter of distance. 80,000 images have been released as of March 8 2018.&nbsp;&nbsp;Read more about the dataset (potentially via Google Translate) here.&nbsp; Additional information: Many of the researchers linked to ApolloScape will be talking at a session on autonomous cars at the IEEE Intelligent Vehicles Symposium in China.&nbsp;&nbsp;Competition: The new &lsquo;WAD&rsquo; competition will give people a chance to test and develop AI systems on the ApolloScape dataset as well as a dataset from Berkeley DeepDrive (the DeepDrive dataset consists of 100,000 video clips, each about 40 seconds long, with one key frame from each clip annotated). There is about $10,000 in cash prizes available, and the researchers are soliciting papers on research techniques in: drivable area segmentation (being able to figure out which bits of a scene correspond to which label and which of these areas are safe); road object detection (figuring out what is on the road); and transfer learning from one semantic domain to another, specifically going from training on the Berkeley dataset (filmed in California, USA) to the ApolloScape dataset (filmed in Beijing, China).&nbsp; &nbsp;Read more about the &lsquo;WAD&rsquo; competition here.
Microsoft releases a &lsquo;Rosetta Stone&rsquo; for deep learning frameworks:&hellip;GitHub repo gives you a couple of basic operations displayed in many different ways&hellip;Microsoft has released a GitHub repository containing similar algorithms implemented in a variety of frameworks, including: Caffe2, Chainer, CNTK, Gluon, Keras (with backends CNTK/TensorFlow/Theano), Tensorflow, Lasagna, MXNet, PyTorch, and Julia &ndash; Knet. The idea here is that if you read one algorithm in one of these frameworks you&rsquo;ll be able to use that knowledge to understand the other frameworks.&nbsp;&nbsp;&ldquo;The repo we are releasing as a full version 1.0 today is like a Rosetta Stone for deep learning frameworks, showing the model building process end to end in the different frameworks,&rdquo; write the researchers in a blog post that also provides some rough benchmarking for training time for a CNN and an RNN.&nbsp; Read more: Comparing Deep Learning Frameworks: A Rosetta Stone Approach (Microsoft Tech Net).&nbsp;&nbsp;View the code examples (GitHub).
Evolution&rsquo;s weird, wonderful, and potentially dangerous implications for AI agent design:&hellip;And why the AI safety community may be able to learn from evolution&hellip;A consortium of international researchers have published some of the weird, infuriating, and frequently funny ways in which evolutionary algorithms have figured out non-obvious solutions and hacks to tasks they&rsquo;re asked to solve. The paper includes an illuminating set of examples of ways in which algorithms have subverted the wishes of their human overseers, including:&ndash; Opportunistic Somersaulting: When trying to evolve creatures to jump, some agents discovered that they could instead evolve very tall bodies and then somersault, gaining a reward in proportion to their feet gaining distance from the floor.&ndash; Pointless Programs: When researchers tried to evolve code with GenProg to solve a buggy data sorting program, GenProg evolved a solution that had the buggy program return an empty list, which wasn&rsquo;t scored negatively as an empty list can&rsquo;t be out of order as it contains nothing to order.&ndash; Physics Hacking: One robot figured out the correct vibrational frequency to su…"

---

### Import AI: #86: Baidu releases a massive self-driving car dataset; DeepMind boosts AI capabilities via neural teachers; and what happens when AIs evolve to do dangerous, subversive things.

Boosting AI capabilities with neural teachers:&hellip;AKA, why my small student with multiple expert teachers beats your larger more well-resourced teacherless-student&hellip;Research from DeepMind shows how to boost the performance of a given agent on a task by transferring knowledge from a pre-trained &lsquo;teacher&rsquo; agent. The technique yields a significant speedup in training AI agents, and there&rsquo;s some evidence that agents that are taught attain higher performance than non-taught ones. The technique comes in two flavors: single teacher and multi-teacher; agents pretrained via multiple specialized teachers do better than ones trained by a single entity, as expected.&nbsp; Strange and subtle: The approach has a few traits that seem helpful for the development of more sophisticated AI agents: in one task DeepMind tests it on the agent needs to figure out how to use a short-term memory to be able to attain a high score. &lsquo;Small&rsquo; agents (which only have two convolutional layers) typically fail to learn to use a memory and therefore cannot achieve scores above a certain threshold, but by training a &lsquo;small&rsquo; agent with multiple specialized teachers the researchers create one that can succeed at the task. &ldquo;This is perhaps surprising because the kickstarting mechanism only guides the student agent in which action to take: it puts no constraint on how the student structures its internal memory state. However, the student can only predict the teacher&rsquo;s behaviour by remembering information from before the respawn, which seems to be enough supervisory signal to drive short-term memory formation. We find this a wonderful parallel with how the best human educators teach: not telling the student what to think, but simply putting the student in a fruitful position to learn for themselves,&rdquo; the researchers write.&nbsp; Why it matters: Trends like this suggest that scientists can speed their own research by using such pre-trained techniques to better evaluate new agents. This adds further credence to the notion that a key input to (some types of) AI research will shift to being compute from pre-labelled static datasets. Though it should be noted that data here is implicit in the form of a procedural, modifiable simulator that researchers can access). More speculatively, this means it may be possible to use mixtures of teachers to train complex agents that far exceed in capabilities any of their forebears &ndash; perhaps an area where the sum really will be greater than its parts.&nbsp;&nbsp;Read more: Kickstarting Deep Reinforcement Learning (Arxiv).
100,000+ developer survey shows AI concerns:&hellip;What developers think is dangerous and exciting, and who they think is responsible&hellip;Developer community StackOverflow has published the results of its annual survey of its community; this year it asked about AI:&ndash; What developers think is &ldquo;dangerous&rdquo; re AI: Increasing automation of jobs (40.8%)&ndash; What developers think is &ldquo;exciting&rdquo; re AI: AI surpassing human intelligence, aka the singularity (28%)&ndash; Who is responsible for considering the ramifications of AI:&nbsp; &nbsp;&ndash; The developers or the people creating the AI: 47.8%&nbsp; &nbsp;&ndash; A governmental or other regulatory body: 27.9%&ndash; Different roles = different concerns: People that identified as technical specialists tended to say they were more concerned about issues of fairness than the singularity, whereas designers and mobile developers tended to be more concerned about the singularity.&nbsp; Read more: Developer Survey Results 2018 (StackOverFlow).
Baidu &amp; Toyota and Berkeley researchers organize self-driving car challenge backed by new self-driving car dataset from Baidu:\&hellip;&rdquo;ApolloScape&rdquo; adds Chinese data for self-driving car researchers, plus Baidu says it has joined Berkeley&rsquo;s &ldquo;DeepDrive&rdquo; self-driving car AI coalition&hellip;A new competition and dataset may give researchers a better way to measure the capabilities and progression of autonomous car AI.&nbsp; The dataset: The &lsquo;ApolloScape&rsquo; dataset from Baidu contains ~200,000 RGB images with corresponding pixel-by-pixel semantic annotation. Each frame is labeled from a set of 25 semantic classes that include: cars, motorcycles, sidewalks, traffic cones, trash cans, vegetation, and so on. Each of the images has a resolution of 3384 x 2710, and each frame is separated by a meter of distance. 80,000 images have been released as of March 8 2018.&nbsp;&nbsp;Read more about the dataset (potentially via Google Translate) here.&nbsp; Additional information: Many of the researchers linked to ApolloScape will be talking at a session on autonomous cars at the IEEE Intelligent Vehicles Symposium in China.&nbsp;&nbsp;Competition: The new &lsquo;WAD&rsquo; competition will give people a chance to test and develop AI systems on the ApolloScape dataset as well as a dataset from Berkeley DeepDrive (the DeepDrive dataset consists of 100,000 video clips, each about 40 seconds long, with one key frame from each clip annotated). There is about $10,000 in cash prizes available, and the researchers are soliciting papers on research techniques in: drivable area segmentation (being able to figure out which bits of a scene correspond to which label and which of these areas are safe); road object detection (figuring out what is on the road); and transfer learning from one semantic domain to another, specifically going from training on the Berkeley dataset (filmed in California, USA) to the ApolloScape dataset (filmed in Beijing, China).&nbsp; &nbsp;Read more about the &lsquo;WAD&rsquo; competition here.
Microsoft releases a &lsquo;Rosetta Stone&rsquo; for deep learning frameworks:&hellip;GitHub repo gives you a couple of basic operations displayed in many different ways&hellip;Microsoft has released a GitHub repository containing similar algorithms implemented in a variety of frameworks, including: Caffe2, Chainer, CNTK, Gluon, Keras (with backends CNTK/TensorFlow/Theano), Tensorflow, Lasagna, MXNet, PyTorch, and Julia &ndash; Knet. The idea here is that if you read one algorithm in one of these frameworks you&rsquo;ll be able to use that knowledge to understand the other frameworks.&nbsp;&nbsp;&ldquo;The repo we are releasing as a full version 1.0 today is like a Rosetta Stone for deep learning frameworks, showing the model building process end to end in the different frameworks,&rdquo; write the researchers in a blog post that also provides some rough benchmarking for training time for a CNN and an RNN.&nbsp; Read more: Comparing Deep Learning Frameworks: A Rosetta Stone Approach (Microsoft Tech Net).&nbsp;&nbsp;View the code examples (GitHub).
Evolution&rsquo;s weird, wonderful, and potentially dangerous implications for AI agent design:&hellip;And why the AI safety community may be able to learn from evolution&hellip;A consortium of international researchers have published some of the weird, infuriating, and frequently funny ways in which evolutionary algorithms have figured out non-obvious solutions and hacks to tasks they&rsquo;re asked to solve. The paper includes an illuminating set of examples of ways in which algorithms have subverted the wishes of their human overseers, including:&ndash; Opportunistic Somersaulting: When trying to evolve creatures to jump, some agents discovered that they could instead evolve very tall bodies and then somersault, gaining a reward in proportion to their feet gaining distance from the floor.&ndash; Pointless Programs: When researchers tried to evolve code with GenProg to solve a buggy data sorting program, GenProg evolved a solution that had the buggy program return an empty list, which wasn&rsquo;t scored negatively as an empty list can&rsquo;t be out of order as it contains nothing to order.&ndash; Physics Hacking: One robot figured out the correct vibrational frequency to su…