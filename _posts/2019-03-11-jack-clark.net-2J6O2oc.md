---

layout: post
category: product
title: "Import AI 137: DeepMind uses (Google) StreetLearn to learn to navigate cities; NeuroCuts learns decent packet classification; plus a 490k labelled image dataset"
date: 2019-03-11 16:56:58
link: https://vrhk.co/2J6O2oc
image: 
domain: jack-clark.net
author: "Jack Clark"
icon: https://s2.wp.com/i/webclip.png
excerpt: "The robots of the future will learn by playing, Google says:&hellip;Want to solve tasks effectively? Don&rsquo;t try to solve tasks during training!&hellip;Researchers with Google Brain have shown how to make robots smarter by showing them what it means to play without a goal in mind. Google does this by collecting a dataset via people tele-operating a robot in simulation. During these periods of teleoperation, the people are playing around, using the robot hand and arm to interact with the world around them without a specific goal in mind, so in one scene a person might pick up a random object, in another they might fiddle around with a door on a piece of furniture, and so on.
Google saves this data, calling it &lsquo;Learning from Play data&rsquo; (LfP). It fees this into a system that attempts to classify such playful sequences of actions, mapping them into a latent space. Meanwhile, another module in the system tries to look across the latent space and propose sequences of actions that could shift the robot from its current state to its goal state.
Multi-task training: Google evaluates this approach by comparing performance of robots trained with play data, from policies that use behavioural cloning to learn to complete tasks based on specific demonstration data. The tests show that robots which learn from play data are more robust to perturbations than ones trained without, and typically reach higher success rates on most tasks.&nbsp; Intriguingly, systems trained with play data display some over desirable traits: &ldquo;We find qualitative evidence that play-supervised models make multiple attempts to retry the task after initial failure&rdquo;, the researchers write. &ldquo;Surprisingly we find that its latent plan space learns to embed task semantics despite never being trained with task labels&rdquo;.
Why this matters: Gathering data for robotics work tends to be expensive, difficult, and prone to distribution problems (you can gather a lot of data, but you may subsequently discover that some quirk of the task or your robot platform means you need to go and re-gather a slightly different type of data). Being able to instead have robots learn behaviors primarily through cheaply-gathered non-goal-oriented play data will make it easier for people to experiment with developing such systems, and could make it easier to create large datasets shared between multiple parties. What might the &lsquo;ImageNet&rsquo; for play robotics look like, I wonder? &nbsp;&nbsp;Read more: Learning Latent Plans from Play (Arxiv).
#####################################################
Google teaches kids to read with AI-infused &lsquo;Bolo&rsquo;:&hellip;Tuition app ships with speech recognition and text-to-speech tech&hellip;Google has released Bolo, a mobile app for Android designed to help Indian children learn to read. Bolo ships with &lsquo;Diya&rsquo;, a software agent that can help children learn to read.
Bilingual: &ldquo;Diya can not only read out the text to your child, but also explain the meaning of English text in Hindi,&rdquo; Google writes on its blog. Bolo ships with 50 stories in Hindi and 40 in English. Google says it found that 64% of children that interacted with Bolo showed an improvement in reading after three months of usage. &nbsp;&nbsp;Read more: Introducing &lsquo;Bolo&rsquo;: a new speech based reading-tutor app that helps children learn to read (Google India Blog).
#####################################################
490,000 fashion images&hellip; for science:&hellip;And advertising. Lots and lots of advertising, probably&hellip;Researchers with SenseTime Research and the Chinese University of Hong Kong have released DeepFashion2, a dataset containing around 490,000 images of 13 clothing categories from commercial shopping stores as well as consumers.
Detailed labeling: In DeepFashion2, &ldquo;each item in an image is labeled with scale, occlusion, zoom-in, viewpoint, category, style, bounding box, dense landmarks and per-pixel mask,&rdquo; the researchers write. &ldquo;To our knowledge, clothing pose estimation is presented for the first time in the literature by defining landmarks and poses of 13 categories that are more diverse and fruitful than human pose&rdquo;, the authors write. 
The second time is the charm: DeepFashion2 is a follow-up to DeepFashion, which was released in early 2017 (see: Import AI #33). DeepFashion2 has 3.5X as many annotations as DeepFashion.
Why this matters: It&rsquo;s likely that various industries will be altered by widely-deployed AI-based image analysis systems, and it seems probable that the fashion industry will take advantage of various image-analysis techniques to automatically analyze &amp; understand changing fashion trends in the world, in part by automatically analyzing the visual world and using these insights to alter the sorts of clothing being developed, or how it is marketed.&nbsp;&nbsp;Read more: DeepFashion2: A Versatile Benchmark for Detection, Post Estimation, Segmentation and Re-Identification of Clothing Images (Arxiv). &nbsp;&nbsp;Get the DeepFashion data here (GitHub).
#####################################################
Facebook tries to shine a LIGHT on language understanding:&hellip;Designs a MUD complete with netherworlds, underwater aquapolises, and more&hellip;LIGHT contains humans and AI agents within a text-based multi-player dungeon (MUD). This MUD consists of 663 locations, 3462 objects, and 1755 individual characters. It also ships with data, as Facebook has already collected a set of around 11,000 interactions between humans roleplaying characters in the game.
Graveyards, bazaars, and more: LIGHT contains a surprisingly diverse gameworld &ndash; not that the AI agents which play within it will care. Locations that AI agents and/or humans can visit include the countryside, forest, castles (inside and outside) as well as some more bizarre locations like a &ldquo;city in the clouds&rdquo; or a &ldquo;netherworld&rdquo; or even an &ldquo;underwater aquapolis&rdquo;.
Actions and emotions: Characters in LIGHT can carry out a range of physical actions (eat, drink, get, drop, etc) as well as express emotive actions (&rsquo;emotes&rsquo;) like to applaud, blush, wave, etc.
Results: To test out the environment, the researchers train some baseline models to predict actions, emotes, and dialogue. They find that a system based on Google&rsquo;s &lsquo;BERT&rsquo; language model (pre-trained on Reddit data) does best. They also perform some ablation studies which indicate that models that are successful in LIGHT use a lot of context, depending on numerous streams of data (dialogue, environment descriptions, and so on).
Why this matters: Language is likely fundamental to how we interact with increasingly powerful systems. I think figuring out how to work with such systems will require us to interact with them in increasingly sophisticated environments, so it&rsquo;ll be interesting to see how rapidly we can improve performance of agents in systems like LIGHT, and learn whether those improvements transfer over to other capabilities as well.&nbsp;&nbsp;Read more: Learning to Speak and Act in a Fantasy Text Adventure Game (Arxiv).
#####################################################
NeuroCuts replaces packet classification systems with learned behaviors:&hellip;Research means that in the future computers will learn to effectively communicate with each other&hellip;
In the future, the majority of the ways our computers talk to each other will be managed by customized, learned behaviors, derived by AI systems. That&rsquo;s the gist of a recent spate of research which has ranged from using AI approaches to try to learn how to perform computer tasks like creating and maintaining database indexes, or figuring out how to automatically search through large documents.
Now, researchers with the University of California at Berkeley and Johns Hopkins University have developed NeuroCuts, a system t…"

---

### Import AI 137: DeepMind uses (Google) StreetLearn to learn to navigate cities; NeuroCuts learns decent packet classification; plus a 490k labelled image dataset

The robots of the future will learn by playing, Google says:&hellip;Want to solve tasks effectively? Don&rsquo;t try to solve tasks during training!&hellip;Researchers with Google Brain have shown how to make robots smarter by showing them what it means to play without a goal in mind. Google does this by collecting a dataset via people tele-operating a robot in simulation. During these periods of teleoperation, the people are playing around, using the robot hand and arm to interact with the world around them without a specific goal in mind, so in one scene a person might pick up a random object, in another they might fiddle around with a door on a piece of furniture, and so on.
Google saves this data, calling it &lsquo;Learning from Play data&rsquo; (LfP). It fees this into a system that attempts to classify such playful sequences of actions, mapping them into a latent space. Meanwhile, another module in the system tries to look across the latent space and propose sequences of actions that could shift the robot from its current state to its goal state.
Multi-task training: Google evaluates this approach by comparing performance of robots trained with play data, from policies that use behavioural cloning to learn to complete tasks based on specific demonstration data. The tests show that robots which learn from play data are more robust to perturbations than ones trained without, and typically reach higher success rates on most tasks.&nbsp; Intriguingly, systems trained with play data display some over desirable traits: &ldquo;We find qualitative evidence that play-supervised models make multiple attempts to retry the task after initial failure&rdquo;, the researchers write. &ldquo;Surprisingly we find that its latent plan space learns to embed task semantics despite never being trained with task labels&rdquo;.
Why this matters: Gathering data for robotics work tends to be expensive, difficult, and prone to distribution problems (you can gather a lot of data, but you may subsequently discover that some quirk of the task or your robot platform means you need to go and re-gather a slightly different type of data). Being able to instead have robots learn behaviors primarily through cheaply-gathered non-goal-oriented play data will make it easier for people to experiment with developing such systems, and could make it easier to create large datasets shared between multiple parties. What might the &lsquo;ImageNet&rsquo; for play robotics look like, I wonder? &nbsp;&nbsp;Read more: Learning Latent Plans from Play (Arxiv).
#####################################################
Google teaches kids to read with AI-infused &lsquo;Bolo&rsquo;:&hellip;Tuition app ships with speech recognition and text-to-speech tech&hellip;Google has released Bolo, a mobile app for Android designed to help Indian children learn to read. Bolo ships with &lsquo;Diya&rsquo;, a software agent that can help children learn to read.
Bilingual: &ldquo;Diya can not only read out the text to your child, but also explain the meaning of English text in Hindi,&rdquo; Google writes on its blog. Bolo ships with 50 stories in Hindi and 40 in English. Google says it found that 64% of children that interacted with Bolo showed an improvement in reading after three months of usage. &nbsp;&nbsp;Read more: Introducing &lsquo;Bolo&rsquo;: a new speech based reading-tutor app that helps children learn to read (Google India Blog).
#####################################################
490,000 fashion images&hellip; for science:&hellip;And advertising. Lots and lots of advertising, probably&hellip;Researchers with SenseTime Research and the Chinese University of Hong Kong have released DeepFashion2, a dataset containing around 490,000 images of 13 clothing categories from commercial shopping stores as well as consumers.
Detailed labeling: In DeepFashion2, &ldquo;each item in an image is labeled with scale, occlusion, zoom-in, viewpoint, category, style, bounding box, dense landmarks and per-pixel mask,&rdquo; the researchers write. &ldquo;To our knowledge, clothing pose estimation is presented for the first time in the literature by defining landmarks and poses of 13 categories that are more diverse and fruitful than human pose&rdquo;, the authors write. 
The second time is the charm: DeepFashion2 is a follow-up to DeepFashion, which was released in early 2017 (see: Import AI #33). DeepFashion2 has 3.5X as many annotations as DeepFashion.
Why this matters: It&rsquo;s likely that various industries will be altered by widely-deployed AI-based image analysis systems, and it seems probable that the fashion industry will take advantage of various image-analysis techniques to automatically analyze &amp; understand changing fashion trends in the world, in part by automatically analyzing the visual world and using these insights to alter the sorts of clothing being developed, or how it is marketed.&nbsp;&nbsp;Read more: DeepFashion2: A Versatile Benchmark for Detection, Post Estimation, Segmentation and Re-Identification of Clothing Images (Arxiv). &nbsp;&nbsp;Get the DeepFashion data here (GitHub).
#####################################################
Facebook tries to shine a LIGHT on language understanding:&hellip;Designs a MUD complete with netherworlds, underwater aquapolises, and more&hellip;LIGHT contains humans and AI agents within a text-based multi-player dungeon (MUD). This MUD consists of 663 locations, 3462 objects, and 1755 individual characters. It also ships with data, as Facebook has already collected a set of around 11,000 interactions between humans roleplaying characters in the game.
Graveyards, bazaars, and more: LIGHT contains a surprisingly diverse gameworld &ndash; not that the AI agents which play within it will care. Locations that AI agents and/or humans can visit include the countryside, forest, castles (inside and outside) as well as some more bizarre locations like a &ldquo;city in the clouds&rdquo; or a &ldquo;netherworld&rdquo; or even an &ldquo;underwater aquapolis&rdquo;.
Actions and emotions: Characters in LIGHT can carry out a range of physical actions (eat, drink, get, drop, etc) as well as express emotive actions (&rsquo;emotes&rsquo;) like to applaud, blush, wave, etc.
Results: To test out the environment, the researchers train some baseline models to predict actions, emotes, and dialogue. They find that a system based on Google&rsquo;s &lsquo;BERT&rsquo; language model (pre-trained on Reddit data) does best. They also perform some ablation studies which indicate that models that are successful in LIGHT use a lot of context, depending on numerous streams of data (dialogue, environment descriptions, and so on).
Why this matters: Language is likely fundamental to how we interact with increasingly powerful systems. I think figuring out how to work with such systems will require us to interact with them in increasingly sophisticated environments, so it&rsquo;ll be interesting to see how rapidly we can improve performance of agents in systems like LIGHT, and learn whether those improvements transfer over to other capabilities as well.&nbsp;&nbsp;Read more: Learning to Speak and Act in a Fantasy Text Adventure Game (Arxiv).
#####################################################
NeuroCuts replaces packet classification systems with learned behaviors:&hellip;Research means that in the future computers will learn to effectively communicate with each other&hellip;
In the future, the majority of the ways our computers talk to each other will be managed by customized, learned behaviors, derived by AI systems. That&rsquo;s the gist of a recent spate of research which has ranged from using AI approaches to try to learn how to perform computer tasks like creating and maintaining database indexes, or figuring out how to automatically search through large documents.
Now, researchers with the University of California at Berkeley and Johns Hopkins University have developed NeuroCuts, a system t…