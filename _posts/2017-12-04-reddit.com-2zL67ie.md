---

layout: post
category: C7WHBLNLR
title: "[D] Attention models and Bidirectional RNNs Combating Vanishing Gradient • r/MachineLearning"
date: 2017-12-04 11:13:17
link: https://vrhk.co/2zL67ie
image: https://www.redditstatic.com/icon.png
domain: reddit.com
author: "reddit"
icon: https://www.reddit.com/favicon.ico
excerpt: "When thinking about it, adding Attention or using a Bidirectional RNN, will create shortcuts for the gradients to flow through. Much like residual..."

---

### [D] Attention models and Bidirectional RNNs Combating Vanishing Gradient • r/MachineLearning

When thinking about it, adding Attention or using a Bidirectional RNN, will create shortcuts for the gradients to flow through. Much like residual...