---

layout: post
category: product
title: "Import AI: #79: Diagnosing AI brains with PsychLab; training drones without drone-derived data; and a Davos AI report."
date: 2018-01-29 16:21:55
link: https://vrhk.co/2DXkh5a
image: 
domain: jack-clark.net
author: "Jack Clark"
icon: https://s0.wp.com/i/webclip.png
excerpt: "Making better video game coaches with deep learning:&hellip;The era of the deep learning-augmented video game coach is nigh!&hellip;What if video game coaches had access to the same sorts of telemetry as coaches for traditional sports like soccer or the NFL? That&rsquo;s the question that DeepLeague, software for analysing the coordinate position of a League of Legends player at a point in time, tries to answer. The software is able to look at a LoL minimap at a given point in time and use that to create a live view of where each specific player is. That sounds minor but it&rsquo;s currently not easy to do this in real-time via Riot Games&rsquo; API, so DL (aided by 100,000 annotated game minimap images) provides a somewhat generalizable workaround.&nbsp;&nbsp;What it means: The significant thing here is that deep learning is making it increasingly easy to point a set of neural machinery at a problem, like figuring out how to map coordinates and player avatars to less-detailed dots on a minimap. This is a new thing: the visual world has never before been this easy for computers to understand, and things that look like toys at first frequently wind up being significant. Take a read of the &lsquo;part 2&rsquo; post to get an idea of the technical details for this sort of project. And remember: this was coded up by a student over 5 days during a hurricane.&nbsp;&nbsp;Jargon, jargon everywhere: DeepLeague will let players &ldquo;analyze how the jungler paths, where he starts his route, when/where he ganks, when he backs, which lane he exerts the most pressure on, when/where mid roams&rdquo;. So there!&nbsp; Read more: DeepLeague: leveraging computer vision and deep learning on the League of Legends minimap + giving away a dataset of over 100,000 labeled images to further esports analytics research (Medium).&nbsp; Read even more: DeepLeague (Part 2): The Technical Details (Medium).&nbsp;&nbsp;Get the data (GitHub).
Rise of the drones: learning to operate and navigate without the right data with DroNet:&hellip;No drone data? No problem! Use car &amp; bicycle data instead and grab enough of it to generalize&hellip;One of the main ways deep learning differs to previous AI techniques lies in its generalizability: neural networks fed on one data type are frequently able to attain reasonable performance on slightly different and/or adjacent domains. We&rsquo;re already pretty familiar with this idea within object recognition &ndash; a network trained to recognize flowers should be able to draw bounding boxes around flowers and plants not in the training set, etc &ndash; but now we&rsquo;re starting to apply the same techniques to systems that take actions in the world, like cars and drones.&nbsp; Now, researchers with the University of Zurich and ETH Zurich and the Universidad Polit&eacute;cnica de Madrid in Spain have proposed Dronet: a way to train drones to drive on city streets using data derived entirely from self-driving cars and bicycles.&nbsp;&nbsp;How it works: The researchers use an 8-layer Residual Network to train a neural network policy to do two things: work out the correct steering angle to stay on the road, and learn to avoid collisions using a dataset gathered via bicycle. They train the model via mean-squared error (steering) and binary cross-entropy (collision). The result is a drone that is able to move around in urban settings and avoid collisions, though as the input data doesn&rsquo;t include information on the drone&rsquo;s vertical position, it operates in these experiments on a plane.&nbsp; Testing: They test it on a number of tasks in the real world which include traveling in a straight line, traveling along a curve and avoiding collisions in an urban area. They also evaluate its ability to transfer to new environments by testing it in a high altitude outdoor environment, a corridor, and a garage, where it roughly matches or beats other baselines. The overall performance of the system is pretty strong, which is surprising given its relative lack of sophistication compared to more innately powerful methods such as a control policy implemented within a 50-layer residual network. &ldquo;We can observe that our design, even though 80 times smaller than the best architecture, maintains a considerable prediction performance while achieving real-time operation (20 frames per second),&rdquo; they say.&nbsp;&nbsp;Datasets: The researchers get the driving dataset from Udacity&rsquo;s self-driving car project; it consists of 70,000 images of cars driving distributed over six distinct experiments. They take data from the front cameras and also the steering telemetry. For the collision dataset they had to collect their own and it&rsquo;s here that they get creative: they mount a GoPro on the handlebars of a bicycle and &ldquo;drive along different areas of a city, trying to diversify the types of obstacles (vehicles, pedestrians, vegetation, under construction sites) and the appearance of the environment. This way, the drone is able to generalize under different scenarios. We start recording when we are far away from an obstacle and stop when we are very close to it. In total, we collect around 32,000 images distributed over 137 sequences for a diverse set of obstacles. We manually annotate the sequences, so that frames far away from collision are labeled as 0 (no collision), and frames very close to the obstacle are labeled as 1 (collision)&rdquo;.&nbsp;&nbsp;Drone used: Parrot Bebop 2.0 drone which passes footage at 30Hz via wifi to a computer running the neural network..&ndash; Read more: DroNet: Learning to Fly by Driving (ETH Zurich).&ndash;&nbsp;Get the pre-trained DroNet weights here (ETH Zurich).&ndash;&nbsp;Get the Collision dataset here (ETH Zurich).&ndash;&nbsp;Access the project&rsquo;s GitHub repository here (GitHub).
UPS workers&rsquo; union seeks to ban drones, driverless vehicles:&hellip;In the absence of alternatives to traditional economic models, people circle the wagons to protect themselves from AI&hellip;People are terrified of AI because they worry for their livelihoods. That&rsquo;s because most politicians around the world are unable to suggest different economic models for an increasingly automated future. Meanwhile, many people are assuming that even if there&rsquo;s not gonna be mass unemployment as a consequence of AI, there&rsquo;s definitely going to be a continued degradation in wage bargaining power and the ability for people to exercise independent judgement in increasingly automated workplaces. As a consequence, workers&rsquo; unions are seeking to protect themselves. Case in point: the Teamsters labor union wants UPS to ban using drones or driverless vehicles for package deliveries so as to better protect their own jobs: this is locally rational, but globally irrational. If only society were better positioned to take advantage of such technologies without harming its own citizens.&ndash; Read more: Union heavyweight wants to ban UPS from using drones or driverless vehicles (CNBC).
Human-in-the-loop AI artists, with &lsquo;Deep Interactive Evolution&rsquo; (DeepIE):&hellip;Battle of the buzzwords as researchers combine generative adversarial networks (GANs) with interactive evolutionary computation (IEC)&hellip;The future of AI will involve humans augmenting themselves with increasingly smart, adaptive, reactive systems. One of the best ways to do this is with &lsquo;human-in-the-loop&rsquo; learning, where a human is able to directly influence the ongoing evolution of a given AI system. One of the first places this is likely to show up is in the art domain, as artists access increasingly creative systems to help enhance their own creative practices. So it&rsquo;s worth reading through this paper from researchers with New York University, the IT University of Copenhagen, and the Beijing University of Posts and Telecommunications, about how to smartly evolve novel images using humans, art, and AI.&nbsp; Their De…"

---

### Import AI: #79: Diagnosing AI brains with PsychLab; training drones without drone-derived data; and a Davos AI report.

Making better video game coaches with deep learning:&hellip;The era of the deep learning-augmented video game coach is nigh!&hellip;What if video game coaches had access to the same sorts of telemetry as coaches for traditional sports like soccer or the NFL? That&rsquo;s the question that DeepLeague, software for analysing the coordinate position of a League of Legends player at a point in time, tries to answer. The software is able to look at a LoL minimap at a given point in time and use that to create a live view of where each specific player is. That sounds minor but it&rsquo;s currently not easy to do this in real-time via Riot Games&rsquo; API, so DL (aided by 100,000 annotated game minimap images) provides a somewhat generalizable workaround.&nbsp;&nbsp;What it means: The significant thing here is that deep learning is making it increasingly easy to point a set of neural machinery at a problem, like figuring out how to map coordinates and player avatars to less-detailed dots on a minimap. This is a new thing: the visual world has never before been this easy for computers to understand, and things that look like toys at first frequently wind up being significant. Take a read of the &lsquo;part 2&rsquo; post to get an idea of the technical details for this sort of project. And remember: this was coded up by a student over 5 days during a hurricane.&nbsp;&nbsp;Jargon, jargon everywhere: DeepLeague will let players &ldquo;analyze how the jungler paths, where he starts his route, when/where he ganks, when he backs, which lane he exerts the most pressure on, when/where mid roams&rdquo;. So there!&nbsp; Read more: DeepLeague: leveraging computer vision and deep learning on the League of Legends minimap + giving away a dataset of over 100,000 labeled images to further esports analytics research (Medium).&nbsp; Read even more: DeepLeague (Part 2): The Technical Details (Medium).&nbsp;&nbsp;Get the data (GitHub).
Rise of the drones: learning to operate and navigate without the right data with DroNet:&hellip;No drone data? No problem! Use car &amp; bicycle data instead and grab enough of it to generalize&hellip;One of the main ways deep learning differs to previous AI techniques lies in its generalizability: neural networks fed on one data type are frequently able to attain reasonable performance on slightly different and/or adjacent domains. We&rsquo;re already pretty familiar with this idea within object recognition &ndash; a network trained to recognize flowers should be able to draw bounding boxes around flowers and plants not in the training set, etc &ndash; but now we&rsquo;re starting to apply the same techniques to systems that take actions in the world, like cars and drones.&nbsp; Now, researchers with the University of Zurich and ETH Zurich and the Universidad Polit&eacute;cnica de Madrid in Spain have proposed Dronet: a way to train drones to drive on city streets using data derived entirely from self-driving cars and bicycles.&nbsp;&nbsp;How it works: The researchers use an 8-layer Residual Network to train a neural network policy to do two things: work out the correct steering angle to stay on the road, and learn to avoid collisions using a dataset gathered via bicycle. They train the model via mean-squared error (steering) and binary cross-entropy (collision). The result is a drone that is able to move around in urban settings and avoid collisions, though as the input data doesn&rsquo;t include information on the drone&rsquo;s vertical position, it operates in these experiments on a plane.&nbsp; Testing: They test it on a number of tasks in the real world which include traveling in a straight line, traveling along a curve and avoiding collisions in an urban area. They also evaluate its ability to transfer to new environments by testing it in a high altitude outdoor environment, a corridor, and a garage, where it roughly matches or beats other baselines. The overall performance of the system is pretty strong, which is surprising given its relative lack of sophistication compared to more innately powerful methods such as a control policy implemented within a 50-layer residual network. &ldquo;We can observe that our design, even though 80 times smaller than the best architecture, maintains a considerable prediction performance while achieving real-time operation (20 frames per second),&rdquo; they say.&nbsp;&nbsp;Datasets: The researchers get the driving dataset from Udacity&rsquo;s self-driving car project; it consists of 70,000 images of cars driving distributed over six distinct experiments. They take data from the front cameras and also the steering telemetry. For the collision dataset they had to collect their own and it&rsquo;s here that they get creative: they mount a GoPro on the handlebars of a bicycle and &ldquo;drive along different areas of a city, trying to diversify the types of obstacles (vehicles, pedestrians, vegetation, under construction sites) and the appearance of the environment. This way, the drone is able to generalize under different scenarios. We start recording when we are far away from an obstacle and stop when we are very close to it. In total, we collect around 32,000 images distributed over 137 sequences for a diverse set of obstacles. We manually annotate the sequences, so that frames far away from collision are labeled as 0 (no collision), and frames very close to the obstacle are labeled as 1 (collision)&rdquo;.&nbsp;&nbsp;Drone used: Parrot Bebop 2.0 drone which passes footage at 30Hz via wifi to a computer running the neural network..&ndash; Read more: DroNet: Learning to Fly by Driving (ETH Zurich).&ndash;&nbsp;Get the pre-trained DroNet weights here (ETH Zurich).&ndash;&nbsp;Get the Collision dataset here (ETH Zurich).&ndash;&nbsp;Access the project&rsquo;s GitHub repository here (GitHub).
UPS workers&rsquo; union seeks to ban drones, driverless vehicles:&hellip;In the absence of alternatives to traditional economic models, people circle the wagons to protect themselves from AI&hellip;People are terrified of AI because they worry for their livelihoods. That&rsquo;s because most politicians around the world are unable to suggest different economic models for an increasingly automated future. Meanwhile, many people are assuming that even if there&rsquo;s not gonna be mass unemployment as a consequence of AI, there&rsquo;s definitely going to be a continued degradation in wage bargaining power and the ability for people to exercise independent judgement in increasingly automated workplaces. As a consequence, workers&rsquo; unions are seeking to protect themselves. Case in point: the Teamsters labor union wants UPS to ban using drones or driverless vehicles for package deliveries so as to better protect their own jobs: this is locally rational, but globally irrational. If only society were better positioned to take advantage of such technologies without harming its own citizens.&ndash; Read more: Union heavyweight wants to ban UPS from using drones or driverless vehicles (CNBC).
Human-in-the-loop AI artists, with &lsquo;Deep Interactive Evolution&rsquo; (DeepIE):&hellip;Battle of the buzzwords as researchers combine generative adversarial networks (GANs) with interactive evolutionary computation (IEC)&hellip;The future of AI will involve humans augmenting themselves with increasingly smart, adaptive, reactive systems. One of the best ways to do this is with &lsquo;human-in-the-loop&rsquo; learning, where a human is able to directly influence the ongoing evolution of a given AI system. One of the first places this is likely to show up is in the art domain, as artists access increasingly creative systems to help enhance their own creative practices. So it&rsquo;s worth reading through this paper from researchers with New York University, the IT University of Copenhagen, and the Beijing University of Posts and Telecommunications, about how to smartly evolve novel images using humans, art, and AI.&nbsp; Their De…