---

layout: post
category: product
title: "Import AI: #94: Google Duplex generates automation anxiety backlash, researchers show how easy it is to make a 3D-printed autonomous drone, Microsoft sells voice cloning services."
date: 2018-05-14 23:46:57
link: https://vrhk.co/2wJYtb6
image: 
domain: jack-clark.net
author: "Jack Clark"
icon: https://s2.wp.com/i/webclip.png
excerpt: "Microsoft sells &ldquo;custom voice&rdquo; speech synthesis:&hellip;The commercial voice cloning era arrives&hellip;Microsoft will soon sell &ldquo;Custom Voice&rdquo; a system to let businesses give their application a &ldquo;one-of-a-kind, recognizable brand voice, with no coding required&rdquo;. This product follows various research breakthroughs in the area of speech synthesis and speech cloning, like work from Baidu on voice cloning, and work from Google and DeepMind on speech synthesis.&nbsp; Why it matters: As the Google &lsquo;Duplex&rsquo; system shows, the era of capable, realistic-sounding natural language systems is arriving. It&rsquo;s going to be crucial to run as many experiments in society as possible to see how people react to automated systems in different domains. Being able to customize the voice of any given system to a particular context seems like a necessary ingredient for further acceptance of AI systems by the world.&nbsp; Read more: Custom Voice (Microsoft).
Teaching neural networks to perform low-light amplification:&hellip;Another case where data + learnable components beats hand-designed algorithms&hellip;Researchers with UIUC and Intel Labs have released a dataset for training image processing systems to take in images that are so dark as to be imperceptible to humans and to automatically process those images so that they&rsquo;re human-visible. The resulting system can be used to amplify low-light images by up to 300 times while displaying meaningful noise reduction and low levels of color transformation.&nbsp; Dataset: The researchers collect and publish the &lsquo;See-in-the-Dark&rsquo; (SID) dataset, which contains 5094 raw short exposure images, each with a corresponding long-exposure reference image. This dataset spans around 400 distinct scenes, as they also produce some bursts of short exposure images of the same scene.&nbsp; Technique: The researchers tested out their system using a multi-scale aggregation network and a U-net (both networks were selected for their ability to process full-resolution images at 4240&times;2832 or 6000&times;4000 in GPU memory). They trained networks by pairing the raw data of the short-exposure image with the corresponding long-exposure image(s). They applied random flipping and rotation for data augmentation, also.&nbsp; Results: They compared the results of their network with the output of BM3D, a non-naive denoising algorithm, and a burst denoising technique, and used Amazon&rsquo;s Mechanical Turk platform to poll people on which images they preferred. Users overwhelmingly preferred the images resulting from the technique described in the paper compared to BM3D, and in some cases preferred images generated by this technique to those created by the burst method.&nbsp; Why it matters: Techniques like this show how we can use neural networks to change how we solve problems from developing specific hand-tuned single-purpose algorithms, to instead learning to effectively mix and match various trainable components and data inputs to solve general problem classes. In the future it&rsquo;d be interesting if the researchers could further cut the time it takes the trained system to process each image as this would make a real-time view possible, potentially giving people another way to see in the dark.&nbsp; Read more: Learning-to-See-in-the-Dark (GitHub).&nbsp; Read more: Learning to See in the Dark (Arxiv).
Google researchers try to boost AI performance via in-graph computation:&hellip;As the AI world relies on more distributed, parallel execution, our need for new systems increases&hellip;Google researchers have outlined many of the steps they&rsquo;ve taken to improve components in the TensorFlow language to let them execute more aspects of a distributed AI job within the same computation graph. This increases the performance and efficiency of algorithms, and shows how AI&rsquo;s tendency towards mass distribution and parallelism is driving significant changes in how we program things (see also: Andrej Karpathy&rsquo;s &ldquo;Software 2.0&rdquo; thesis.)&nbsp; The main idea explored in the paper is how to distribute a modern machine learning job in such a way it can seamlessly run across CPUs, GPUs, TPUs, and other novel chip architectures. This is trickier than it sounds, since within a large-scale, contemporary job there are typically a multitude of components which need to interact with eachother, sometimes multiple times. This has caused Google to extend and refine various TensorFlow components to better support plotting all the computations within a model on the same computational graph, which lets it optimize the graph for underlying architectures. That differs to traditional approaches which usually involve specifying aspects of the execution in a separate block of code usually written in the control logic of the application (eg, invoking various AI modules written in TensorFlow within a big chunk of Python code, as opposed to executing everything within a big unified TF lump of code.&nbsp; Results: There&rsquo;s some preliminary evidence that this approach can have significant benefits. &ldquo;A baseline implementation of DQN without dynamic control flow requires conditional execution to be driven sequentially from the client program. The in-graph approach fuses all steps of the DQN algorithm into a single dataflow graph with dynamic control flow, which is invoked once per interaction with the reinforcement learning environment. Thus, this approach allows the entire computation to stay inside the system runtime, and enables parallel execution, including the overlapping of I/O with other work on a GPU. It yields a speedup of 21% over the baseline. Qualitatively, users report that the in-graph approach yields a more self-contained and deployable DQN implementation; the algorithm is encapsulated in the dataflow graph, rather than split between the dataflow graph and code in the host language,&rdquo; write the researchers.&nbsp; Read more: Dynamic Control Flow in Large-Scale Machine Learning (Arxiv).&nbsp; Read more: Software 2.0 (Andrej Karpathy).
Google tries to automate rote customer service with Duplex:&hellip;New service sees Google accidentally take people for a hike through the uncanny valley of AI&hellip;Google has revealed Duplex, an AI system that uses language modelling, speech recognition, and speech synthesis to automate tasks like booking appointments at hair salons, or reserving tables at restaurants. Duplex will let Google&rsquo;s own automated AI systems talk directly to humans at other businesses, letting the company automate human interactions and also more easily harvest data from the messy real world.&nbsp; How it works: &ldquo;The network uses the output of Google&rsquo;s automatic speech recognition (ASR) technology, as well as features from the audio, the history of the conversation, the parameters of the conversation (e.g. the desired service for an appointment, or the current time of day) and more. We trained our understanding model separately for each task, but leveraged the shared corpus across tasks,&rdquo; Google writes. Speech synthesis is achieved via both Tacotron and Wavenet (systems developed respectively by Google Brain and by DeepMind). It also uses human traits, like &ldquo;hmm&rdquo;s and &ldquo;uh&rdquo;s, to sound more natural to humans on the other end.&nbsp; Data harvesting: One use of the system is to help Google harvest more information from the world, for instance by autonomously calling up businesses and finding out their opening hours, then digitizing this information and making it available through Google.&nbsp; Accessibility: The system could be potentially useful for people with accessibility needs, like those with hearing impairments, and could potentially work in other languages, where you might ask Duplex to accomplish something and then it will use a local language to interface with a local business.&nbsp; The creepy uncan…"

---

### Import AI: #94: Google Duplex generates automation anxiety backlash, researchers show how easy it is to make a 3D-printed autonomous drone, Microsoft sells voice cloning services.

Microsoft sells &ldquo;custom voice&rdquo; speech synthesis:&hellip;The commercial voice cloning era arrives&hellip;Microsoft will soon sell &ldquo;Custom Voice&rdquo; a system to let businesses give their application a &ldquo;one-of-a-kind, recognizable brand voice, with no coding required&rdquo;. This product follows various research breakthroughs in the area of speech synthesis and speech cloning, like work from Baidu on voice cloning, and work from Google and DeepMind on speech synthesis.&nbsp; Why it matters: As the Google &lsquo;Duplex&rsquo; system shows, the era of capable, realistic-sounding natural language systems is arriving. It&rsquo;s going to be crucial to run as many experiments in society as possible to see how people react to automated systems in different domains. Being able to customize the voice of any given system to a particular context seems like a necessary ingredient for further acceptance of AI systems by the world.&nbsp; Read more: Custom Voice (Microsoft).
Teaching neural networks to perform low-light amplification:&hellip;Another case where data + learnable components beats hand-designed algorithms&hellip;Researchers with UIUC and Intel Labs have released a dataset for training image processing systems to take in images that are so dark as to be imperceptible to humans and to automatically process those images so that they&rsquo;re human-visible. The resulting system can be used to amplify low-light images by up to 300 times while displaying meaningful noise reduction and low levels of color transformation.&nbsp; Dataset: The researchers collect and publish the &lsquo;See-in-the-Dark&rsquo; (SID) dataset, which contains 5094 raw short exposure images, each with a corresponding long-exposure reference image. This dataset spans around 400 distinct scenes, as they also produce some bursts of short exposure images of the same scene.&nbsp; Technique: The researchers tested out their system using a multi-scale aggregation network and a U-net (both networks were selected for their ability to process full-resolution images at 4240&times;2832 or 6000&times;4000 in GPU memory). They trained networks by pairing the raw data of the short-exposure image with the corresponding long-exposure image(s). They applied random flipping and rotation for data augmentation, also.&nbsp; Results: They compared the results of their network with the output of BM3D, a non-naive denoising algorithm, and a burst denoising technique, and used Amazon&rsquo;s Mechanical Turk platform to poll people on which images they preferred. Users overwhelmingly preferred the images resulting from the technique described in the paper compared to BM3D, and in some cases preferred images generated by this technique to those created by the burst method.&nbsp; Why it matters: Techniques like this show how we can use neural networks to change how we solve problems from developing specific hand-tuned single-purpose algorithms, to instead learning to effectively mix and match various trainable components and data inputs to solve general problem classes. In the future it&rsquo;d be interesting if the researchers could further cut the time it takes the trained system to process each image as this would make a real-time view possible, potentially giving people another way to see in the dark.&nbsp; Read more: Learning-to-See-in-the-Dark (GitHub).&nbsp; Read more: Learning to See in the Dark (Arxiv).
Google researchers try to boost AI performance via in-graph computation:&hellip;As the AI world relies on more distributed, parallel execution, our need for new systems increases&hellip;Google researchers have outlined many of the steps they&rsquo;ve taken to improve components in the TensorFlow language to let them execute more aspects of a distributed AI job within the same computation graph. This increases the performance and efficiency of algorithms, and shows how AI&rsquo;s tendency towards mass distribution and parallelism is driving significant changes in how we program things (see also: Andrej Karpathy&rsquo;s &ldquo;Software 2.0&rdquo; thesis.)&nbsp; The main idea explored in the paper is how to distribute a modern machine learning job in such a way it can seamlessly run across CPUs, GPUs, TPUs, and other novel chip architectures. This is trickier than it sounds, since within a large-scale, contemporary job there are typically a multitude of components which need to interact with eachother, sometimes multiple times. This has caused Google to extend and refine various TensorFlow components to better support plotting all the computations within a model on the same computational graph, which lets it optimize the graph for underlying architectures. That differs to traditional approaches which usually involve specifying aspects of the execution in a separate block of code usually written in the control logic of the application (eg, invoking various AI modules written in TensorFlow within a big chunk of Python code, as opposed to executing everything within a big unified TF lump of code.&nbsp; Results: There&rsquo;s some preliminary evidence that this approach can have significant benefits. &ldquo;A baseline implementation of DQN without dynamic control flow requires conditional execution to be driven sequentially from the client program. The in-graph approach fuses all steps of the DQN algorithm into a single dataflow graph with dynamic control flow, which is invoked once per interaction with the reinforcement learning environment. Thus, this approach allows the entire computation to stay inside the system runtime, and enables parallel execution, including the overlapping of I/O with other work on a GPU. It yields a speedup of 21% over the baseline. Qualitatively, users report that the in-graph approach yields a more self-contained and deployable DQN implementation; the algorithm is encapsulated in the dataflow graph, rather than split between the dataflow graph and code in the host language,&rdquo; write the researchers.&nbsp; Read more: Dynamic Control Flow in Large-Scale Machine Learning (Arxiv).&nbsp; Read more: Software 2.0 (Andrej Karpathy).
Google tries to automate rote customer service with Duplex:&hellip;New service sees Google accidentally take people for a hike through the uncanny valley of AI&hellip;Google has revealed Duplex, an AI system that uses language modelling, speech recognition, and speech synthesis to automate tasks like booking appointments at hair salons, or reserving tables at restaurants. Duplex will let Google&rsquo;s own automated AI systems talk directly to humans at other businesses, letting the company automate human interactions and also more easily harvest data from the messy real world.&nbsp; How it works: &ldquo;The network uses the output of Google&rsquo;s automatic speech recognition (ASR) technology, as well as features from the audio, the history of the conversation, the parameters of the conversation (e.g. the desired service for an appointment, or the current time of day) and more. We trained our understanding model separately for each task, but leveraged the shared corpus across tasks,&rdquo; Google writes. Speech synthesis is achieved via both Tacotron and Wavenet (systems developed respectively by Google Brain and by DeepMind). It also uses human traits, like &ldquo;hmm&rdquo;s and &ldquo;uh&rdquo;s, to sound more natural to humans on the other end.&nbsp; Data harvesting: One use of the system is to help Google harvest more information from the world, for instance by autonomously calling up businesses and finding out their opening hours, then digitizing this information and making it available through Google.&nbsp; Accessibility: The system could be potentially useful for people with accessibility needs, like those with hearing impairments, and could potentially work in other languages, where you might ask Duplex to accomplish something and then it will use a local language to interface with a local business.&nbsp; The creepy uncan…