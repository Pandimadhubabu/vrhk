---

layout: post
category: threads
title: "[D] Preventing exploding gradients when using ReLU?"
date: 2018-04-05 02:18:09
link: https://vrhk.co/2uNlY22
image: https://www.redditstatic.com/icon.png
domain: reddit.com
author: "reddit"
icon: http://www.redditstatic.com/desktop2x/img/favicon/apple-icon-57x57.png
excerpt: "In something I'm currently working on, I've found that switching out my ReLU activations for sigmoids actually ends up letting my network perform..."

---

### [D] Preventing exploding gradients when using ReLU? â€¢ r/MachineLearning

In something I'm currently working on, I've found that switching out my ReLU activations for sigmoids actually ends up letting my network perform...