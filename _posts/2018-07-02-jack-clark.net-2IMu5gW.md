---

layout: post
category: product
title: "Import AI: #101: Teaching robots to grasp with two-stage networks; Silicon Valley VS Government AI; why procedural learning can generate natural curriculums."
date: 2018-07-02 18:42:09
link: https://vrhk.co/2IMu5gW
image: 
domain: jack-clark.net
author: "Jack Clark"
icon: https://s2.wp.com/i/webclip.png
excerpt: "Making better maps via AI:&hellip;Telenav pairs machine learning with OpenStreetCam data to let everyone make better maps&hellip;Navigation company Telenav has released datasets, machine learning software, and technical results to help people build AI services on top of mapping infrastructure. The company says it has done this to create a more open ecosystem around mapping, specifically around &lsquo;Open Street Map&rsquo;, a popular open source map).&nbsp; Release: The release includes a training set of ~50,000 images annotated with labels to help identify common road signs; a machine-learning technology stack that includes a notebook with visualizations, a RetinaNet system for detecting traffic signs, and the results from running these AI tools over more than 140-million existing street-level images; and more. &nbsp;&nbsp;Why it matters: Maps are fundamental to the modern world. AI promises to give us the tools needed to automatically label and analyze much of the world around us, holding with it the promise to create truly capable open source maps that can rival those developed by proprietary interests (see: Google Maps, HERE, etc). Mapping may also become better through the use of larger datasets to create better automatic-mapping systems, like tools that can parse the meaning of photos of road signs. &nbsp;&nbsp;Read more: The Future of Map-Making is Open and Powered by Sensors and AI (OpenStreetMap @ Telenav blog). &nbsp;&nbsp;Read more: Telenav MapAI Contest (Telenav). &nbsp;&nbsp;Check out the GitHub (Telenav GitHub).
Silicon Valley tries to draw a line in shifting sand: surveillance edition:&hellip;CEO of facial recognition startup says won&rsquo;t sell to law enforcement&hellip;Brian Brackeen, the CEO of facial recognition software developed Kairos, says his company is unwilling to sell facial recognition technologies to government or law enforcement. This follows Amazon coming under fire from the ACLU for selling facial recognition services to law enforcement via its &lsquo;Rekognition&rsquo; API. &nbsp;&nbsp;&ldquo;I (and my company) have come to belief that the use of commercial facial recognition in law enforcement or in government surveillance of any kind is wrong &ndash; and that it opens the door for gross misconduct by the morally corrupt,&rdquo; Brackeen writes. &ldquo;In the hands of government surveillance programs and law enforcement agencies, there&rsquo;s simply no way that face recognition software will not be used to harm citizens&rdquo;, he writes. &nbsp;&nbsp;Why it matters: The American government is currently reckoning with the outcome of an ideological preference leading to its military industrial infrastructure relying on an ever-shifting constellation of private compares, whereas other countries tend to perform more direct investment for certain key capabilities, like AI. That&rsquo;s led to today&rsquo;s situation where American government entities and organizes are, upon seeing how other governments (mainly China) are implementing AI, seeking to find ways to implement AI in America. But getting people to build these AI systems for the US government has proved difficult: many of the companies able to provide strategic AI services (see: Google, Amazon, Microsoft, etc) have become so large they&rsquo;ve become literal multinationals: their offices and markets are distributed around the world, and their staff come from anywhere. Therefore, these companies aren&rsquo;t super thrilled about working on behalf of any one specific government, and their staff are mounting internal protests to get the companies to not sell to the US government (among others).. How the American government deals with this will determine many of the contours of American AI policy in the coming years.&nbsp;&nbsp;Read more: Facial recognition software is not ready for use by law enforcement (TechCrunch).
&ldquo;Say it again, but like you&rsquo;re sad&rdquo;. Researchers create and release data for emotion synthesis:&hellip;Parallel universe terrifying future: a literal HR robot that can detect your &lsquo;tone&rsquo; during awkward discussions and chide you for it&hellip;You&rsquo;ve heard of speech recognition. Well, what about emotion recognition and emotional tweaking? That&rsquo;s the problem of listening to speech, categorizing the emotional inflections of the voices within it, and learning to change an existing speech sample to sound like it is spoken with a different emotion &nbsp;&ndash; a potentially useful technology to have for passive monitoring of audio feeds, as well as active impersonation or warping, or other purposes. But to be able to create a system capable of this we need to have access to the underlying data necessary to train it. That&rsquo;s why researchers with the University of Mons in Belgium and Northeastern University in the USA have created &lsquo;the Emotional Voices dataset&rsquo;. &nbsp;&nbsp;The dataset: &ldquo;This database&rsquo;s primary purpose it to build models that could not only produce emotional speech but also control the emotional dimension in speech,&rdquo; write the researchers. The dataset contains five different speakers and two spoken languages (north American English and Belgian French), with four of the five speakers contributing ~1,000 utterances each, and one speaker contributing around ~500. These utterances are split across five distinct emotions: neutral, amused, angry, sleepy, and disgust.&nbsp; You sound angry. Now you sound amused: In experiments, the researchers tested how well they could use this dataset to transform speech from the same speaker from one emotion to another. They found that people would roughly categorize voices transformed from neutral to angry in this way with roughly 70 to 80 percent accuracy &ndash; somewhat encouraging, but hardly definitive. In the future, the researchers &ldquo;hope that such systems will be efficient enough to learn not only the prosody representing the emotional voices but also the nonverbal expressions characterizing them which are also present in our database.&rdquo; &nbsp;&nbsp;Read more: The Emotional Voices Database: Towards Controlling the Emotion Dimension in Voice Generation Systems (Arxiv).
Giving robots a grasp of good tasks with two-stage networks:&hellip;End-to-end learning multii-stage tasks is getting easier, Stanford researchers show&hellip;Think about a typical DIY task you might do at home &ndash; what do you do? You probably grab the tool in one hand, then approach the object you need to fix or build, and go from there. But how do you know the best way to grip the object so you can accomplish the task? And why do you barely ever get this grasp wrong? This type of integrated reasoning and action is representative of the many ways in which humans are smarter than machines. Can we teach machines to do the same? Researchers with Stanford University have published new research showing how to train basic robots to perform simple, real-world DIY-style tasks, using deep learning techniques. &nbsp;&nbsp;Technique: The researchers use a simulator to repeatedly train a robot arm and a tool (in this case, a simplified toy hammer) to pick up the tool then use it to manipulate objects in a variety of situations. The approach relies on a &lsquo;Task-Oriented Grasping Network (TOG-Net), which is a two-stage system that first predicts effective grasps for the object, then predicts manipulation actions to perform to achieve a task.&nbsp; Data: One of the few nice things about working with robots is that if you have a simulator it&rsquo;s possible to automatically generate large amounts of data for training and evaluation. Here, the researchers use the open source physics simulator Bullet to generate many permutations of the scene to be learned, using different objects and behaviors. They train using 18,000 procedurally generated objects. &nbsp;&nbsp;Results: The system is tested in two limited domains: sweeping and hammering, where sweeping cons…"

---

### Import AI: #101: Teaching robots to grasp with two-stage networks; Silicon Valley VS Government AI; why procedural learning can generate natural curriculums.

Making better maps via AI:&hellip;Telenav pairs machine learning with OpenStreetCam data to let everyone make better maps&hellip;Navigation company Telenav has released datasets, machine learning software, and technical results to help people build AI services on top of mapping infrastructure. The company says it has done this to create a more open ecosystem around mapping, specifically around &lsquo;Open Street Map&rsquo;, a popular open source map).&nbsp; Release: The release includes a training set of ~50,000 images annotated with labels to help identify common road signs; a machine-learning technology stack that includes a notebook with visualizations, a RetinaNet system for detecting traffic signs, and the results from running these AI tools over more than 140-million existing street-level images; and more. &nbsp;&nbsp;Why it matters: Maps are fundamental to the modern world. AI promises to give us the tools needed to automatically label and analyze much of the world around us, holding with it the promise to create truly capable open source maps that can rival those developed by proprietary interests (see: Google Maps, HERE, etc). Mapping may also become better through the use of larger datasets to create better automatic-mapping systems, like tools that can parse the meaning of photos of road signs. &nbsp;&nbsp;Read more: The Future of Map-Making is Open and Powered by Sensors and AI (OpenStreetMap @ Telenav blog). &nbsp;&nbsp;Read more: Telenav MapAI Contest (Telenav). &nbsp;&nbsp;Check out the GitHub (Telenav GitHub).
Silicon Valley tries to draw a line in shifting sand: surveillance edition:&hellip;CEO of facial recognition startup says won&rsquo;t sell to law enforcement&hellip;Brian Brackeen, the CEO of facial recognition software developed Kairos, says his company is unwilling to sell facial recognition technologies to government or law enforcement. This follows Amazon coming under fire from the ACLU for selling facial recognition services to law enforcement via its &lsquo;Rekognition&rsquo; API. &nbsp;&nbsp;&ldquo;I (and my company) have come to belief that the use of commercial facial recognition in law enforcement or in government surveillance of any kind is wrong &ndash; and that it opens the door for gross misconduct by the morally corrupt,&rdquo; Brackeen writes. &ldquo;In the hands of government surveillance programs and law enforcement agencies, there&rsquo;s simply no way that face recognition software will not be used to harm citizens&rdquo;, he writes. &nbsp;&nbsp;Why it matters: The American government is currently reckoning with the outcome of an ideological preference leading to its military industrial infrastructure relying on an ever-shifting constellation of private compares, whereas other countries tend to perform more direct investment for certain key capabilities, like AI. That&rsquo;s led to today&rsquo;s situation where American government entities and organizes are, upon seeing how other governments (mainly China) are implementing AI, seeking to find ways to implement AI in America. But getting people to build these AI systems for the US government has proved difficult: many of the companies able to provide strategic AI services (see: Google, Amazon, Microsoft, etc) have become so large they&rsquo;ve become literal multinationals: their offices and markets are distributed around the world, and their staff come from anywhere. Therefore, these companies aren&rsquo;t super thrilled about working on behalf of any one specific government, and their staff are mounting internal protests to get the companies to not sell to the US government (among others).. How the American government deals with this will determine many of the contours of American AI policy in the coming years.&nbsp;&nbsp;Read more: Facial recognition software is not ready for use by law enforcement (TechCrunch).
&ldquo;Say it again, but like you&rsquo;re sad&rdquo;. Researchers create and release data for emotion synthesis:&hellip;Parallel universe terrifying future: a literal HR robot that can detect your &lsquo;tone&rsquo; during awkward discussions and chide you for it&hellip;You&rsquo;ve heard of speech recognition. Well, what about emotion recognition and emotional tweaking? That&rsquo;s the problem of listening to speech, categorizing the emotional inflections of the voices within it, and learning to change an existing speech sample to sound like it is spoken with a different emotion &nbsp;&ndash; a potentially useful technology to have for passive monitoring of audio feeds, as well as active impersonation or warping, or other purposes. But to be able to create a system capable of this we need to have access to the underlying data necessary to train it. That&rsquo;s why researchers with the University of Mons in Belgium and Northeastern University in the USA have created &lsquo;the Emotional Voices dataset&rsquo;. &nbsp;&nbsp;The dataset: &ldquo;This database&rsquo;s primary purpose it to build models that could not only produce emotional speech but also control the emotional dimension in speech,&rdquo; write the researchers. The dataset contains five different speakers and two spoken languages (north American English and Belgian French), with four of the five speakers contributing ~1,000 utterances each, and one speaker contributing around ~500. These utterances are split across five distinct emotions: neutral, amused, angry, sleepy, and disgust.&nbsp; You sound angry. Now you sound amused: In experiments, the researchers tested how well they could use this dataset to transform speech from the same speaker from one emotion to another. They found that people would roughly categorize voices transformed from neutral to angry in this way with roughly 70 to 80 percent accuracy &ndash; somewhat encouraging, but hardly definitive. In the future, the researchers &ldquo;hope that such systems will be efficient enough to learn not only the prosody representing the emotional voices but also the nonverbal expressions characterizing them which are also present in our database.&rdquo; &nbsp;&nbsp;Read more: The Emotional Voices Database: Towards Controlling the Emotion Dimension in Voice Generation Systems (Arxiv).
Giving robots a grasp of good tasks with two-stage networks:&hellip;End-to-end learning multii-stage tasks is getting easier, Stanford researchers show&hellip;Think about a typical DIY task you might do at home &ndash; what do you do? You probably grab the tool in one hand, then approach the object you need to fix or build, and go from there. But how do you know the best way to grip the object so you can accomplish the task? And why do you barely ever get this grasp wrong? This type of integrated reasoning and action is representative of the many ways in which humans are smarter than machines. Can we teach machines to do the same? Researchers with Stanford University have published new research showing how to train basic robots to perform simple, real-world DIY-style tasks, using deep learning techniques. &nbsp;&nbsp;Technique: The researchers use a simulator to repeatedly train a robot arm and a tool (in this case, a simplified toy hammer) to pick up the tool then use it to manipulate objects in a variety of situations. The approach relies on a &lsquo;Task-Oriented Grasping Network (TOG-Net), which is a two-stage system that first predicts effective grasps for the object, then predicts manipulation actions to perform to achieve a task.&nbsp; Data: One of the few nice things about working with robots is that if you have a simulator it&rsquo;s possible to automatically generate large amounts of data for training and evaluation. Here, the researchers use the open source physics simulator Bullet to generate many permutations of the scene to be learned, using different objects and behaviors. They train using 18,000 procedurally generated objects. &nbsp;&nbsp;Results: The system is tested in two limited domains: sweeping and hammering, where sweeping cons…