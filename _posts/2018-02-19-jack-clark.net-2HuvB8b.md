---

layout: post
category: product
title: "Import AI #82: 2.9 million anime images, reproducibility problems in AI research, and detecting dangerous URLs with deep learning."
date: 2018-02-19 23:22:13
link: https://vrhk.co/2HuvB8b
image: 
domain: jack-clark.net
author: "Jack Clark"
icon: https://s0.wp.com/i/webclip.png
excerpt: "Neural architecture search for the 99%:&hellip;Researchers figure out a way to make NAS techniques work on a single GPU, rather than several hundred&hellip;One of the more striking recent trends in AI has been the emergence of neural architecture search techniques, which is where you automate the design of &nbsp;AI systems, like image classifiers. The drawbacks to these approaches have so far mostly been that they&rsquo;re expensive, using hundreds of GPUs at a time, and therefore are infeasible for most researchers. That started to change last year with the publication of SMASH (covered in Import AI #56), a technique to do neural architecture search on a significant compute budget but with slight trade-offs in accuracy and in flexibility. Now, researchers with Google, CMU, and Stanford University, have pushed the idea of low-cost NAS techniques forward, via a new technique, &lsquo;Efficient Neural Architecture Search&rsquo;, or ENAS, that can design state-of-the-art systems using less than a day&rsquo;s computation on a single NVIDIA 1080 GPU. This represents a 1000X reduction in computational cost for the technique, and leads to a system that can create architectures that are almost as good as those trained on the larger systems.&nbsp;&nbsp;How it works: Instead of training each new model from scratch, ENAS gets the models to share weights with one another. It does this by re-casting the problem of neural architecture search as finding a specific task-specific sub-graph within one large directed acyclic graph (DAG). This approach works for designing both recurrent and convolutional networks: ENAS-designed networks obtain close-to-state-of-the-art results on Penn Treebank (Perplexity: 55.8), and on image classification for CIFAR-10 (Error: 2.89%.) &nbsp;&nbsp;Why it matters: For the past few years lots of very intelligent people have been busy turning food and sleep into brainpower which they&rsquo;ve used to get very good at hand-designing neural network architectures. Approaches like NAS promise to let us automate the design of specific architectures, freeing up researchers to spend more time on fundamental tasks like deriving new building blocks that NAS systems can learn to build compositions out of, or other techniques to further increase the efficiency of architecture design. Broadly, approaches like NAS means we can simply offload a huge chunk of work from (hyper-efficient, relatively costly, somewhat rare) human brains to (somewhat inefficient, extremely cheap, plentiful) computer brains. That seems like a worthwhile trade.&nbsp; Read more: Efficient Neural Architecture Search via Parameter Sharing (Arxiv).&nbsp; Read more: SMASH: One-Shot Model Architecture Search through HyperNetworks (Arxiv).
The anime-network rises, with 2.9 million images and 77.5 million tags:&hellip;It sure aint ImageNet, but it&rsquo;s certain very large&hellip;Some enterprising people have created a large-scale dataset of images taken from anime pictures. The &lsquo;Danbooru&rsquo; dataset &ldquo;is larger than ImageNet as a whole and larger than the current largest multi-description dataset, MS COCO,&rdquo; they write. Each image has a bunch of metadata associated with it including things like its popularity on the image web board (a &lsquo;booru&rsquo;) it has been taken from.&nbsp; Problematic structures ahead:&nbsp;The corpus &ldquo;does focus heavily on female anime characters&rdquo;, though the researchers note &ldquo;they are placed in a wide variety of circumstances with numerous surrounding tagged objects or actions, and the sheer size implies that many more miscellaneous images will be included&rdquo;. Images in the dataset are classified according to &ldquo;safe&rdquo;, &ldquo;questionable&rdquo;, and &ldquo;explicit&rdquo;, with the rough distribution at launch consisting of 76.3% &lsquo;safe&rsquo; images, 14.9% as &lsquo;questionable&rsquo;, and &lsquo;8.7% as &lsquo;explicit&rsquo;. There are a number of ethical questions the compilation and release of this dataset seems to raise, and my main concern at outset is that such a large corpus of explicit imagery will almost invariably lead to various grubby AI experiments that further alienate people from the AI community. I hope I&rsquo;m proved wrong! &nbsp;&nbsp;Example uses: The researchers imagine the dataset could be used for a bunch of tasks, ranging from classification, to image generation, to predicting traits about images from available metadata, and so on. &nbsp;&nbsp;Justification: A further justification for the dataset is that drawn images will encourage people to develop models with higher levels of abstraction than those which can simply map combinations of textures (as in the case of ImageNet), and so on. &ldquo;Illustrations are frequently black-and-white rather than color, line art rather than photographs, and even color illustrations tend to rely far less on textures and far more on lines (with textures omitted or filled in with standard repetitive patterns), working on a higher level of abstraction &ndash; a leopard would not be as trivially recognized by pattern-matching on yellow and black dots &ndash; with irrelevant details that a discriminator might cheaply classify based on typically suppressed in favor of global gestalt, and often heavily stylized,&rdquo; they write. &ldquo;Because illustrations are produced by an entirely different process and focus only on salient details while abstracting the rest, they offer a way to test external validity and the extent to which taggers are tapping into higher-level semantic perception.&rdquo; &nbsp;&nbsp;Read more: Danbooru2017: A large-scale crowdsourced and tagged anime illustration dataset (Gwern.)
Stanford researchers regale reproducibility horrors encountered during the design of DAWNBench:&hellip;Lies, damned lies, and deep learning&hellip;Stanford researchers have discussed some of the difficulties they encountered when developing DAWNBench, a benchmark that assess deep learning methods in a holistic way using a set of different metrics, like inference latency and cost, along with training time and training cost. Their conclusions should be familiar to most deep learning practitioners: deep learning performance is poorly understood, widely shared intuitions are likely based on imperfect information, and we still lack the theoretical guarantees to understand how one research breakthrough might interact with another when combined.&nbsp; Why it matters: Deep learning is still very much in a phase of &rsquo;empirical experimentation&rsquo; and the arrival of benchmarks like DAWNBench, as well as prior work like the paper Deep Reinforcement Learning that Matters (whose conclusion was that random seeds determine a huge amount of the end performance of RL), will help surface problems and force the community to develop more rigorous methods. &nbsp;&nbsp;Read more: Deep Learning Pitfalls Encountered while Developing DAWNBench. &nbsp;&nbsp;Read more: Deep Reinforcement Learning that Matters (Arxiv).
Detecting dangerous URLs with deep learning:&hellip;Character-level &amp; word-level combination leads to better performance on malicious URL categorization&hellip;Researchers with Singapore Management University have published details on URLNet, a system for using neural network approaches to automatically classify URLs as being risky or safe to click on.&nbsp;&nbsp;Why it matters: &nbsp;&ldquo;Without using any expert or hand-designed features, URLNet methods offer a significant jump in [performance] over baselines,&rdquo; they write. By now this should be a familiar trend, but it&rsquo;s worth repeating: given a sufficiently large dataset, neural network-based techniques tend to provide superior performance to hand-crafted features. (Caveat: In many domains getting the data is difficult, and these models all need to be refreshed to account for an ever-changing world.) &nbsp;&nbsp;How it works: URLNet uses convolutional neural networks to clas…"

---

### Import AI #82: 2.9 million anime images, reproducibility problems in AI research, and detecting dangerous URLs with deep learning.

Neural architecture search for the 99%:&hellip;Researchers figure out a way to make NAS techniques work on a single GPU, rather than several hundred&hellip;One of the more striking recent trends in AI has been the emergence of neural architecture search techniques, which is where you automate the design of &nbsp;AI systems, like image classifiers. The drawbacks to these approaches have so far mostly been that they&rsquo;re expensive, using hundreds of GPUs at a time, and therefore are infeasible for most researchers. That started to change last year with the publication of SMASH (covered in Import AI #56), a technique to do neural architecture search on a significant compute budget but with slight trade-offs in accuracy and in flexibility. Now, researchers with Google, CMU, and Stanford University, have pushed the idea of low-cost NAS techniques forward, via a new technique, &lsquo;Efficient Neural Architecture Search&rsquo;, or ENAS, that can design state-of-the-art systems using less than a day&rsquo;s computation on a single NVIDIA 1080 GPU. This represents a 1000X reduction in computational cost for the technique, and leads to a system that can create architectures that are almost as good as those trained on the larger systems.&nbsp;&nbsp;How it works: Instead of training each new model from scratch, ENAS gets the models to share weights with one another. It does this by re-casting the problem of neural architecture search as finding a specific task-specific sub-graph within one large directed acyclic graph (DAG). This approach works for designing both recurrent and convolutional networks: ENAS-designed networks obtain close-to-state-of-the-art results on Penn Treebank (Perplexity: 55.8), and on image classification for CIFAR-10 (Error: 2.89%.) &nbsp;&nbsp;Why it matters: For the past few years lots of very intelligent people have been busy turning food and sleep into brainpower which they&rsquo;ve used to get very good at hand-designing neural network architectures. Approaches like NAS promise to let us automate the design of specific architectures, freeing up researchers to spend more time on fundamental tasks like deriving new building blocks that NAS systems can learn to build compositions out of, or other techniques to further increase the efficiency of architecture design. Broadly, approaches like NAS means we can simply offload a huge chunk of work from (hyper-efficient, relatively costly, somewhat rare) human brains to (somewhat inefficient, extremely cheap, plentiful) computer brains. That seems like a worthwhile trade.&nbsp; Read more: Efficient Neural Architecture Search via Parameter Sharing (Arxiv).&nbsp; Read more: SMASH: One-Shot Model Architecture Search through HyperNetworks (Arxiv).
The anime-network rises, with 2.9 million images and 77.5 million tags:&hellip;It sure aint ImageNet, but it&rsquo;s certain very large&hellip;Some enterprising people have created a large-scale dataset of images taken from anime pictures. The &lsquo;Danbooru&rsquo; dataset &ldquo;is larger than ImageNet as a whole and larger than the current largest multi-description dataset, MS COCO,&rdquo; they write. Each image has a bunch of metadata associated with it including things like its popularity on the image web board (a &lsquo;booru&rsquo;) it has been taken from.&nbsp; Problematic structures ahead:&nbsp;The corpus &ldquo;does focus heavily on female anime characters&rdquo;, though the researchers note &ldquo;they are placed in a wide variety of circumstances with numerous surrounding tagged objects or actions, and the sheer size implies that many more miscellaneous images will be included&rdquo;. Images in the dataset are classified according to &ldquo;safe&rdquo;, &ldquo;questionable&rdquo;, and &ldquo;explicit&rdquo;, with the rough distribution at launch consisting of 76.3% &lsquo;safe&rsquo; images, 14.9% as &lsquo;questionable&rsquo;, and &lsquo;8.7% as &lsquo;explicit&rsquo;. There are a number of ethical questions the compilation and release of this dataset seems to raise, and my main concern at outset is that such a large corpus of explicit imagery will almost invariably lead to various grubby AI experiments that further alienate people from the AI community. I hope I&rsquo;m proved wrong! &nbsp;&nbsp;Example uses: The researchers imagine the dataset could be used for a bunch of tasks, ranging from classification, to image generation, to predicting traits about images from available metadata, and so on. &nbsp;&nbsp;Justification: A further justification for the dataset is that drawn images will encourage people to develop models with higher levels of abstraction than those which can simply map combinations of textures (as in the case of ImageNet), and so on. &ldquo;Illustrations are frequently black-and-white rather than color, line art rather than photographs, and even color illustrations tend to rely far less on textures and far more on lines (with textures omitted or filled in with standard repetitive patterns), working on a higher level of abstraction &ndash; a leopard would not be as trivially recognized by pattern-matching on yellow and black dots &ndash; with irrelevant details that a discriminator might cheaply classify based on typically suppressed in favor of global gestalt, and often heavily stylized,&rdquo; they write. &ldquo;Because illustrations are produced by an entirely different process and focus only on salient details while abstracting the rest, they offer a way to test external validity and the extent to which taggers are tapping into higher-level semantic perception.&rdquo; &nbsp;&nbsp;Read more: Danbooru2017: A large-scale crowdsourced and tagged anime illustration dataset (Gwern.)
Stanford researchers regale reproducibility horrors encountered during the design of DAWNBench:&hellip;Lies, damned lies, and deep learning&hellip;Stanford researchers have discussed some of the difficulties they encountered when developing DAWNBench, a benchmark that assess deep learning methods in a holistic way using a set of different metrics, like inference latency and cost, along with training time and training cost. Their conclusions should be familiar to most deep learning practitioners: deep learning performance is poorly understood, widely shared intuitions are likely based on imperfect information, and we still lack the theoretical guarantees to understand how one research breakthrough might interact with another when combined.&nbsp; Why it matters: Deep learning is still very much in a phase of &rsquo;empirical experimentation&rsquo; and the arrival of benchmarks like DAWNBench, as well as prior work like the paper Deep Reinforcement Learning that Matters (whose conclusion was that random seeds determine a huge amount of the end performance of RL), will help surface problems and force the community to develop more rigorous methods. &nbsp;&nbsp;Read more: Deep Learning Pitfalls Encountered while Developing DAWNBench. &nbsp;&nbsp;Read more: Deep Reinforcement Learning that Matters (Arxiv).
Detecting dangerous URLs with deep learning:&hellip;Character-level &amp; word-level combination leads to better performance on malicious URL categorization&hellip;Researchers with Singapore Management University have published details on URLNet, a system for using neural network approaches to automatically classify URLs as being risky or safe to click on.&nbsp;&nbsp;Why it matters: &nbsp;&ldquo;Without using any expert or hand-designed features, URLNet methods offer a significant jump in [performance] over baselines,&rdquo; they write. By now this should be a familiar trend, but it&rsquo;s worth repeating: given a sufficiently large dataset, neural network-based techniques tend to provide superior performance to hand-crafted features. (Caveat: In many domains getting the data is difficult, and these models all need to be refreshed to account for an ever-changing world.) &nbsp;&nbsp;How it works: URLNet uses convolutional neural networks to clas…