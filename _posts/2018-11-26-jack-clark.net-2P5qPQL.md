---

layout: post
category: product
title: "Import AI: 122: Google obtains new ImageNet state-of-the-art with GPipe; drone learns to land more effectively than PD controller policy; and Facebook releases its ‘CherryPi’ StarCraft bot"
date: 2018-11-26 22:31:56
link: https://vrhk.co/2P5qPQL
image: 
domain: jack-clark.net
author: "Jack Clark"
icon: https://s2.wp.com/i/webclip.png
excerpt: "Google obtains new ImageNet state-of-the-art accuracy with mammoth networks trained via &lsquo;GPipe&rsquo; infrastructure:&hellip;If you want to industrialize AI, you need to build infrastructure like GPipe&hellip;Parameter growth = Performance Growth: The researchers note that the winner of the 2014 ImageNet competition had 4 million parameters in its model, while the winner of the 2017 challenge had 145.8 million parameters &ndash; a 36X increase in three years. GPipe, by comparison, can support models of up to almost 2-billion parameters across 8 accelerators.&nbsp; Pipeline parallelism via GPipe: GPipe is a distributed ML library that uses synchronous mini-batch gradient descent for training. It is designed to spread workloads across heterogeneous hardware systems (multiple types of chips) and comes with a bunch of inbuilt features which let it efficiently scale up model training, with the researchers reporting a (very rare) near-linear speedup: &ldquo;with 4 times more accelerators we can achieve a 3.5 times speedup for training giant neural networks [with GPipe]&rdquo; they write.&nbsp; Results: To test out how effective GPipe is the researchers trained ResNet and AmoebaNet (previous ImageNet SOTA) networks on it, running the experiments on TPU-V2s, each of which has 8 accelerator cores and an aggregate memory of 64GB. Using this technique they were able to train a new ImageNet system with a state-of-the-art Top-1 Accuracy of 84.3% (up from 82.7 percent), and a Top-5 Accuracy of 97 percent.&nbsp; Why it matters: &ldquo;Our work validates the hypothesis that bigger models and more computation would lead to higher model quality,&rdquo; write the researchers. This trend of research bifurcating into large-compute and small-compute domains has significant ramifications for the ability for smaller entities (for instance, startups) to effectively compete with organizations with access to large computational infrastructure (eg, Google). A more troubling effect with long-term negative consequences is that at these compute scales it is almost impossible for academia to do research at the same scale as corporate research entities. I continue to worry that this will lead to a splitting of the AI research community and potentially the creation of the sort of factionalism and &lsquo;us vs them&rsquo; attitude seen elsewhere in contemporary life.
Companies will seek to ameliorate this inequality of compute by releasing the artifacts of compute (eg, pre-trained models). Though this will go some way to empowering researchers it will fail to deal with the underlying problems which are systemic and likely require a policy solution (aka, more money for academia, and so on).&nbsp; &nbsp; Read more: GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism (Arxiv).
Neural net beats tuned PD controller at tricky drone landing task:&hellip;The future of drones: many neural modules&hellip;A recent trend in AI research has been work showing that deep learning-based techniques can outperform hand-crafted rule-based systems in domains as varied as image recognition, speech recognition, and even the design of neural network architectures. Now, researchers with CalTech, Northeastern University, and the University of California at Irvine, have shown that it is possible to use neural networks to learn how to land quadcopters with a greater accuracy than a PD (proportional derivative) controller.&nbsp; Neural Lander: The researchers call their system the &lsquo;Neural Lander&rsquo; and say it is designed &ldquo;to improve the precision of quadrotor landing with guaranteed stability. Our approach directly learns the ground effect on coupled unsteady aerodynamics and vehicular dynamics&hellip;We evaluate Neural-Lander for trajectory tracking of quadrotor during take-off, landing and near ground maneuvers. Neural-Lander is able to land a quadrotor much more accurately than a naive PD controller with a pre-identified system.&rdquo;&nbsp; Testing: The researchers evaluate their approach on a real world system and show that &ldquo;compared to the PD controller, Neural-Lander can decrease error in z direction from 0.13m to zero, and mitigate average x and y drifts by 90% and 34% respectively, in 1D landing. Meanwhile, NeuralLander can decrease z error from 0.12m to zero, in 3D landing. We also empirically show that the DNN generalizes well to new test inputs outside the training domain.&rdquo;Why it matters: Systems like this show not only the broad utility of AI systems for diverse tasks, but also highlight how researchers are beginning to think about meshing these learnable modules into products. It&rsquo;s likely of interest that one of the sponsors of this research was defense contractor Raytheon (though as with the vast majority of academic research it&rsquo;s almost certain Raytheon did not have any particular role or input into this research, but rather has decided to broadly fund research into drone autonomy &ndash; nonetheless, this indicates the direction where major defense contractors think the future lies).&nbsp; Read more: Neural Lander: Stable Drone Landing Control using Learned Dynamics (Arxiv).Watch videos of the Neural Lander in action (YouTube).
AI Research Group MIRI plans future insights to be &ldquo;nondisclosed-by-default&rdquo;:&hellip;.Research organization says recent progress, desire for ability to concentrate, and worries that its safety research will be increasingly related to capabilities research, means it should go private&hellip;Nate Soares, the executive director of AI research group MIRI, says the organization &ldquo;recently decided to make most of its research &lsquo;nondisclosed-by-default&rsquo;, by which we mean that going forward, most results discovered within MIRI will remain internal-only unless there is an explicit decision to release those results&rdquo;.&nbsp; MIRI is doing this because it thinks it can increase the pace of its research if it focuses on making research progress &ldquo;rather than on exposition, and if we aren&rsquo;t feeling pressure to justify our intuitions to wide audiences, and that it is worried that some of its new research paths could have &ldquo;capabilities insights&rdquo; which thereby speed the arrival of (in its view, unsafe-by-default) AGI. It also sees some merit to deliberate isolation, based on an observation that &ldquo;historically, early-stage scientific work has often been done by people who were solitary or geographically isolated&rdquo;.&nbsp; Why going quiet could be dangerous: MIRI acknowledges some of the potential risks of this approach, noting that it may make it more difficult for it to hire and evaluate researchers; makes it harder to get useful feedback on its ideas from other people around the world; increases the difficulty of it obtaining funding; and leading to various &ldquo;social costs and logistical overhead&rdquo; from keeping research private.
&ldquo;Many of us are somewhat alarmed by the speed of recent machine learning progress&rdquo;, Soares writes. That&rsquo;s combined with the fact MIRI believes it is highly likely people will successfully develop artificial general intelligence at some point with or without safety. &ldquo;Humanity doesn&rsquo;t need coherent versions of [AI safety/alignment] concepts to hill-climb its way to AGI,&rdquo; Soares writes. &ldquo;Evolution hill-climbed that distance, and evolution had no model of what it was doing&rdquo;.&nbsp; Money: MIRI benefited from the cryptocurrency boom in 2017, receiving millions of dollars in donations from people who had made money on the spike in Ethereum. It has subsequently gained further funding, so &ndash; having surpassed many of its initial fundraising goals &ndash; is able to plan for the long term.&nbsp; Secrecy is not so crazy: Many AI researchers are privately contemplating when and if certain bits of research should be taken private. This is driven by a combination of near-term concer…"

---

### Import AI: 122: Google obtains new ImageNet state-of-the-art with GPipe; drone learns to land more effectively than PD controller policy; and Facebook releases its ‘CherryPi’ StarCraft bot

Google obtains new ImageNet state-of-the-art accuracy with mammoth networks trained via &lsquo;GPipe&rsquo; infrastructure:&hellip;If you want to industrialize AI, you need to build infrastructure like GPipe&hellip;Parameter growth = Performance Growth: The researchers note that the winner of the 2014 ImageNet competition had 4 million parameters in its model, while the winner of the 2017 challenge had 145.8 million parameters &ndash; a 36X increase in three years. GPipe, by comparison, can support models of up to almost 2-billion parameters across 8 accelerators.&nbsp; Pipeline parallelism via GPipe: GPipe is a distributed ML library that uses synchronous mini-batch gradient descent for training. It is designed to spread workloads across heterogeneous hardware systems (multiple types of chips) and comes with a bunch of inbuilt features which let it efficiently scale up model training, with the researchers reporting a (very rare) near-linear speedup: &ldquo;with 4 times more accelerators we can achieve a 3.5 times speedup for training giant neural networks [with GPipe]&rdquo; they write.&nbsp; Results: To test out how effective GPipe is the researchers trained ResNet and AmoebaNet (previous ImageNet SOTA) networks on it, running the experiments on TPU-V2s, each of which has 8 accelerator cores and an aggregate memory of 64GB. Using this technique they were able to train a new ImageNet system with a state-of-the-art Top-1 Accuracy of 84.3% (up from 82.7 percent), and a Top-5 Accuracy of 97 percent.&nbsp; Why it matters: &ldquo;Our work validates the hypothesis that bigger models and more computation would lead to higher model quality,&rdquo; write the researchers. This trend of research bifurcating into large-compute and small-compute domains has significant ramifications for the ability for smaller entities (for instance, startups) to effectively compete with organizations with access to large computational infrastructure (eg, Google). A more troubling effect with long-term negative consequences is that at these compute scales it is almost impossible for academia to do research at the same scale as corporate research entities. I continue to worry that this will lead to a splitting of the AI research community and potentially the creation of the sort of factionalism and &lsquo;us vs them&rsquo; attitude seen elsewhere in contemporary life.
Companies will seek to ameliorate this inequality of compute by releasing the artifacts of compute (eg, pre-trained models). Though this will go some way to empowering researchers it will fail to deal with the underlying problems which are systemic and likely require a policy solution (aka, more money for academia, and so on).&nbsp; &nbsp; Read more: GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism (Arxiv).
Neural net beats tuned PD controller at tricky drone landing task:&hellip;The future of drones: many neural modules&hellip;A recent trend in AI research has been work showing that deep learning-based techniques can outperform hand-crafted rule-based systems in domains as varied as image recognition, speech recognition, and even the design of neural network architectures. Now, researchers with CalTech, Northeastern University, and the University of California at Irvine, have shown that it is possible to use neural networks to learn how to land quadcopters with a greater accuracy than a PD (proportional derivative) controller.&nbsp; Neural Lander: The researchers call their system the &lsquo;Neural Lander&rsquo; and say it is designed &ldquo;to improve the precision of quadrotor landing with guaranteed stability. Our approach directly learns the ground effect on coupled unsteady aerodynamics and vehicular dynamics&hellip;We evaluate Neural-Lander for trajectory tracking of quadrotor during take-off, landing and near ground maneuvers. Neural-Lander is able to land a quadrotor much more accurately than a naive PD controller with a pre-identified system.&rdquo;&nbsp; Testing: The researchers evaluate their approach on a real world system and show that &ldquo;compared to the PD controller, Neural-Lander can decrease error in z direction from 0.13m to zero, and mitigate average x and y drifts by 90% and 34% respectively, in 1D landing. Meanwhile, NeuralLander can decrease z error from 0.12m to zero, in 3D landing. We also empirically show that the DNN generalizes well to new test inputs outside the training domain.&rdquo;Why it matters: Systems like this show not only the broad utility of AI systems for diverse tasks, but also highlight how researchers are beginning to think about meshing these learnable modules into products. It&rsquo;s likely of interest that one of the sponsors of this research was defense contractor Raytheon (though as with the vast majority of academic research it&rsquo;s almost certain Raytheon did not have any particular role or input into this research, but rather has decided to broadly fund research into drone autonomy &ndash; nonetheless, this indicates the direction where major defense contractors think the future lies).&nbsp; Read more: Neural Lander: Stable Drone Landing Control using Learned Dynamics (Arxiv).Watch videos of the Neural Lander in action (YouTube).
AI Research Group MIRI plans future insights to be &ldquo;nondisclosed-by-default&rdquo;:&hellip;.Research organization says recent progress, desire for ability to concentrate, and worries that its safety research will be increasingly related to capabilities research, means it should go private&hellip;Nate Soares, the executive director of AI research group MIRI, says the organization &ldquo;recently decided to make most of its research &lsquo;nondisclosed-by-default&rsquo;, by which we mean that going forward, most results discovered within MIRI will remain internal-only unless there is an explicit decision to release those results&rdquo;.&nbsp; MIRI is doing this because it thinks it can increase the pace of its research if it focuses on making research progress &ldquo;rather than on exposition, and if we aren&rsquo;t feeling pressure to justify our intuitions to wide audiences, and that it is worried that some of its new research paths could have &ldquo;capabilities insights&rdquo; which thereby speed the arrival of (in its view, unsafe-by-default) AGI. It also sees some merit to deliberate isolation, based on an observation that &ldquo;historically, early-stage scientific work has often been done by people who were solitary or geographically isolated&rdquo;.&nbsp; Why going quiet could be dangerous: MIRI acknowledges some of the potential risks of this approach, noting that it may make it more difficult for it to hire and evaluate researchers; makes it harder to get useful feedback on its ideas from other people around the world; increases the difficulty of it obtaining funding; and leading to various &ldquo;social costs and logistical overhead&rdquo; from keeping research private.
&ldquo;Many of us are somewhat alarmed by the speed of recent machine learning progress&rdquo;, Soares writes. That&rsquo;s combined with the fact MIRI believes it is highly likely people will successfully develop artificial general intelligence at some point with or without safety. &ldquo;Humanity doesn&rsquo;t need coherent versions of [AI safety/alignment] concepts to hill-climb its way to AGI,&rdquo; Soares writes. &ldquo;Evolution hill-climbed that distance, and evolution had no model of what it was doing&rdquo;.&nbsp; Money: MIRI benefited from the cryptocurrency boom in 2017, receiving millions of dollars in donations from people who had made money on the spike in Ethereum. It has subsequently gained further funding, so &ndash; having surpassed many of its initial fundraising goals &ndash; is able to plan for the long term.&nbsp; Secrecy is not so crazy: Many AI researchers are privately contemplating when and if certain bits of research should be taken private. This is driven by a combination of near-term concer…