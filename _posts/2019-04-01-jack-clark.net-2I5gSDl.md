---

layout: post
category: product
title: "Import AI 140: Surveilling a city via the ‘CityFlow’ dataset; 25,000 images of Chinese shop signs; and the seven traps of AI ethics"
date: 2019-04-01 17:21:47
link: https://vrhk.co/2I5gSDl
image: 
domain: jack-clark.net
author: "Jack Clark"
icon: https://s2.wp.com/i/webclip.png
excerpt: "NVIDIA&rsquo;s &lsquo;CityFlow&rsquo; dataset shows how to do citywide-surveillance:&hellip;CityFlow promises more efficient, safer transit systems&hellip; as well as far better surveillance systems&hellip;Researchers with NVIDIA, San Jose State University, and the University of Washington, have released CityFlow, a dataset to help researchers develop algorithms for surveilling and tracking multiple cars as they travel around a city.
The CityFlow Dataset contains of 3.25 hours of video collected from 40 cameras distributed across 10 intersections in a US city. &ldquo;The dataset covers a diverse set of location types, including intersections, stretches of roadways, and highways&rdquo;. CityFlow contains over 229,680 bounding boxes across 666 vehicles, which include cars, buses, pickup trucks, vans, SUVs, and so on. Each video has a resolution of at least 960pixels, and &ldquo;the majority&rdquo; have a frame rate of 10 FPS. &nbsp;&nbsp;Sub-dataset: CityFlow ReID: The researchers have created a subset of the data for the purpose of re-identifying pedestrians and vehicles as they disappear from the view of one camera and re-appear in another. This subset of the data includes 56,277 bounding boxes,
Baselines: CityFlow ships with a set of baselines for the following tasks: 
Pedestrian re-identification.
Vehicle re-identification.
Single-camera tracking of a distinct object. 
Multi-camera tracking of a given object.
Why this matters &ndash; surveillance, everywhere: It would be nice to see some discussion within the paper about the wide-ranging surveillance applications and implications of this technology. Yes, it&rsquo;ll clearly be used to improve the efficiency (and safety!) of urban transit systems, but it will also be plugged into local police services and national and international intelligence-gathering systems. This has numerous ramifications and I would be excited to see more researchers take the time to discuss these aspects of their work. &nbsp;&nbsp;Read more: CityFlow: A City-Scale Benchmark for Multi-Target Multi-Camera Vehicle Tracking and Re-Identification (Arxiv). 
#####################################################
SkelNetOn challenges researchers to extract skeletons from images, point clouds, and parametric representations:&hellip;New dataset and competition track could make it easier for AI systems to extract more fundamental (somewhat low-fidelity) representations of the objects in the world they want to interact with&hellip;A group of researchers from multiple institutions have announced the &lsquo;SkelNetOn&rsquo; dataset and challenge, which seeks to &ldquo;utilize existing and develop novel deep learning architectures for shape understanding&rdquo;. The challenge involves the geometric modelling of objects, which is a useful problem to work on as techniques that can solve it naturally generate &ldquo;a compact and intuitive representation of the shape for modeling, synthesis, compression, and analysis&rdquo;.
Three challenges in three domains: Each SkelNetOn challenge ships with its own dataset of 1,725 paired images/point clouds/parametric representations of objects and skeletons.
Why this matters: Datasets contribute to broader progress in AI research, and being able to smartly infer 2D and 3D skeletons from images will unlock applications, ranging from Kinect-style interfaces that rely on the computer knowing where the user is, to being able to cheaply generate (basic) skeletal models for use in media production, for example video games.&nbsp; The authors &ldquo;believe that SkelNetOn has the potential to become a fundamental benchmark for the intersection of deep learning and geometry understanding&hellip; ultimately, we envision that such deep learning approaches can be used to extract expressive parameters and hierarchical representations that can be utilized for generative models and for proceduralization&rdquo;. &nbsp;&nbsp;Read more: SkelNetOn 2019 Datast and Challenge on Deep Learning for Geometric Shape Understanding (Arxiv). 
#####################################################
Want over 25,000 images of Chinese shop signs? Come get &rsquo;em:&hellip;ShopSign dataset took more than two years to collect, and includes five hard categories of sign&hellip;Chinese researchers have created ShopSign, a dataset of images of shop signs. Chinese shop signs tend to be set against a variety of backgrounds with varying lengths, materials used, and styles, the researchers note; this compares to signs in places like the USA, Italy, and France, which tend to be more standardized, they explain. This dataset will help people train automatic captioning systems that work against (some) Chinese signs, and could lead to secondary applications, like using generative models to create synthetic Chinese shop signs.
Key statistics: 
25,362: Chinese shop sign images within the dataset.
4,000: Images taken at night. 
2,516: Pairs of images where signs have been photographed from both an angle and a front-facing perspective.
50: Different types of camera used to collect the dataset, leading to natural variety within images.
2.4 years: Time it took to collect the dataset.
&gt;10: Locations of images, including Shanghai, Beijing, inner Mongolia, Xinjiang, Heilongjiang, Liaoning, Fujian, Shangqiu, Zhoukou, as well as several urban areas in Henan Province.
5: &ldquo;special categories&rdquo;; these are &lsquo;hard images&rsquo; which are signs against wood, deformed, exposed, mirrored, or obscure backdrops.
196,010 &ndash; Lines of text in the dataset.
626,280 &ndash; Chinese characters in the dataset.
Why this matters: The creation of open datasets of images not predominantly written in English will help to make AI more diverse, making it easier for researchers from other parts of the world to build tools and conduct research in contexts relevant to them. I can&rsquo;t wait to see ShopSigns for every language, covering the signs of the world (and then I hope someone trains a Style/Cycle/Big-GAN on them to generate synthetic street sign art!). &nbsp;&nbsp;Get the data: The authors promise to share the dataset on their GitHub repository. As of Sunday&nbsp;March 31st&nbsp;the images are yet to be uploaded their. Check out GitHub here.&nbsp; &nbsp; &nbsp;Read more: ShopSign: a Diverse Scene Text Dataset of Chinese Shop Signs in Street Views (Arxiv).
#####################################################
Stanford (briefly) sets state-of-the-art for GLUE language modelling challenge:&hellip;Invest in researching new training signals, not architectures, say researchers&hellip;Stanford University researchers recently set a new state-of-the-art on a multi-task natural language benchmark called GLUE, obtaining a score of 83.2 on GLUE on 20th of March, compared to 83.1 for the prior high score and 87.1 for human baseline performance.
Nine tasks, one benchmark: GLUE consists of nine natural language understanding tasks and was introduced in early 2018. Last year, systems from OpenAI (GPT) and Google (BERT) led the GLUE leaderboard; the Stanford system uses BERT in conjunction with additional supervision signals (supervised learning, transfer learning, multi-task learning, weak supervision, and ensembling) in a &lsquo;Massive Multi-Task Learning (MMTL) setting&rsquo;. The resulting model obtains state-of-the-art scores on four of GLUE&rsquo;s nine tasks, and sets the new overall state-of-the-art.
RTE: The researchers detail how they improved performance on RTE (Recognizing Textual Entailment), one of GLUE&rsquo;s nine tasks. The goal of RTE is to figure out if a sentence is implied by the preceding one, for example: in the following example, the second sentence is related to the first: &ldquo;The cat sat on the mat. The dog liked to sit on the mat, so it barked at the cat.&rdquo;
Boosting performance with five supervisory signals:
1 signal: Supervised Learning [SL]: Score: 58.9
Train a standard biLSTM on the &lsquo;RTE&rsquo; dataset, using ELMo embeddings an…"

---

### Import AI 140: Surveilling a city via the ‘CityFlow’ dataset; 25,000 images of Chinese shop signs; and the seven traps of AI ethics

NVIDIA&rsquo;s &lsquo;CityFlow&rsquo; dataset shows how to do citywide-surveillance:&hellip;CityFlow promises more efficient, safer transit systems&hellip; as well as far better surveillance systems&hellip;Researchers with NVIDIA, San Jose State University, and the University of Washington, have released CityFlow, a dataset to help researchers develop algorithms for surveilling and tracking multiple cars as they travel around a city.
The CityFlow Dataset contains of 3.25 hours of video collected from 40 cameras distributed across 10 intersections in a US city. &ldquo;The dataset covers a diverse set of location types, including intersections, stretches of roadways, and highways&rdquo;. CityFlow contains over 229,680 bounding boxes across 666 vehicles, which include cars, buses, pickup trucks, vans, SUVs, and so on. Each video has a resolution of at least 960pixels, and &ldquo;the majority&rdquo; have a frame rate of 10 FPS. &nbsp;&nbsp;Sub-dataset: CityFlow ReID: The researchers have created a subset of the data for the purpose of re-identifying pedestrians and vehicles as they disappear from the view of one camera and re-appear in another. This subset of the data includes 56,277 bounding boxes,
Baselines: CityFlow ships with a set of baselines for the following tasks: 
Pedestrian re-identification.
Vehicle re-identification.
Single-camera tracking of a distinct object. 
Multi-camera tracking of a given object.
Why this matters &ndash; surveillance, everywhere: It would be nice to see some discussion within the paper about the wide-ranging surveillance applications and implications of this technology. Yes, it&rsquo;ll clearly be used to improve the efficiency (and safety!) of urban transit systems, but it will also be plugged into local police services and national and international intelligence-gathering systems. This has numerous ramifications and I would be excited to see more researchers take the time to discuss these aspects of their work. &nbsp;&nbsp;Read more: CityFlow: A City-Scale Benchmark for Multi-Target Multi-Camera Vehicle Tracking and Re-Identification (Arxiv). 
#####################################################
SkelNetOn challenges researchers to extract skeletons from images, point clouds, and parametric representations:&hellip;New dataset and competition track could make it easier for AI systems to extract more fundamental (somewhat low-fidelity) representations of the objects in the world they want to interact with&hellip;A group of researchers from multiple institutions have announced the &lsquo;SkelNetOn&rsquo; dataset and challenge, which seeks to &ldquo;utilize existing and develop novel deep learning architectures for shape understanding&rdquo;. The challenge involves the geometric modelling of objects, which is a useful problem to work on as techniques that can solve it naturally generate &ldquo;a compact and intuitive representation of the shape for modeling, synthesis, compression, and analysis&rdquo;.
Three challenges in three domains: Each SkelNetOn challenge ships with its own dataset of 1,725 paired images/point clouds/parametric representations of objects and skeletons.
Why this matters: Datasets contribute to broader progress in AI research, and being able to smartly infer 2D and 3D skeletons from images will unlock applications, ranging from Kinect-style interfaces that rely on the computer knowing where the user is, to being able to cheaply generate (basic) skeletal models for use in media production, for example video games.&nbsp; The authors &ldquo;believe that SkelNetOn has the potential to become a fundamental benchmark for the intersection of deep learning and geometry understanding&hellip; ultimately, we envision that such deep learning approaches can be used to extract expressive parameters and hierarchical representations that can be utilized for generative models and for proceduralization&rdquo;. &nbsp;&nbsp;Read more: SkelNetOn 2019 Datast and Challenge on Deep Learning for Geometric Shape Understanding (Arxiv). 
#####################################################
Want over 25,000 images of Chinese shop signs? Come get &rsquo;em:&hellip;ShopSign dataset took more than two years to collect, and includes five hard categories of sign&hellip;Chinese researchers have created ShopSign, a dataset of images of shop signs. Chinese shop signs tend to be set against a variety of backgrounds with varying lengths, materials used, and styles, the researchers note; this compares to signs in places like the USA, Italy, and France, which tend to be more standardized, they explain. This dataset will help people train automatic captioning systems that work against (some) Chinese signs, and could lead to secondary applications, like using generative models to create synthetic Chinese shop signs.
Key statistics: 
25,362: Chinese shop sign images within the dataset.
4,000: Images taken at night. 
2,516: Pairs of images where signs have been photographed from both an angle and a front-facing perspective.
50: Different types of camera used to collect the dataset, leading to natural variety within images.
2.4 years: Time it took to collect the dataset.
&gt;10: Locations of images, including Shanghai, Beijing, inner Mongolia, Xinjiang, Heilongjiang, Liaoning, Fujian, Shangqiu, Zhoukou, as well as several urban areas in Henan Province.
5: &ldquo;special categories&rdquo;; these are &lsquo;hard images&rsquo; which are signs against wood, deformed, exposed, mirrored, or obscure backdrops.
196,010 &ndash; Lines of text in the dataset.
626,280 &ndash; Chinese characters in the dataset.
Why this matters: The creation of open datasets of images not predominantly written in English will help to make AI more diverse, making it easier for researchers from other parts of the world to build tools and conduct research in contexts relevant to them. I can&rsquo;t wait to see ShopSigns for every language, covering the signs of the world (and then I hope someone trains a Style/Cycle/Big-GAN on them to generate synthetic street sign art!). &nbsp;&nbsp;Get the data: The authors promise to share the dataset on their GitHub repository. As of Sunday&nbsp;March 31st&nbsp;the images are yet to be uploaded their. Check out GitHub here.&nbsp; &nbsp; &nbsp;Read more: ShopSign: a Diverse Scene Text Dataset of Chinese Shop Signs in Street Views (Arxiv).
#####################################################
Stanford (briefly) sets state-of-the-art for GLUE language modelling challenge:&hellip;Invest in researching new training signals, not architectures, say researchers&hellip;Stanford University researchers recently set a new state-of-the-art on a multi-task natural language benchmark called GLUE, obtaining a score of 83.2 on GLUE on 20th of March, compared to 83.1 for the prior high score and 87.1 for human baseline performance.
Nine tasks, one benchmark: GLUE consists of nine natural language understanding tasks and was introduced in early 2018. Last year, systems from OpenAI (GPT) and Google (BERT) led the GLUE leaderboard; the Stanford system uses BERT in conjunction with additional supervision signals (supervised learning, transfer learning, multi-task learning, weak supervision, and ensembling) in a &lsquo;Massive Multi-Task Learning (MMTL) setting&rsquo;. The resulting model obtains state-of-the-art scores on four of GLUE&rsquo;s nine tasks, and sets the new overall state-of-the-art.
RTE: The researchers detail how they improved performance on RTE (Recognizing Textual Entailment), one of GLUE&rsquo;s nine tasks. The goal of RTE is to figure out if a sentence is implied by the preceding one, for example: in the following example, the second sentence is related to the first: &ldquo;The cat sat on the mat. The dog liked to sit on the mat, so it barked at the cat.&rdquo;
Boosting performance with five supervisory signals:
1 signal: Supervised Learning [SL]: Score: 58.9
Train a standard biLSTM on the &lsquo;RTE&rsquo; dataset, using ELMo embeddings an…