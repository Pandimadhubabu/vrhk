---

layout: post
category: product
title: "Import AI 149: China’s AI principles call for international collaboration; what it takes to fit a neural net onto a microcontroller; and solving Sudoko with a hybrid AI system"
date: 2019-06-03 20:31:24
link: https://vrhk.co/2Wrskla
image: 
domain: jack-clark.net
author: "Jack Clark"
icon: https://s2.wp.com/i/webclip.png
excerpt: "China publishes its own set of AI principles &ndash; and they emphasize international collaboration:&hellip;Principles for education, impacts of AI, cooperation, and AGI&hellip;A coalition of influential Chinese groups have published a set of ethical standards for AI research, called the Beijing AI Principles. These principles are meant to govern how developers research AI, how they use it, and how society should manage AI. The principles heavily emphasize international cooperation at a time of rising tension between nations over the strategic implications of rapidly advancing digital technologies. 
The principles were revealed last week by a coalition that included the Beijing Academy of Artificial Intelligence (BAAI), Tsinghua University, and a league of companies including Baidu, Alibaba, and Tencent. &ldquo;The Beijing Principles reflect our position, vision and our willingness to create a dialogue with the international society,&rdquo; said the director of BAAI, Zeng Yi, according to Xinhua. &ldquo;Only through coordination on a global scale can we build AI that is beneficial to both humanity and nature&rdquo;.
Highlights of the Beijing AI principles: Some of the notable principles include establishing open systems &ldquo;to avoid data/platform monopolies&rdquo;, that people should receive education and training &ldquo;to help them adapt to the impact of AI development in psychological, emotional and technical aspects&rdquo;, and that people should approach the technology with an emphasis on long-term planning, including anticipating the need for research focused on &ldquo;the potential risks of Augmented Intelligence, Artificial General Intelligence (AGI) and Superintelligence should be encouraged&rdquo;. 
Why this matters: Principles are ones of the ways that large policy institutions develop norms to govern technology, so Beijing&rsquo;s AI principles should be seen as a prism via which the Chinese government will seek to regulate aspects of AI. These principles will sit alongside multi-national principles like those developed by the OECD, as well as those developed by individual entities (eg: Google, OpenAI). The United States government is yet to outline the principles with which it will approach the development and deployment of AI technology, though it has participated in and supported the creation of the OECD AI principles.&nbsp; Read more: Beijing AI Principles (Official Site).&nbsp; Read more: Beijing publishes AI ethical standards, calls for int&rsquo;l cooperation (Xinhua).
#####################################################
Faster, smaller, cheaper, better! Google trains SOTA-exceeding &lsquo;EfficientNets&rsquo;:&hellip;What&rsquo;s better than scaling up by width? Depth? Resolution? How about all three in harmony?&hellip;Google has developed a way to scale up neural networks more efficiently and has used this technique to find a new family of neural network models called EfficientNets. EfficientNets outperform existing state-of-the-art image recognition systems, while being up to ten times as efficient (in terms of memory footprint).
How EfficientNets work: Compound Scaling: Typically, when scaling up a neural network, people fool around with things like width (how wide are the layers in the network), depth (how many layers are stacked on top of eachother), and resolution (what resolution are inputs being processed it). For this project, Google performed a large-scale study of the ways in which it could scale networks and discovered an effective approach it calls &lsquo;compound scaling&rsquo;, based on the idea that &ldquo;in order to pursue better accuracy and efficiency, it is critical to balance all dimensions of network width, depth, and resolution during ConvNet scaling&rdquo;. EfficientNets are trained using a compound scaling method that scales width, depth, and resolution in an optimal way. 
Results: Faster, cheaper, lighter, better! Google shows that it can train existing networks (eg, ResNet, MobileNet) with good performance properties by scaling them up using its compound training technique. The company also develops new EfficientNet models on the ImageNet dataset &ndash; widely considered to be a gold-standard for evaluating new systems &ndash; setting a new state-of-the-art score on image identification (both top-1 and top-5) accuracy, while achieving this with around 10X fewer parameters than other systems. &nbsp;
Why this matters: As part of the industrialization of AI, we&rsquo;re seeing organizations dump resources into learning how to train large-scale networks more efficiently, while preserving the performance of resource-hungry ones. To me, this is analogous to going from the expensive prototype phase of production of an invention, to the beginnings of mass production.&nbsp; Read more: EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks (Arxiv).&nbsp; Read more: EfficientNet: Improving Accuracy and Efficiency through AutoML and Model Scaling (Google AI Blog).
#####################################################
Pairing deep learning systems with symbolic systems, for SAT solving:&hellip;Using neural nets for logical reasoning gets a bit easier&hellip;Researchers with Carnegie Mellon University and the University of Southern California have paired deep learning systems with symbolic AI by creating MAXSAT, a differentiable satisfiability solver that can be knitted into larger deep learning systems. This means it is now easier to integrate logical structures into systems that use deep learning components. 
Sudoko results: The SATNet model does well against a basic ConvNet model, as well as a model fed with a binary mask which indicates which bits need to be learned. SATNet outperforms these systems, scoring 98.3% on an original sudoko set when given the numeric inputs. More impressively, it obtains a score of 63.2% on &lsquo;visual sudoko&rsquo; (traditional convnet: 0%), which is where they replace the digits with handwritten MNIST digits and feed it in. Specifically, they use a convnet to parse the figures in the Sudoko image, then pass this
Why this matters: Hybrid AI systems which fuse the general utility-class capabilities of deep learning components with more specific systems seems like a way to bridge traditional and symbolic AI, and making such systems be easy to add into larger systems. &ldquo;Our hope is that by wrapping a powerful yet generic primitive such as MAXSAT solving within a differentiable framework, our solver can enable &ldquo;implicit&rdquo; logical reasoning to occur where needed within larger frameworks, even if the precise structure of the domain is unknown and must be learned from data&rdquo;.&nbsp;&nbsp;Read more: SATNet: Bridging deep learning and logical reasoning using a differentiable satisfiability solver (Arxiv).
#####################################################
Squeezing neural nets onto microcontrollers via neural architecture search:&hellip;Get ready for billions of things to gain deep learning-based sense&amp;respond capacity&hellip;Researchers with ARM ML Research and Princeton University want to make it easier for people to deploy advanced artificial intelligence capabilities onto microcontrollers (MCUs) &ndash; something that has been difficult to do so far because today&rsquo;s neural networks techniques are too computationally expensive and memory-intensive to be easily deployed onto MCUs. 
MCUs and why they matter: Microcontrollers are the sorts of ultra-tiny lumps of computation embedded in things like fridges, microwaves, very small drones, small cameras, and other electronic widgets. To put this in perspective, in the developed world a typical person will have around four distinct desktop-class chips (eg, their phone, a laptop, etc), while having somewhere on the order of three dozen MCUs; a typical mid-range car might pack as many as 30 MCUs inside itself. 
MCUs shipped in 2019 (projection): 50 billionGPUs shipped in 2018: 100 million 
…"

---

### Import AI 149: China’s AI principles call for international collaboration; what it takes to fit a neural net onto a microcontroller; and solving Sudoko with a hybrid AI system

China publishes its own set of AI principles &ndash; and they emphasize international collaboration:&hellip;Principles for education, impacts of AI, cooperation, and AGI&hellip;A coalition of influential Chinese groups have published a set of ethical standards for AI research, called the Beijing AI Principles. These principles are meant to govern how developers research AI, how they use it, and how society should manage AI. The principles heavily emphasize international cooperation at a time of rising tension between nations over the strategic implications of rapidly advancing digital technologies. 
The principles were revealed last week by a coalition that included the Beijing Academy of Artificial Intelligence (BAAI), Tsinghua University, and a league of companies including Baidu, Alibaba, and Tencent. &ldquo;The Beijing Principles reflect our position, vision and our willingness to create a dialogue with the international society,&rdquo; said the director of BAAI, Zeng Yi, according to Xinhua. &ldquo;Only through coordination on a global scale can we build AI that is beneficial to both humanity and nature&rdquo;.
Highlights of the Beijing AI principles: Some of the notable principles include establishing open systems &ldquo;to avoid data/platform monopolies&rdquo;, that people should receive education and training &ldquo;to help them adapt to the impact of AI development in psychological, emotional and technical aspects&rdquo;, and that people should approach the technology with an emphasis on long-term planning, including anticipating the need for research focused on &ldquo;the potential risks of Augmented Intelligence, Artificial General Intelligence (AGI) and Superintelligence should be encouraged&rdquo;. 
Why this matters: Principles are ones of the ways that large policy institutions develop norms to govern technology, so Beijing&rsquo;s AI principles should be seen as a prism via which the Chinese government will seek to regulate aspects of AI. These principles will sit alongside multi-national principles like those developed by the OECD, as well as those developed by individual entities (eg: Google, OpenAI). The United States government is yet to outline the principles with which it will approach the development and deployment of AI technology, though it has participated in and supported the creation of the OECD AI principles.&nbsp; Read more: Beijing AI Principles (Official Site).&nbsp; Read more: Beijing publishes AI ethical standards, calls for int&rsquo;l cooperation (Xinhua).
#####################################################
Faster, smaller, cheaper, better! Google trains SOTA-exceeding &lsquo;EfficientNets&rsquo;:&hellip;What&rsquo;s better than scaling up by width? Depth? Resolution? How about all three in harmony?&hellip;Google has developed a way to scale up neural networks more efficiently and has used this technique to find a new family of neural network models called EfficientNets. EfficientNets outperform existing state-of-the-art image recognition systems, while being up to ten times as efficient (in terms of memory footprint).
How EfficientNets work: Compound Scaling: Typically, when scaling up a neural network, people fool around with things like width (how wide are the layers in the network), depth (how many layers are stacked on top of eachother), and resolution (what resolution are inputs being processed it). For this project, Google performed a large-scale study of the ways in which it could scale networks and discovered an effective approach it calls &lsquo;compound scaling&rsquo;, based on the idea that &ldquo;in order to pursue better accuracy and efficiency, it is critical to balance all dimensions of network width, depth, and resolution during ConvNet scaling&rdquo;. EfficientNets are trained using a compound scaling method that scales width, depth, and resolution in an optimal way. 
Results: Faster, cheaper, lighter, better! Google shows that it can train existing networks (eg, ResNet, MobileNet) with good performance properties by scaling them up using its compound training technique. The company also develops new EfficientNet models on the ImageNet dataset &ndash; widely considered to be a gold-standard for evaluating new systems &ndash; setting a new state-of-the-art score on image identification (both top-1 and top-5) accuracy, while achieving this with around 10X fewer parameters than other systems. &nbsp;
Why this matters: As part of the industrialization of AI, we&rsquo;re seeing organizations dump resources into learning how to train large-scale networks more efficiently, while preserving the performance of resource-hungry ones. To me, this is analogous to going from the expensive prototype phase of production of an invention, to the beginnings of mass production.&nbsp; Read more: EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks (Arxiv).&nbsp; Read more: EfficientNet: Improving Accuracy and Efficiency through AutoML and Model Scaling (Google AI Blog).
#####################################################
Pairing deep learning systems with symbolic systems, for SAT solving:&hellip;Using neural nets for logical reasoning gets a bit easier&hellip;Researchers with Carnegie Mellon University and the University of Southern California have paired deep learning systems with symbolic AI by creating MAXSAT, a differentiable satisfiability solver that can be knitted into larger deep learning systems. This means it is now easier to integrate logical structures into systems that use deep learning components. 
Sudoko results: The SATNet model does well against a basic ConvNet model, as well as a model fed with a binary mask which indicates which bits need to be learned. SATNet outperforms these systems, scoring 98.3% on an original sudoko set when given the numeric inputs. More impressively, it obtains a score of 63.2% on &lsquo;visual sudoko&rsquo; (traditional convnet: 0%), which is where they replace the digits with handwritten MNIST digits and feed it in. Specifically, they use a convnet to parse the figures in the Sudoko image, then pass this
Why this matters: Hybrid AI systems which fuse the general utility-class capabilities of deep learning components with more specific systems seems like a way to bridge traditional and symbolic AI, and making such systems be easy to add into larger systems. &ldquo;Our hope is that by wrapping a powerful yet generic primitive such as MAXSAT solving within a differentiable framework, our solver can enable &ldquo;implicit&rdquo; logical reasoning to occur where needed within larger frameworks, even if the precise structure of the domain is unknown and must be learned from data&rdquo;.&nbsp;&nbsp;Read more: SATNet: Bridging deep learning and logical reasoning using a differentiable satisfiability solver (Arxiv).
#####################################################
Squeezing neural nets onto microcontrollers via neural architecture search:&hellip;Get ready for billions of things to gain deep learning-based sense&amp;respond capacity&hellip;Researchers with ARM ML Research and Princeton University want to make it easier for people to deploy advanced artificial intelligence capabilities onto microcontrollers (MCUs) &ndash; something that has been difficult to do so far because today&rsquo;s neural networks techniques are too computationally expensive and memory-intensive to be easily deployed onto MCUs. 
MCUs and why they matter: Microcontrollers are the sorts of ultra-tiny lumps of computation embedded in things like fridges, microwaves, very small drones, small cameras, and other electronic widgets. To put this in perspective, in the developed world a typical person will have around four distinct desktop-class chips (eg, their phone, a laptop, etc), while having somewhere on the order of three dozen MCUs; a typical mid-range car might pack as many as 30 MCUs inside itself. 
MCUs shipped in 2019 (projection): 50 billionGPUs shipped in 2018: 100 million 
…