---

layout: post
category: product
title: "Import AI 133: The death of Moore’s Law means spring for chip designers; TF-Replicator lets people parallelize easily; and fighting human trafficking with the Hotels 50K dataset"
date: 2019-02-12 06:11:46
link: https://vrhk.co/2TLIMqK
image: 
domain: jack-clark.net
author: "Jack Clark"
icon: https://s2.wp.com/i/webclip.png
excerpt: "Administrative note: A short issue this week as I&rsquo;ve spent the past few days participating in an OECD working group on AI principles and then spending time at the Global Governance of AI Summit in Dubai.The death of Moore&rsquo;s Law means springtime for new chips, say long-time hardware researchers (one of whom is the chairman of Alphabet):&hellip;Or: follow these tips and you may also make a chip 80X as cost-effective as an Intel or AMD chip&hellip;General purpose computer chips are not going to get dramatically faster in the future as they are running into fundamental limitations dictated by physics. Put another way: we live currently in the twilight era of Moore&rsquo;s Law, as almost five decades of predictable improvements in computer power give way to more discontinuous leaps in capability as a consequence of the invention of specialized hardware platforms, rather than improvement in general chips.&nbsp; What does this mean? According to John Hennessy and David Patterson &ndash; who are responsible for some major inventions in computer architecture, like TKTKTK &ndash; today&rsquo;s engineers have three main options to pursue when seeking to create chips of greater capability:&nbsp; &nbsp;&ndash; Rewrite software to increase performance: its 47X faster to do a matrix multiply in (well-optimized) C code than it is in Python. You can further optimize this by adding in techniques for better parallelizing code (gets you a 366X improvement when paired with C); optimize the way the code interfaces to the physical memory layout of the computer(s) you&rsquo;re dealing with (gets you a 6,727X improvement, when stacked on the two prior optimizations); and you can improvement performance further by using SIMD parallelism techniques (a further 62,806X faster than plain python). The authors think &ldquo;there are likely many programs for which factors of 100 to 1,000 could be achieved&rdquo; if people bothered to write their code in this way.&nbsp; &nbsp;&ndash; Use domain-specific chip architectures: What&rsquo;s better, a hammer designed for everything, or a hammer designed for specific objects with a specific mass and frictional property? There&rsquo;s obviously a tradeoff here, but the gist of this piece is that: normal hammers aren&rsquo;t gonna get dramatically better, so engineers need to design custom ones. This is the same sort of logic that has led to Google creating its own internal chip-design team to work on Tensor Processing Units (TPUs), or for Microsoft to create teams of people working to write stuff to customize field-programmable gate arrays (FPGAs) fpr specific tasks.&nbsp; &nbsp;&ndash; Domain-specific, highly-optimized languages: The way to get the best performance is to combine both of the above ideas: design a new hardware platform, and also design a new domain-specific software language to run on top of it, stacking the efficiencies. You can get pretty good gains here: &ldquo;Using a weighted arithmetic mean based on six common inference programs in Google data centers, the TPU is 29X faster than a general-purpose CPU. Since the TPU requires less than half the power, it has an energy efficiency for this workload that is more than 80X better than a general-purpose CPU,&rdquo; they explain.&nbsp; Why this matters: If we don&rsquo;t figure out how to further increase the efficiency of our compute hardware and the software we use to run programs on it, then most existing AI techniques based on deep learning are going to fail to deliver on their promise &ndash; this is because we know that for many DL applications it&rsquo;s relatively easy to further improve performance simply by throwing larger chunks of compute at the problem. At the same time, parallelization across increasingly large pools of hardware can be a pain (see: TF-Replicator), so at some point these gains may diminish. Therefore, if we don&rsquo;t figure out ways to make our chips substantially faster and more efficient, we&rsquo;re going to have to design dramatically more sample-efficient AI approaches to get the gains many researchers are targeting.&nbsp; Read more: A New Golden Age for Computer Architecture (Communications of the ACM).
Want to deploy machine learning models on a bunch of hardware without your brain melting? Consider using TF-Replicator:&hellip;Deepmind-designed software library reduces the pain of parallelizing AI workloads&hellip;More powerful AI capabilities tend to require throwing more compute or time at a given AI training run; the majority of (well-funded) researchers opt for compute, and this has driven an explosion in the amount of computers used to train AI systems. That has meant that researchers are starting to need to program AI systems that can neatly run across multiple blobs of hardware of varying size without crashing &ndash; this is extremely hard to do!&nbsp; To help with this, DeepMind has released TF-Replicator, a framework for distributed machine learning on TensorFlow. TF-Replicator makes it easy for people to run code on different hardware platforms (for example, GPUs or TPUs) at large-scale using the TensorFlow AI framework. One of the key concepts introduced by TF-Replicator is the notion of wrapping up different parts of a machine learning job in wrappers that make it easy to parallelize the workloads within.&nbsp; Case study: TF-Replicator can train systems to obtain scores that match the best published result on the ImageNet dataset, scaling to up to 64 GPUs or or 32 TPUs, &ldquo;without any systems optimization specific to ImageNet classification&rdquo;, they write. They also show how to use TF-Replicator to train more sophisticated synthetic imagery systems by scaling training to enough GPUs to use a bigger batch size, which appears to lead to qualitative improvements. They also show how to use the technology to further speed training of reinforcement learning approaches.&nbsp; Why it matters: Software packages like TF-Replicator represent the industrialization of AI &ndash; in some sense, they can be seen as abstractions that help take information from one domain and port it into another. In my head, whenever I see stuff like TF-Replicator I think of it as being emblematic of a new merchant arriving that can work as a middleman between a shopkeeper and a factory that the shopkeeper wants to buy goods from &ndash; in the same way a middleman makes it so the shopkeeper doesn&rsquo;t have to think about the finer points of international shipping &amp; taxation &amp; regulation and can just focus on running their shop, TF-Replicator stops researchers from having to know too much about the finer details of distributed systems design when building their experiments.&nbsp; Read more: TF-Replicator: Distributed Machine Learning For Researchers (Arxiv).
Fighting human trafficking with the Hotels-50k dataset:&hellip;New dataset designed to help people match photos to specific hotels&hellip;Researchers with George Washington University, Adobe Research, and Temple University have released Hotels-50k, &ldquo;a large-scale dataset designed to support research in hotel recognition for images with the long term goal of supporting robust applications to aid in criminal investigations&rdquo;.&nbsp; Hotels-50k consists of one million images from approximately 50,000 hotels. The data primarily comes from travel websites such as Expedia, as well as around 50,000 images from the &lsquo;TrafficCam&rdquo; anti-human trafficking application.&nbsp; The dataset includes metadata like the hotel name, geographic location, and hotel chain it is a part of (if at all), as well as the source of the data. &ldquo;Images are most abundant in the United States, Western Europe and along popular coastlines,&rdquo; the researchers explain.&nbsp; Why this matters: Datasets like this will let us use AI systems to create a &ldquo;sense and respond&rdquo; automatic capability to respond to things like photos from human trafficking hotels. I&rsquo;m general…"

---

### Import AI 133: The death of Moore’s Law means spring for chip designers; TF-Replicator lets people parallelize easily; and fighting human trafficking with the Hotels 50K dataset

Administrative note: A short issue this week as I&rsquo;ve spent the past few days participating in an OECD working group on AI principles and then spending time at the Global Governance of AI Summit in Dubai.The death of Moore&rsquo;s Law means springtime for new chips, say long-time hardware researchers (one of whom is the chairman of Alphabet):&hellip;Or: follow these tips and you may also make a chip 80X as cost-effective as an Intel or AMD chip&hellip;General purpose computer chips are not going to get dramatically faster in the future as they are running into fundamental limitations dictated by physics. Put another way: we live currently in the twilight era of Moore&rsquo;s Law, as almost five decades of predictable improvements in computer power give way to more discontinuous leaps in capability as a consequence of the invention of specialized hardware platforms, rather than improvement in general chips.&nbsp; What does this mean? According to John Hennessy and David Patterson &ndash; who are responsible for some major inventions in computer architecture, like TKTKTK &ndash; today&rsquo;s engineers have three main options to pursue when seeking to create chips of greater capability:&nbsp; &nbsp;&ndash; Rewrite software to increase performance: its 47X faster to do a matrix multiply in (well-optimized) C code than it is in Python. You can further optimize this by adding in techniques for better parallelizing code (gets you a 366X improvement when paired with C); optimize the way the code interfaces to the physical memory layout of the computer(s) you&rsquo;re dealing with (gets you a 6,727X improvement, when stacked on the two prior optimizations); and you can improvement performance further by using SIMD parallelism techniques (a further 62,806X faster than plain python). The authors think &ldquo;there are likely many programs for which factors of 100 to 1,000 could be achieved&rdquo; if people bothered to write their code in this way.&nbsp; &nbsp;&ndash; Use domain-specific chip architectures: What&rsquo;s better, a hammer designed for everything, or a hammer designed for specific objects with a specific mass and frictional property? There&rsquo;s obviously a tradeoff here, but the gist of this piece is that: normal hammers aren&rsquo;t gonna get dramatically better, so engineers need to design custom ones. This is the same sort of logic that has led to Google creating its own internal chip-design team to work on Tensor Processing Units (TPUs), or for Microsoft to create teams of people working to write stuff to customize field-programmable gate arrays (FPGAs) fpr specific tasks.&nbsp; &nbsp;&ndash; Domain-specific, highly-optimized languages: The way to get the best performance is to combine both of the above ideas: design a new hardware platform, and also design a new domain-specific software language to run on top of it, stacking the efficiencies. You can get pretty good gains here: &ldquo;Using a weighted arithmetic mean based on six common inference programs in Google data centers, the TPU is 29X faster than a general-purpose CPU. Since the TPU requires less than half the power, it has an energy efficiency for this workload that is more than 80X better than a general-purpose CPU,&rdquo; they explain.&nbsp; Why this matters: If we don&rsquo;t figure out how to further increase the efficiency of our compute hardware and the software we use to run programs on it, then most existing AI techniques based on deep learning are going to fail to deliver on their promise &ndash; this is because we know that for many DL applications it&rsquo;s relatively easy to further improve performance simply by throwing larger chunks of compute at the problem. At the same time, parallelization across increasingly large pools of hardware can be a pain (see: TF-Replicator), so at some point these gains may diminish. Therefore, if we don&rsquo;t figure out ways to make our chips substantially faster and more efficient, we&rsquo;re going to have to design dramatically more sample-efficient AI approaches to get the gains many researchers are targeting.&nbsp; Read more: A New Golden Age for Computer Architecture (Communications of the ACM).
Want to deploy machine learning models on a bunch of hardware without your brain melting? Consider using TF-Replicator:&hellip;Deepmind-designed software library reduces the pain of parallelizing AI workloads&hellip;More powerful AI capabilities tend to require throwing more compute or time at a given AI training run; the majority of (well-funded) researchers opt for compute, and this has driven an explosion in the amount of computers used to train AI systems. That has meant that researchers are starting to need to program AI systems that can neatly run across multiple blobs of hardware of varying size without crashing &ndash; this is extremely hard to do!&nbsp; To help with this, DeepMind has released TF-Replicator, a framework for distributed machine learning on TensorFlow. TF-Replicator makes it easy for people to run code on different hardware platforms (for example, GPUs or TPUs) at large-scale using the TensorFlow AI framework. One of the key concepts introduced by TF-Replicator is the notion of wrapping up different parts of a machine learning job in wrappers that make it easy to parallelize the workloads within.&nbsp; Case study: TF-Replicator can train systems to obtain scores that match the best published result on the ImageNet dataset, scaling to up to 64 GPUs or or 32 TPUs, &ldquo;without any systems optimization specific to ImageNet classification&rdquo;, they write. They also show how to use TF-Replicator to train more sophisticated synthetic imagery systems by scaling training to enough GPUs to use a bigger batch size, which appears to lead to qualitative improvements. They also show how to use the technology to further speed training of reinforcement learning approaches.&nbsp; Why it matters: Software packages like TF-Replicator represent the industrialization of AI &ndash; in some sense, they can be seen as abstractions that help take information from one domain and port it into another. In my head, whenever I see stuff like TF-Replicator I think of it as being emblematic of a new merchant arriving that can work as a middleman between a shopkeeper and a factory that the shopkeeper wants to buy goods from &ndash; in the same way a middleman makes it so the shopkeeper doesn&rsquo;t have to think about the finer points of international shipping &amp; taxation &amp; regulation and can just focus on running their shop, TF-Replicator stops researchers from having to know too much about the finer details of distributed systems design when building their experiments.&nbsp; Read more: TF-Replicator: Distributed Machine Learning For Researchers (Arxiv).
Fighting human trafficking with the Hotels-50k dataset:&hellip;New dataset designed to help people match photos to specific hotels&hellip;Researchers with George Washington University, Adobe Research, and Temple University have released Hotels-50k, &ldquo;a large-scale dataset designed to support research in hotel recognition for images with the long term goal of supporting robust applications to aid in criminal investigations&rdquo;.&nbsp; Hotels-50k consists of one million images from approximately 50,000 hotels. The data primarily comes from travel websites such as Expedia, as well as around 50,000 images from the &lsquo;TrafficCam&rdquo; anti-human trafficking application.&nbsp; The dataset includes metadata like the hotel name, geographic location, and hotel chain it is a part of (if at all), as well as the source of the data. &ldquo;Images are most abundant in the United States, Western Europe and along popular coastlines,&rdquo; the researchers explain.&nbsp; Why this matters: Datasets like this will let us use AI systems to create a &ldquo;sense and respond&rdquo; automatic capability to respond to things like photos from human trafficking hotels. I&rsquo;m general…