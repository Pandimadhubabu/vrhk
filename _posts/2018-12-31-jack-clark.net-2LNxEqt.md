---

layout: post
category: product
title: "Import AI 127: Why language AI advancements may make Google more competitive; COCO image captioning systems don’t live up to the hype, and Amazon sees 3X growth in voice shopping via Alexa"
date: 2018-12-31 19:36:45
link: https://vrhk.co/2LNxEqt
image: 
domain: jack-clark.net
author: "Jack Clark"
icon: https://s2.wp.com/i/webclip.png
excerpt: "Amazon sees 3X growth in voice shopping via Alexa:&hellip;Growth correlates to a deepening data moat for the e-retailer&hellip;Retail colossus Amazon saw a 3X increase in the number of orders place via its virtual personal assistant Alexa during Christmas 2018, compared to Christmas 2017. &nbsp;&nbsp;Why it matters: The more people use Alexa, the more data Amazon will be able to access to further improve the effectiveness of the personal assistant &ndash; and as explored in last week&rsquo;s discussion of Microsoft&rsquo;s &lsquo;XiaoIce&rsquo; chatbot, it&rsquo;s likely that such data can ultimately be fed back into the training of Alexa to carry out longer, free-flowing conversations, potentially driving usage even higher.&nbsp; Read more: Amazon Customers Made This Holiday Season Record-Breaking with More Items Ordered Worldwide Than Ever Before ( Press Release).
Step aside COCO, Nocaps is the new image captioning challenge to target:&hellip;Thought image captioning was super-human? New benchmark suggests otherwise&hellip;Researchers with the Georgia Institute of Technology and Facebook AI Research have developed nocaps, &ldquo;the first rigorous and large-scale benchmark for novel object captioning, containing over 500 novel object classes&rdquo;. Novel object captioning tests the ability of computers to describe images containing objects not seen in the original imagecaption datasets (like COCO) that object recognition systems have been trained on.&nbsp;&nbsp;How Nocaps works: The benchmark consists of a validation and a test set comprised of 4,500 and 10,6000 images sources from the &lsquo;Open Images&rsquo; object detection dataset, with each image accompanied by 10 reference captions. For the training set, developers can use image-caption pairs from the COCO image captioning training set (which contain 118,000 images across 80 object classes) as well as the Open Images V4 training set, which contains 1.7 million images annotated with bounding boxes for 600 object classes. Successful Nocaps systems will have to learn to use knowledge gained from the large training set to create captions for scenes containing objects for which they lack imageobject sentence pairs in the training set. Out of the 600 objects in open images, &ldquo;500 are never or exceedingly rarely mentioned in COCO captions&rdquo;. &nbsp;&nbsp;Reassuringly difficult: &ldquo;To the best of our knowledge, nocaps is the only image captioning benchmark in which humans outperform state-of-the-art models in automatic evaluation&rdquo;, the researchers write. Nocaps is also significantly more diverse than the COCO benchmark, with Nocaps images typically containing more object classes per image, and greater diversity. &ldquo;Less than 10% of all COCO images contain more than 6 object classes, while such images constitutes almost 22% of nocaps dataset.&rdquo;&nbsp; Data plumbing: One of the secrets of modern AI research is how much work goes into developing datasets or compute infrastructure, relative to work on actual AI algorithms. One challenge the Nocaps researchers dealt with when creating data was having to train crowd workers on services like Mechanical Turk to come up with good captions: one challenge they experienced was that if they didn&rsquo;t &ldquo;prime&rdquo; the crowd workers with prompts to use when coming up with the captions, they wouldn&rsquo;t necessarily use the keywords that correlated to the 500 obscure objects in the dataset. &nbsp;&nbsp;Baseline results: The researchers test two baseline algorithms (Up-Down and Neural Baby Talk, both with augmentations) against nocaps. They also split the dataset into subsets of various difficulty &ndash; in-domain contains objects which also belong to the COCO dataset (so the algorithms can train on imagecaption pairs); near-domain contains objects that include some objects which aren&rsquo;t in COCO, and out-of-domain consists of images that do not contain any object labels from COCO classes. They use a couple of different evaluative techniques (CIDEr and SPICE) to evaluate the performance of these systems, and also evaluate these systems against the human captions to create a baseline. The results show that nocaps is more challenging than COCO, and systems currently lack generalization properties sufficient to score well on out-of-domain challenges.&nbsp; To give you a sense of what performance looks like here, here&rsquo;s how Up-Down augmented with Constrained Beam Search does, compared to human baselines (evaluation via CIDEr), on the nocaps validation set: In-domain 72.3 (versus 83.3 for humans); near-domain 63.2 (versus 85.5); out-of-domain 41.4 (versus 91.4). &nbsp;&nbsp;Why this matters: AI progress can be catalyzed via the invention of better benchmarks which highlight areas where existing algorithms are deficient, and provide motivating tests against which researchers can develop new systems. The takeaway from the baselines study of nocaps is that we&rsquo;re yet to develop truly robust image captioning systems capable of integrating object representations from open images with captions primed from COCO. &ldquo;We strongly believe that improvements on this benchmark will accelerate progress towards image captioning in the wild,&rdquo; the researchers write.&nbsp; Read more: nocaps: novel object captioning at scale (Arxiv). &nbsp;&nbsp;More information about nocaps can be found on its official website (nocaps).
Google boosts document retrieval performance by 50-100% using BERT language model:&hellip;Enter the fully neural search engine&hellip;Google has shown how to use recent innovations in language modeling to dramatically improve the skill with which AI systems can take in a search query and re-word the question to generate the most relevant answer for a user. This research has significant implications for the online economy, as it shows how yet another piece of traditionally hand-written rule-based software can be replaced with systems where the rules are figured out by machines on their own. &nbsp;&nbsp;How it works: Google&rsquo;s research shows how to convert a search problem into one amenable to a system that implements hierarchical reinforcement learning, where an RL agent controls multiple RL agents that interact with an environment that provides answers and rewards (e.g.: a search engine with user feedback) with the goal &ldquo;to generate reformulations [of questions] such that the expected returned reward (i.e., correct answers) is maximized&rdquo;. One of the key parts of this research is splitting it into a hierarchical problem by having a meta-agent and numerous sub agents &ndash; the sub-agents are sequence-to-sequence models trained on a partition of the dataset that take in the query and output reformulated queries, these candidate queries are sent to a meta-agent which aggregates these queries and is trained via RL to select for the best scoring ones.&nbsp;&nbsp;The Surprising Power of BERT: The researchers test their system again question answering baselines &ndash; here they show that a stock BERT system &ldquo;without any modification from its original implementation&rdquo; gets state-of-the-art scores. (One odd thing: When they augment BERT with their own multi-agent approach they don&rsquo;t see a further increase in performance, suggesting more research is needed to better suss out the benefits of systems like this. &nbsp;&nbsp;50-100% improvement, with BERT: They also test their system against three document retrieval benchmarks: TREC-CAR, where the query is a Wikipedia article with the title of one of its sections and the answer is a paragraph within that section; Jeopardy, which asks the system to come up with the correct answer in response to a question from the eponymous game show, and MSA, where the query is the title of an academic paper and the answer is the papers cited within the paper. The researchers test various versions of their approach against baselines BM25, PRF, and Rele…"

---

### Import AI 127: Why language AI advancements may make Google more competitive; COCO image captioning systems don’t live up to the hype, and Amazon sees 3X growth in voice shopping via Alexa

Amazon sees 3X growth in voice shopping via Alexa:&hellip;Growth correlates to a deepening data moat for the e-retailer&hellip;Retail colossus Amazon saw a 3X increase in the number of orders place via its virtual personal assistant Alexa during Christmas 2018, compared to Christmas 2017. &nbsp;&nbsp;Why it matters: The more people use Alexa, the more data Amazon will be able to access to further improve the effectiveness of the personal assistant &ndash; and as explored in last week&rsquo;s discussion of Microsoft&rsquo;s &lsquo;XiaoIce&rsquo; chatbot, it&rsquo;s likely that such data can ultimately be fed back into the training of Alexa to carry out longer, free-flowing conversations, potentially driving usage even higher.&nbsp; Read more: Amazon Customers Made This Holiday Season Record-Breaking with More Items Ordered Worldwide Than Ever Before ( Press Release).
Step aside COCO, Nocaps is the new image captioning challenge to target:&hellip;Thought image captioning was super-human? New benchmark suggests otherwise&hellip;Researchers with the Georgia Institute of Technology and Facebook AI Research have developed nocaps, &ldquo;the first rigorous and large-scale benchmark for novel object captioning, containing over 500 novel object classes&rdquo;. Novel object captioning tests the ability of computers to describe images containing objects not seen in the original imagecaption datasets (like COCO) that object recognition systems have been trained on.&nbsp;&nbsp;How Nocaps works: The benchmark consists of a validation and a test set comprised of 4,500 and 10,6000 images sources from the &lsquo;Open Images&rsquo; object detection dataset, with each image accompanied by 10 reference captions. For the training set, developers can use image-caption pairs from the COCO image captioning training set (which contain 118,000 images across 80 object classes) as well as the Open Images V4 training set, which contains 1.7 million images annotated with bounding boxes for 600 object classes. Successful Nocaps systems will have to learn to use knowledge gained from the large training set to create captions for scenes containing objects for which they lack imageobject sentence pairs in the training set. Out of the 600 objects in open images, &ldquo;500 are never or exceedingly rarely mentioned in COCO captions&rdquo;. &nbsp;&nbsp;Reassuringly difficult: &ldquo;To the best of our knowledge, nocaps is the only image captioning benchmark in which humans outperform state-of-the-art models in automatic evaluation&rdquo;, the researchers write. Nocaps is also significantly more diverse than the COCO benchmark, with Nocaps images typically containing more object classes per image, and greater diversity. &ldquo;Less than 10% of all COCO images contain more than 6 object classes, while such images constitutes almost 22% of nocaps dataset.&rdquo;&nbsp; Data plumbing: One of the secrets of modern AI research is how much work goes into developing datasets or compute infrastructure, relative to work on actual AI algorithms. One challenge the Nocaps researchers dealt with when creating data was having to train crowd workers on services like Mechanical Turk to come up with good captions: one challenge they experienced was that if they didn&rsquo;t &ldquo;prime&rdquo; the crowd workers with prompts to use when coming up with the captions, they wouldn&rsquo;t necessarily use the keywords that correlated to the 500 obscure objects in the dataset. &nbsp;&nbsp;Baseline results: The researchers test two baseline algorithms (Up-Down and Neural Baby Talk, both with augmentations) against nocaps. They also split the dataset into subsets of various difficulty &ndash; in-domain contains objects which also belong to the COCO dataset (so the algorithms can train on imagecaption pairs); near-domain contains objects that include some objects which aren&rsquo;t in COCO, and out-of-domain consists of images that do not contain any object labels from COCO classes. They use a couple of different evaluative techniques (CIDEr and SPICE) to evaluate the performance of these systems, and also evaluate these systems against the human captions to create a baseline. The results show that nocaps is more challenging than COCO, and systems currently lack generalization properties sufficient to score well on out-of-domain challenges.&nbsp; To give you a sense of what performance looks like here, here&rsquo;s how Up-Down augmented with Constrained Beam Search does, compared to human baselines (evaluation via CIDEr), on the nocaps validation set: In-domain 72.3 (versus 83.3 for humans); near-domain 63.2 (versus 85.5); out-of-domain 41.4 (versus 91.4). &nbsp;&nbsp;Why this matters: AI progress can be catalyzed via the invention of better benchmarks which highlight areas where existing algorithms are deficient, and provide motivating tests against which researchers can develop new systems. The takeaway from the baselines study of nocaps is that we&rsquo;re yet to develop truly robust image captioning systems capable of integrating object representations from open images with captions primed from COCO. &ldquo;We strongly believe that improvements on this benchmark will accelerate progress towards image captioning in the wild,&rdquo; the researchers write.&nbsp; Read more: nocaps: novel object captioning at scale (Arxiv). &nbsp;&nbsp;More information about nocaps can be found on its official website (nocaps).
Google boosts document retrieval performance by 50-100% using BERT language model:&hellip;Enter the fully neural search engine&hellip;Google has shown how to use recent innovations in language modeling to dramatically improve the skill with which AI systems can take in a search query and re-word the question to generate the most relevant answer for a user. This research has significant implications for the online economy, as it shows how yet another piece of traditionally hand-written rule-based software can be replaced with systems where the rules are figured out by machines on their own. &nbsp;&nbsp;How it works: Google&rsquo;s research shows how to convert a search problem into one amenable to a system that implements hierarchical reinforcement learning, where an RL agent controls multiple RL agents that interact with an environment that provides answers and rewards (e.g.: a search engine with user feedback) with the goal &ldquo;to generate reformulations [of questions] such that the expected returned reward (i.e., correct answers) is maximized&rdquo;. One of the key parts of this research is splitting it into a hierarchical problem by having a meta-agent and numerous sub agents &ndash; the sub-agents are sequence-to-sequence models trained on a partition of the dataset that take in the query and output reformulated queries, these candidate queries are sent to a meta-agent which aggregates these queries and is trained via RL to select for the best scoring ones.&nbsp;&nbsp;The Surprising Power of BERT: The researchers test their system again question answering baselines &ndash; here they show that a stock BERT system &ldquo;without any modification from its original implementation&rdquo; gets state-of-the-art scores. (One odd thing: When they augment BERT with their own multi-agent approach they don&rsquo;t see a further increase in performance, suggesting more research is needed to better suss out the benefits of systems like this. &nbsp;&nbsp;50-100% improvement, with BERT: They also test their system against three document retrieval benchmarks: TREC-CAR, where the query is a Wikipedia article with the title of one of its sections and the answer is a paragraph within that section; Jeopardy, which asks the system to come up with the correct answer in response to a question from the eponymous game show, and MSA, where the query is the title of an academic paper and the answer is the papers cited within the paper. The researchers test various versions of their approach against baselines BM25, PRF, and Rele…