---

layout: post
category: product
title: "Import AI 129: Uber’s POET creates its own curriculum; improving old games with ESRGAN; and controlling drones with gestures via UAV-CAPTURE"
date: 2019-01-14 19:36:47
link: https://vrhk.co/2FqXgZF
image: 
domain: jack-clark.net
author: "Jack Clark"
icon: https://s2.wp.com/i/webclip.png
excerpt: "Want 18 million labelled images? Tencent has got you covered:&hellip;Tencent ML-Images merges ImageNet and Open Images together&hellip;Data details: Tencent ML-Images is made of a combination of existing image databases such as ImageNet and Open Images, as well as associated class vocabularies. The new dataset contains 18 million images across 11,000 categories; on average, each image has eight tags applied to it.&nbsp; Transfer learning: The researchers train a ResNet-101 model on Tencent ML-Images, then finetune this pre-trained model on the ImageNet dataset and obtain scores in line with the state-of-the-art. One notable score is a claim of 80.73% top-1 accuracy on ImageNet when compared to a Google system pre-trained on an internal Google dataset called JFT-300M and fine-tuned on ImageNet &ndash; it&rsquo;s not clear to me why the authors would get a higher score than Google, when Google has almost 20X the amount of data available to it for pre-training (JFT contains ~300 million images). &nbsp;&nbsp;Why this matters: Datasets are one of the key inputs into the practice of AI research, and having access to larger-scale datasets will let researchers do two useful things: 1) Check promising techniques for robustness by seeing if they break when exposed to scaled-up datasets, and 2) Encourage the development of newer techniques that would otherwise overfit on smaller datasets (by some metrics, ImageNet is already quite well taken care of by existing research approaches, though more work is needed for things like improving top-1 accuracy).&nbsp; Read more: Tencent ML-Images: A Large-Scale Multi-Label Image Database for Visual Representation Learning (Arxiv). &nbsp;&nbsp;Get the data: Tencent ML-Images (Github).
Want an AI that teaches itself how to evolve? You want a POET:&hellip;Uber AI Labs research shows how to create potentially infinite curriculums&hellip;What happens when machines design and solve their own curriculums? That&rsquo;s an idea explored in a new research paper from Uber AI Labs. The researchers introduce Paired Open-Ended Trailblazer (POET), a system that aims to create machines with this capability &ldquo;by evolving a set of diverse and increasingly complex environmental challenges at the same time as collectively optimizing their solutions&rdquo;. Most research is a form of educated bet, and that&rsquo;s the case here: &ldquo;An important motivating hypothesis for POET is that the stepping stones that lead to solutions to very challenging environments are more likely to be found through a divergent, open-ended process than through a direct attempt to optimize in the challenging environment,&rdquo; they write. &nbsp;&nbsp;Testing in 2D: The researchers test POET in a 2-D environment where a robot is challenged to walk across a varied obstacle course of terrain. POET discovers behaviors that &ndash; the researchers claim &ndash; &ldquo;cannot be found directly on those same environmental challenges by optimizing on them only from scratch; neither can they be found through a curriculum-based process aimed at gradually building up to the same challenges POET invented and solved&rdquo;.&nbsp; &nbsp;How POET works: Unlike human poets, who work on the basis of some combination of lived experience and a keen sense of anguish, POET derives its power from an algorithm called &lsquo;trailblazer&rsquo;. Trailblazer works by starting with &ldquo;a simple environment (e.g. an obstacle course of entirely flat ground) and a randomly initialized weight vector (e.g. for a neural network)&rdquo;. The algorithm then performs the following three tasks at each iteration of the loop: generates new environments from those currently active, optimize paired agents with their respective environments, and try to transfer current agents from one environment to another. The researchers use Evolution Strategies from OpenAI to compute each iteration &ldquo;but any reinforcement learning algorithm could conceivably apply&rdquo;. &nbsp;&nbsp;The secret is Goldilocks: POET tries to create what I&rsquo;ll call &lsquo;goldilocks environments&rsquo;, in the sense that &ldquo;when new environments are generated, they are not added to the current population of environments unless they are neither too hard nor too easy for the current population&rdquo;. During training, POET creates an expanding set of environments which are made by modifying various obstacles within the 2D environment the agent needs to traverse.&nbsp; Results: Systems trained with POET learn solutions to environments that systems trained with Evolution Strategies from scratch are not able to do. The authors theorize that this is &ldquo;because newer environments in POET are created through mutations of older environments and because POET only accepts new environments that are not too easy not too hard for current agents, POET implicitly builds a curriculum for learning each environment it creates.&rdquo; &nbsp;&nbsp;Why it matters: Approaches like POET show how researchers can essentially use compute to generate arbitrarily large amounts of data to train systems on, and highlights how coming up with training regimes that involve an interactive loop between an agent, an environment, and a governing system for creating agents and environments, can create more capable systems than those that would be derived otherwise. Additionally, the implicit ideas governing the POET paper are that systems like this are a good fit for any problem where computers need to be able to learn flexible behaviors that deal with unanticipated scenarios. &ldquo;POET also offers practical opportunities in domains like autonomous driving, where through generating increasingly challenging and diverse scenarios it could uncover important edge cases and policies to solve them,&rdquo; the researchers write.&nbsp; Read more: Paired Open-Ended Trailblazer (POET): Endlessly Generating Increasingly Complex and Diverse Learning Environments and Their Solutions (Arxiv).
Making old games look better with GANs:&hellip;ESRGAN revitalises Max Payne&hellip;A post to the Gamespot video gaming forums shows how ESRGAN &ndash; Enhanced Super Resolution Generative Adversarial Networks &ndash; can improve the graphics of old games like Max Payne. ESRGAN gives game modders the ability to upscale old game textures through the use of GANs, improving the appearance of old games.&nbsp; Read more: Max Payne gets an amazing HD Texture Pack using ESRGAN that is available for download (Dark Side of Gaming).
Google teaches AI to learn to semantically segment objects:&hellip;Auto-DeepLab takes neural architecture search to harder problem domain&hellip;Researchers with Johns Hopkins University, Google, and Stanford University have created an AI system called Auto-DeepLab that has learned to perform efficient semantic segmentation of images &ndash; a challenging task in computer vision, which requires labeling the various objects in an image and understanding their borders. The system developed by the researchers uses a hierarchical search function to both learn to come up with specific neural network cell designs to inform layer-wise computations, as well as figuring out the overall network architecture that chains these cells together. &ldquo;Our goal is to jointly learn a good combination of repeatable cell structure and network structure specifically for semantic image segmentation,&rdquo; the researchers write.&nbsp;&nbsp;Efficiency: One of the drawbacks of neural architecture search approaches is the inherent computational expense, with many techniques demanding hundreds of GPUs to train systems. Here, the researchers show that their approach is efficient, able to find well-performing architectures for semantic segmentation of the &lsquo;Cityscapes&rsquo; dataset in about 3 days of one P100 GPU. &nbsp;&nbsp;&nbsp;Results: The network comes up with an effective design, as evidenced by the results on the cityscapes dataset. &ldquo;With extra coarse annotat…"

---

### Import AI 129: Uber’s POET creates its own curriculum; improving old games with ESRGAN; and controlling drones with gestures via UAV-CAPTURE

Want 18 million labelled images? Tencent has got you covered:&hellip;Tencent ML-Images merges ImageNet and Open Images together&hellip;Data details: Tencent ML-Images is made of a combination of existing image databases such as ImageNet and Open Images, as well as associated class vocabularies. The new dataset contains 18 million images across 11,000 categories; on average, each image has eight tags applied to it.&nbsp; Transfer learning: The researchers train a ResNet-101 model on Tencent ML-Images, then finetune this pre-trained model on the ImageNet dataset and obtain scores in line with the state-of-the-art. One notable score is a claim of 80.73% top-1 accuracy on ImageNet when compared to a Google system pre-trained on an internal Google dataset called JFT-300M and fine-tuned on ImageNet &ndash; it&rsquo;s not clear to me why the authors would get a higher score than Google, when Google has almost 20X the amount of data available to it for pre-training (JFT contains ~300 million images). &nbsp;&nbsp;Why this matters: Datasets are one of the key inputs into the practice of AI research, and having access to larger-scale datasets will let researchers do two useful things: 1) Check promising techniques for robustness by seeing if they break when exposed to scaled-up datasets, and 2) Encourage the development of newer techniques that would otherwise overfit on smaller datasets (by some metrics, ImageNet is already quite well taken care of by existing research approaches, though more work is needed for things like improving top-1 accuracy).&nbsp; Read more: Tencent ML-Images: A Large-Scale Multi-Label Image Database for Visual Representation Learning (Arxiv). &nbsp;&nbsp;Get the data: Tencent ML-Images (Github).
Want an AI that teaches itself how to evolve? You want a POET:&hellip;Uber AI Labs research shows how to create potentially infinite curriculums&hellip;What happens when machines design and solve their own curriculums? That&rsquo;s an idea explored in a new research paper from Uber AI Labs. The researchers introduce Paired Open-Ended Trailblazer (POET), a system that aims to create machines with this capability &ldquo;by evolving a set of diverse and increasingly complex environmental challenges at the same time as collectively optimizing their solutions&rdquo;. Most research is a form of educated bet, and that&rsquo;s the case here: &ldquo;An important motivating hypothesis for POET is that the stepping stones that lead to solutions to very challenging environments are more likely to be found through a divergent, open-ended process than through a direct attempt to optimize in the challenging environment,&rdquo; they write. &nbsp;&nbsp;Testing in 2D: The researchers test POET in a 2-D environment where a robot is challenged to walk across a varied obstacle course of terrain. POET discovers behaviors that &ndash; the researchers claim &ndash; &ldquo;cannot be found directly on those same environmental challenges by optimizing on them only from scratch; neither can they be found through a curriculum-based process aimed at gradually building up to the same challenges POET invented and solved&rdquo;.&nbsp; &nbsp;How POET works: Unlike human poets, who work on the basis of some combination of lived experience and a keen sense of anguish, POET derives its power from an algorithm called &lsquo;trailblazer&rsquo;. Trailblazer works by starting with &ldquo;a simple environment (e.g. an obstacle course of entirely flat ground) and a randomly initialized weight vector (e.g. for a neural network)&rdquo;. The algorithm then performs the following three tasks at each iteration of the loop: generates new environments from those currently active, optimize paired agents with their respective environments, and try to transfer current agents from one environment to another. The researchers use Evolution Strategies from OpenAI to compute each iteration &ldquo;but any reinforcement learning algorithm could conceivably apply&rdquo;. &nbsp;&nbsp;The secret is Goldilocks: POET tries to create what I&rsquo;ll call &lsquo;goldilocks environments&rsquo;, in the sense that &ldquo;when new environments are generated, they are not added to the current population of environments unless they are neither too hard nor too easy for the current population&rdquo;. During training, POET creates an expanding set of environments which are made by modifying various obstacles within the 2D environment the agent needs to traverse.&nbsp; Results: Systems trained with POET learn solutions to environments that systems trained with Evolution Strategies from scratch are not able to do. The authors theorize that this is &ldquo;because newer environments in POET are created through mutations of older environments and because POET only accepts new environments that are not too easy not too hard for current agents, POET implicitly builds a curriculum for learning each environment it creates.&rdquo; &nbsp;&nbsp;Why it matters: Approaches like POET show how researchers can essentially use compute to generate arbitrarily large amounts of data to train systems on, and highlights how coming up with training regimes that involve an interactive loop between an agent, an environment, and a governing system for creating agents and environments, can create more capable systems than those that would be derived otherwise. Additionally, the implicit ideas governing the POET paper are that systems like this are a good fit for any problem where computers need to be able to learn flexible behaviors that deal with unanticipated scenarios. &ldquo;POET also offers practical opportunities in domains like autonomous driving, where through generating increasingly challenging and diverse scenarios it could uncover important edge cases and policies to solve them,&rdquo; the researchers write.&nbsp; Read more: Paired Open-Ended Trailblazer (POET): Endlessly Generating Increasingly Complex and Diverse Learning Environments and Their Solutions (Arxiv).
Making old games look better with GANs:&hellip;ESRGAN revitalises Max Payne&hellip;A post to the Gamespot video gaming forums shows how ESRGAN &ndash; Enhanced Super Resolution Generative Adversarial Networks &ndash; can improve the graphics of old games like Max Payne. ESRGAN gives game modders the ability to upscale old game textures through the use of GANs, improving the appearance of old games.&nbsp; Read more: Max Payne gets an amazing HD Texture Pack using ESRGAN that is available for download (Dark Side of Gaming).
Google teaches AI to learn to semantically segment objects:&hellip;Auto-DeepLab takes neural architecture search to harder problem domain&hellip;Researchers with Johns Hopkins University, Google, and Stanford University have created an AI system called Auto-DeepLab that has learned to perform efficient semantic segmentation of images &ndash; a challenging task in computer vision, which requires labeling the various objects in an image and understanding their borders. The system developed by the researchers uses a hierarchical search function to both learn to come up with specific neural network cell designs to inform layer-wise computations, as well as figuring out the overall network architecture that chains these cells together. &ldquo;Our goal is to jointly learn a good combination of repeatable cell structure and network structure specifically for semantic image segmentation,&rdquo; the researchers write.&nbsp;&nbsp;Efficiency: One of the drawbacks of neural architecture search approaches is the inherent computational expense, with many techniques demanding hundreds of GPUs to train systems. Here, the researchers show that their approach is efficient, able to find well-performing architectures for semantic segmentation of the &lsquo;Cityscapes&rsquo; dataset in about 3 days of one P100 GPU. &nbsp;&nbsp;&nbsp;Results: The network comes up with an effective design, as evidenced by the results on the cityscapes dataset. &ldquo;With extra coarse annotat…