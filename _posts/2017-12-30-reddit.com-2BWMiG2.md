---

layout: post
category: threads
title: "[D] seq2seq why use cross entropy loss?"
date: 2017-12-30 09:18:23
link: https://vrhk.co/2BWMiG2
image: https://www.redditstatic.com/icon.png
domain: reddit.com
author: "reddit"
icon: http://www.redditstatic.com/desktop2x/img/favicon/apple-icon-57x57.png
excerpt: "i know that we can use cosine similarity to find closest vector to an word embedding vector like glove or word2vec. If we use word embedding in..."

---

### [D] seq2seq why use cross entropy loss? â€¢ r/MachineLearning

i know that we can use cosine similarity to find closest vector to an word embedding vector like glove or word2vec. If we use word embedding in...