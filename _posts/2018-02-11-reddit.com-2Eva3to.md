---

layout: post
category: threads
title: "[D] Take a fully trained ReLU net. Replace the activation function with max(0, x). Does it still perform fine?"
date: 2018-02-11 01:18:14
link: https://vrhk.co/2Eva3to
image: https://www.redditstatic.com/icon.png
domain: reddit.com
author: "reddit"
icon: http://www.redditstatic.com/desktop2x/img/favicon/apple-icon-57x57.png
excerpt: "(Obviously it's not possible to train with a discontinuous activation function, but that's not an issue for evaluation.) **Edit:** I got my..."

---

### [D] Take a fully trained ReLU net. Replace the activation function with max(0, x). Does it still perform fine? â€¢ r/MachineLearning

(Obviously it's not possible to train with a discontinuous activation function, but that's not an issue for evaluation.) **Edit:** I got my...