---

layout: post
category: product
title: "Import AI: 108: Learning language with fake sentences, Chinese researchers use RL to train prototype warehouse robots; and what the implications are of scaled-up Neural Architecture Search"
date: 2018-08-20 21:01:51
link: https://vrhk.co/2nRPCN9
image: 
domain: jack-clark.net
author: "Jack Clark"
icon: https://s2.wp.com/i/webclip.png
excerpt: "Learning with Junk:&hellip;Reversing sentences for better language models&hellip;Sometimes a little bit of junk data can be useful: that&rsquo;s the implication of new research from Stony Brook University, which shows that you can improve natural language processing systems by teaching them during training to distinguish between real and fake sentences.&nbsp; Technique: &ldquo;Given a large unlabeled corpus, for every original sentence, we add multiple fake sentences. The training task is then to take any given sentence as input and predict whether it is a real or fake sentence,&rdquo; they write. &ldquo;In particular, we propose to learn a sentence encoder by training a sequential model to solve the binary classification task of detecting whether a given input sentence is fake or real&rdquo;. &nbsp;&nbsp;The researchers create fake sentences in two ways: WordShuffle, which sees them shuffle some of the orders of the words in the sentence; and WordDrop, which sees them drop a random word from a sentence.&nbsp; Evaluation: They evaluate these systems on tasks including sentiment classification, question answering, subjectivity, retrieval, and others. Systems trained with this approach display significantly higher scores than prior language modeling approaches (specifically, the FastSent and Skipthought techniques. &nbsp;&nbsp;Why this matters: Language modeling is one of the hardest tasks that contemporary AI is evaluated on. Typically, most of today&rsquo;s systems fail to display much complexity in their learned models, likely due to the huge representational space of language, paired with the increased costs for getting different things wrong (it&rsquo;s way easier to notice a sentence error or spelling error than to see how the value of one or two of the pixels in a large generated image are off). Systems and approaches like those described in this paper show how we can use data augmentation techniques and discriminative training approaches to create high-performing systems. &nbsp;&nbsp;Read more: Fake Sentence Detection as a Training Task for Sentence Encoding (Arxiv).
Reinforcement learning breaks out of the simulator with new Chinese research:&hellip;Training robots via reinforcement learning to solve warehouse robot problems&hellip;Researchers with the Department of Mechanical and Biomedical Engineering of City University of Hong Kong, China, along with Metoak Technology Co, and Fuzhou University&rsquo;s College of Mathematics and Computer Science, have used reinforcement learning to train warehouse robots in simulation and transfer them to the real world. These are the same sorts of robots used by companies like Amazon and Walmart for automation of their own warehouses. The research has implications for how AI is going to revolutionize logistics and supply chains, as well as broadening the scope of capabilities of robots.&nbsp; The researchers&rsquo; develop a system for their logistics robots based around what they call: &ldquo;sensor-level decentralized collision avoidance&rdquo;. This &ldquo;requires neither perfect sensing for neighboring agents and obstacles nor tedious offline parameter-tuning for adapting to different scenarios&rdquo;. Each robot makes navigation decisions independently without any communication with others, and are trained in simulation via a multi-stage reinforcement learning scheme. The robots are able to perceive the world around them via a 2D laser scanner, and have full control over their translational and rotational velocity (think of them as autonomous dog-sized hockey pucks). &nbsp;&nbsp;Network architecture: They tweak and extend the Proximal Policy Optimization (PPO) algorithm to make it work in large-scale, parallel environments, then they train their robots using a two-stage training process: they first train 20 of them in a 2D randomized placement navigation scenario, where the robots need to learn basic movement and collision avoidance primitives. They then save the trained policy and use this to start a second training cycle, which trains 58 robots in a series of more complicated scenarios that involve different building dimensions, and so on. &nbsp;&nbsp;Mo&rsquo; AI, Mo Problems: Though the trained policies are useful and transfer into the world, they exhibit many of the idiosyncratic behaviors typical of AI systems, which will make them harder to deploy. &ldquo;For instance, as a robot runs towards its goal through a wide-open space without other agents, the robot may approach the goal in a curved trajectory rather than in a straight line,&rdquo; the researchers say. &ldquo;We have also observed that a robot may wander around its goal rather than directly moving toward the goal, even though the robot is already in the close proximity of the target.&rdquo; To get around this, the researchers design software to classify the type of scenario being faced by the robot, and then switch the robot between fully autonomous and PID-controlled modes according to the scenario. By using the hybrid system they create more efficient robots, because switching opportunistically into PID-control regimes leads to the robots typically taking straight line courses or moving and turning more precisely. &nbsp;&nbsp;Generalization: The researchers test their system&rsquo;s generalization by evaluating it on scenarios with non-cooperative robots which don&rsquo;t automatically help the other robots; with heterogeneous robots, so ones with different sizes and shapes; and in scenarios with larger numbers of robots than those controlled during simulation (100 versus 58). In tests, systems trained with both the RL and hybrid-RL system display far improved accuracy relative to supervised learning baselines; the systems are also flexible, able to get stuck less as you scale up the number of agents, and go through fewer collisions.&nbsp; Real world: The researchers also successfully test out their approach on robots deployed in the real world. For this, they develop a robot platform that uses the Hokuyo URG-04LX-UG01 2D LiDAR, a Pozyx localization based based on Ultra-Wide Band (UWB) tech, and the NVIDIA Jetson TX1 for computing, and then they test this platform on a variety of different robot chassis including a Turtlebod, the &lsquo;Igor&rsquo; robot from Hebi robotics, the Baidu Bear robot, and the Baidu shopping cart. They test their robots on simulated warehouse and office scenarios, including ones where robots need to shuttle between two transportation stations while avoiding pedestrian foot traffic; they also test the robots on tasks like following a person through a crowd, and around a platform. &ldquo;Our future work would be how to incorporate our approach with classical mapping methods (e.g. SLAM) and global path planners (e.g. RRT and A&lowast; ) to achieve satisfactory performance for planning a safe trajectory through a dynamic environment,&rdquo; they say. &nbsp;&nbsp;Why it matters: One of the legitimate criticisms of contemporary artificial intelligence is that though we&rsquo;ve got a lot of known successes for supervised learning, we have relatively few examples of ways in which reinforcement learning-based systems are doing productive economic work in the world &ndash; though somewhat preliminary, research papers like this indicate that RL is becoming tractable on real world hardware, and that the same qualities of generalization and flexibility seen on RL-trained policies developed in simulation also appear to be present in reality. If this trend holds it will increase the rate at which we deploy AI technology like this into the world.&nbsp;&nbsp;Read more: Fully Distributed Multi-Robot Collision Avoidance via Deep Reinforcement Learning for Safe and Efficient Navigation in Complex Scenarios (Arxiv).
Watch James Mickens explain, in a humorous manner, why life is pointless and we&rsquo;re all going to die:&hellip;Famed oddball researcher gives terror-inducing rant at Usenix&hellip;James Mickens is a sentient r…"

---

### Import AI: 108: Learning language with fake sentences, Chinese researchers use RL to train prototype warehouse robots; and what the implications are of scaled-up Neural Architecture Search

Learning with Junk:&hellip;Reversing sentences for better language models&hellip;Sometimes a little bit of junk data can be useful: that&rsquo;s the implication of new research from Stony Brook University, which shows that you can improve natural language processing systems by teaching them during training to distinguish between real and fake sentences.&nbsp; Technique: &ldquo;Given a large unlabeled corpus, for every original sentence, we add multiple fake sentences. The training task is then to take any given sentence as input and predict whether it is a real or fake sentence,&rdquo; they write. &ldquo;In particular, we propose to learn a sentence encoder by training a sequential model to solve the binary classification task of detecting whether a given input sentence is fake or real&rdquo;. &nbsp;&nbsp;The researchers create fake sentences in two ways: WordShuffle, which sees them shuffle some of the orders of the words in the sentence; and WordDrop, which sees them drop a random word from a sentence.&nbsp; Evaluation: They evaluate these systems on tasks including sentiment classification, question answering, subjectivity, retrieval, and others. Systems trained with this approach display significantly higher scores than prior language modeling approaches (specifically, the FastSent and Skipthought techniques. &nbsp;&nbsp;Why this matters: Language modeling is one of the hardest tasks that contemporary AI is evaluated on. Typically, most of today&rsquo;s systems fail to display much complexity in their learned models, likely due to the huge representational space of language, paired with the increased costs for getting different things wrong (it&rsquo;s way easier to notice a sentence error or spelling error than to see how the value of one or two of the pixels in a large generated image are off). Systems and approaches like those described in this paper show how we can use data augmentation techniques and discriminative training approaches to create high-performing systems. &nbsp;&nbsp;Read more: Fake Sentence Detection as a Training Task for Sentence Encoding (Arxiv).
Reinforcement learning breaks out of the simulator with new Chinese research:&hellip;Training robots via reinforcement learning to solve warehouse robot problems&hellip;Researchers with the Department of Mechanical and Biomedical Engineering of City University of Hong Kong, China, along with Metoak Technology Co, and Fuzhou University&rsquo;s College of Mathematics and Computer Science, have used reinforcement learning to train warehouse robots in simulation and transfer them to the real world. These are the same sorts of robots used by companies like Amazon and Walmart for automation of their own warehouses. The research has implications for how AI is going to revolutionize logistics and supply chains, as well as broadening the scope of capabilities of robots.&nbsp; The researchers&rsquo; develop a system for their logistics robots based around what they call: &ldquo;sensor-level decentralized collision avoidance&rdquo;. This &ldquo;requires neither perfect sensing for neighboring agents and obstacles nor tedious offline parameter-tuning for adapting to different scenarios&rdquo;. Each robot makes navigation decisions independently without any communication with others, and are trained in simulation via a multi-stage reinforcement learning scheme. The robots are able to perceive the world around them via a 2D laser scanner, and have full control over their translational and rotational velocity (think of them as autonomous dog-sized hockey pucks). &nbsp;&nbsp;Network architecture: They tweak and extend the Proximal Policy Optimization (PPO) algorithm to make it work in large-scale, parallel environments, then they train their robots using a two-stage training process: they first train 20 of them in a 2D randomized placement navigation scenario, where the robots need to learn basic movement and collision avoidance primitives. They then save the trained policy and use this to start a second training cycle, which trains 58 robots in a series of more complicated scenarios that involve different building dimensions, and so on. &nbsp;&nbsp;Mo&rsquo; AI, Mo Problems: Though the trained policies are useful and transfer into the world, they exhibit many of the idiosyncratic behaviors typical of AI systems, which will make them harder to deploy. &ldquo;For instance, as a robot runs towards its goal through a wide-open space without other agents, the robot may approach the goal in a curved trajectory rather than in a straight line,&rdquo; the researchers say. &ldquo;We have also observed that a robot may wander around its goal rather than directly moving toward the goal, even though the robot is already in the close proximity of the target.&rdquo; To get around this, the researchers design software to classify the type of scenario being faced by the robot, and then switch the robot between fully autonomous and PID-controlled modes according to the scenario. By using the hybrid system they create more efficient robots, because switching opportunistically into PID-control regimes leads to the robots typically taking straight line courses or moving and turning more precisely. &nbsp;&nbsp;Generalization: The researchers test their system&rsquo;s generalization by evaluating it on scenarios with non-cooperative robots which don&rsquo;t automatically help the other robots; with heterogeneous robots, so ones with different sizes and shapes; and in scenarios with larger numbers of robots than those controlled during simulation (100 versus 58). In tests, systems trained with both the RL and hybrid-RL system display far improved accuracy relative to supervised learning baselines; the systems are also flexible, able to get stuck less as you scale up the number of agents, and go through fewer collisions.&nbsp; Real world: The researchers also successfully test out their approach on robots deployed in the real world. For this, they develop a robot platform that uses the Hokuyo URG-04LX-UG01 2D LiDAR, a Pozyx localization based based on Ultra-Wide Band (UWB) tech, and the NVIDIA Jetson TX1 for computing, and then they test this platform on a variety of different robot chassis including a Turtlebod, the &lsquo;Igor&rsquo; robot from Hebi robotics, the Baidu Bear robot, and the Baidu shopping cart. They test their robots on simulated warehouse and office scenarios, including ones where robots need to shuttle between two transportation stations while avoiding pedestrian foot traffic; they also test the robots on tasks like following a person through a crowd, and around a platform. &ldquo;Our future work would be how to incorporate our approach with classical mapping methods (e.g. SLAM) and global path planners (e.g. RRT and A&lowast; ) to achieve satisfactory performance for planning a safe trajectory through a dynamic environment,&rdquo; they say. &nbsp;&nbsp;Why it matters: One of the legitimate criticisms of contemporary artificial intelligence is that though we&rsquo;ve got a lot of known successes for supervised learning, we have relatively few examples of ways in which reinforcement learning-based systems are doing productive economic work in the world &ndash; though somewhat preliminary, research papers like this indicate that RL is becoming tractable on real world hardware, and that the same qualities of generalization and flexibility seen on RL-trained policies developed in simulation also appear to be present in reality. If this trend holds it will increase the rate at which we deploy AI technology like this into the world.&nbsp;&nbsp;Read more: Fully Distributed Multi-Robot Collision Avoidance via Deep Reinforcement Learning for Safe and Efficient Navigation in Complex Scenarios (Arxiv).
Watch James Mickens explain, in a humorous manner, why life is pointless and we&rsquo;re all going to die:&hellip;Famed oddball researcher gives terror-inducing rant at Usenix&hellip;James Mickens is a sentient r…