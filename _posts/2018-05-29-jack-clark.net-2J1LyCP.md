---

layout: post
category: product
title: "Import AI: #96: Seeing heartbeats with DeepPhys, better synthetic images via SAGAN, and spotting pedestrians via a trans-European dataset"
date: 2018-05-29 19:07:06
link: https://vrhk.co/2J1LyCP
image: 
domain: jack-clark.net
author: "Jack Clark"
icon: https://s2.wp.com/i/webclip.png
excerpt: "Satellite imagery competition challenges systems to outline buildings, segment roads, and analyze land use patterns:&hellip;DeepGlobe competition and associated datasets designed to speed progress on strategic domain&hellip;Researchers with Facebook, DigitalGlobe, CosmiQ Works, Wageningen University, and the MIT Media Lab have revealed DeepGlobe 2018, a satellite imagery competition with three tasks and associated datasets. DeepGlobe is intended to yield improvements in the automated analysis of satellite images for disaster response, planning, and object detection. DeepGlobe 2018 has three tracks with linked datasets: road extraction (8,570 images), building detection (24,586 &lsquo;scenes&rsquo;, equivalent to a 650&times;650 image), and land cover classification (1,146 satellite images).&nbsp; Results: The researchers introduce some baseline performance numbers for each task; for road extraction they used a modified version of DeepLab with a ResNet18 backbone and Focal Loss, obtaining an Intersection over Union (IoU) score of 0.545; for building detection they used the top scoring solutions from a competition held on the same dataset in 2017, which obtain IoU scores of as high as .88 on cities like Las Vegas and as low as 0.54 on Khartoum; for land cover classification they implement a DeepLab system with a ResNet18 backbone and atrous spatial pyramid pooling (ASPP) to obtain an IoU scoe of 0.43. &nbsp;&nbsp;Why it matters: AI will increase the automated analysis capabilities people can nations can wield over their satellite imagery repositories. Progress in this domain directly influences geopolitics by giving rise to new techniques that different nations can use in conjunction with satellite data to watch and react to the world.&nbsp;&nbsp;Read more: DeepGlobe 2018: A Challenge to Parse the Earth through Satellite Images (Arxiv).
Trans-Europe Express!: Researchers release diverse &lsquo;EuroCity&rsquo; dataset:&hellip;31 cities across 12 countries yields a diverse dataset containing people in a huge variety of contexts&hellip;Researchers with the Environment Perception Group at carmaker Daimler AG and the Intelligent Vehicles Group at TU Delft have released EuroCity, a large-scale dataset for object and pedestrian detection within urban scenes. EuroCity comprises 45,000 distinct images containing more than a hundred thousand pedestrians in weather settings ranging from dry to wet. The dataset is one of the largest and most diverse yet released for detecting people in urban scenes and will be of particular interest to self-driving car developers.&nbsp;&nbsp;Data: The researchers collected the data via a two megapixel camera installed on the dashboard of their car which they drove through 31 cities in 12 European countries.. The dataset&rsquo;s diversity may help with generalization; results indicate that pre-training on the dataset substantially improved performance when transferring to solve tasks within the more widely-used CityPersons and KITTI datasets. &nbsp;&nbsp;Annotations: Pedestrians and vehicle riders are annotated in the dataset. If a rider, they are also annotated with sub-labels to describe their vehicle, such as bicycle, buggy, motorbike, scooter, tricycle, and wheelchair. The researchers also annotate confounding images, like posters that depict people, or images that catch reflections of people in windows, and additional phenomena like lens flares, motion blurs, raindrops, and so on. Annotations were performed via hand. &nbsp;&nbsp;Baselines: Four approaches &ndash; R-CNN, R-FCN, SSD, and YOLOv3 &ndash; are tested on the dataset to create baseline performance figures. Different variants of R-CNN perform best on all three tasks, followed by the performance of YOLOv3. &ldquo;Processing rates for the R-FCN, Faster R-CNN, SSD and YOLOv3 on non-upscaled test images were 1.2 fps, 1.7 fps, 2.4 fps and 3.8 fps, respectively, on a Intel(R) Core i7-5960X CPU 3.00 GHz processor and a NVidia GeForce GTX TITAN X with 12.2 GB memory&rdquo;.&nbsp;&nbsp;Why it matters: Datasets tend to motivate work on problems contained within them. Given the breadth and scale of EuroCity, it&rsquo;s likely its release will improve the state-of-the-art when it comes to pedestrian detection in busy or partially occluded scenes. It also hints at a future where hundreds of thousands of cars with dash cams are used to grow and augment continent-scale datasets. &nbsp;&nbsp;Read more: The EuroCity Persons Dataset: A Novel Benchmark for Object Detection (Arxiv).
&ldquo;I can guess your heart rate!&rdquo; (with DeepPhys):&hellip;Trained system predicts your heart beat from pixel inputs alone&hellip;MIT and Microsoft researchers have built DeepPhys, a network that can crudely predict a person&rsquo;s heart rate and breathing rate from RGB or infrared videos. They developed the network by building a couple of specific classification models based on domain knowledge about how to detect and analyze skin appearance and changes over time to better infer underlying biological phenomena.&nbsp; Results: The researchers test their system on four datasets, three recorded under controlled and uncontrolled lighting conditions, and the fourth involving infrared. Their approach outperforms other systems on a variety of evaluation criteria. Additionally, further tests showed that training the system on diverse data inputs can lead to better performance. &ldquo;The performance improvements were especially good for the tasks with increasing range and angular velocities of head rotation,&rdquo; they write. &ldquo;We attribute this improvement to the end-to-end nature of the model which is able to learn an improved mapping between the video color and motion information&rdquo;. &nbsp;&nbsp;Why it matters: Systems like this bring us closer to a world where the majority of cameras around us are performing a multitude of different analysis tasks, including ones we may not suspect are possible, like predicting our heart rate from images taken from security camera feeds.&nbsp; Read more: DeepPhys: Video-Based Physiological Measurement Using Convolutional Attention Networks (Arxiv).
Funny dogs no more! Google and Rutgers introduce &lsquo;SAGAN&rsquo;:&hellip;Want to be a better artist? Look inside yourself&hellip;One of the classic problems with GAN-generated images is the number of dog legs. What I mean by that is though these systems have become adept in recent years at generating synthetic imagery in a bunch of different domains, they&rsquo;ve remained stubbornly bad at modeling aspects of images that require a holistic understanding of the whole &ndash; like getting the number of legs right on a dogs body, or figuring out the correct physical dimensions of a cat&rsquo;s tail and paw relationship, and so on. &nbsp;&ldquo;While the state-of-the-art ImageNet GAN model excels at synthesizing image classes with few structural constraints (e.g. ocean, sky and landscape classes, which are distinguished more by texture than by geometry), it fails to capture geometric or structural patterns that occur consistently in some classes (for example, dogs are often drawn with realistic fur texture but without clearly defined separate feet),&rdquo; the researchers explain.&nbsp; Attention to the rescue: The researchers, which include GAN-inventor Ian Goodfellow, get around this issue by implementing what they call a Self-Attention Generative Adversarial Network (SAGAN). A SAGAN works by pairing a self-attention mechanism with the traditional machinery of GAN. &ldquo;The self-attention module is complementary to convolutions and helps with modeling long range, multi-level dependencies across image regions. Armed with self-attention, the generator can draw images in which fine details at every location are carefully coordinated with fine details in distant portions of the image,&rdquo; they write. &nbsp;&nbsp;Results: The resulting systems dramatically outperform other approaches when assessed by the Incepti…"

---

### Import AI: #96: Seeing heartbeats with DeepPhys, better synthetic images via SAGAN, and spotting pedestrians via a trans-European dataset

Satellite imagery competition challenges systems to outline buildings, segment roads, and analyze land use patterns:&hellip;DeepGlobe competition and associated datasets designed to speed progress on strategic domain&hellip;Researchers with Facebook, DigitalGlobe, CosmiQ Works, Wageningen University, and the MIT Media Lab have revealed DeepGlobe 2018, a satellite imagery competition with three tasks and associated datasets. DeepGlobe is intended to yield improvements in the automated analysis of satellite images for disaster response, planning, and object detection. DeepGlobe 2018 has three tracks with linked datasets: road extraction (8,570 images), building detection (24,586 &lsquo;scenes&rsquo;, equivalent to a 650&times;650 image), and land cover classification (1,146 satellite images).&nbsp; Results: The researchers introduce some baseline performance numbers for each task; for road extraction they used a modified version of DeepLab with a ResNet18 backbone and Focal Loss, obtaining an Intersection over Union (IoU) score of 0.545; for building detection they used the top scoring solutions from a competition held on the same dataset in 2017, which obtain IoU scores of as high as .88 on cities like Las Vegas and as low as 0.54 on Khartoum; for land cover classification they implement a DeepLab system with a ResNet18 backbone and atrous spatial pyramid pooling (ASPP) to obtain an IoU scoe of 0.43. &nbsp;&nbsp;Why it matters: AI will increase the automated analysis capabilities people can nations can wield over their satellite imagery repositories. Progress in this domain directly influences geopolitics by giving rise to new techniques that different nations can use in conjunction with satellite data to watch and react to the world.&nbsp;&nbsp;Read more: DeepGlobe 2018: A Challenge to Parse the Earth through Satellite Images (Arxiv).
Trans-Europe Express!: Researchers release diverse &lsquo;EuroCity&rsquo; dataset:&hellip;31 cities across 12 countries yields a diverse dataset containing people in a huge variety of contexts&hellip;Researchers with the Environment Perception Group at carmaker Daimler AG and the Intelligent Vehicles Group at TU Delft have released EuroCity, a large-scale dataset for object and pedestrian detection within urban scenes. EuroCity comprises 45,000 distinct images containing more than a hundred thousand pedestrians in weather settings ranging from dry to wet. The dataset is one of the largest and most diverse yet released for detecting people in urban scenes and will be of particular interest to self-driving car developers.&nbsp;&nbsp;Data: The researchers collected the data via a two megapixel camera installed on the dashboard of their car which they drove through 31 cities in 12 European countries.. The dataset&rsquo;s diversity may help with generalization; results indicate that pre-training on the dataset substantially improved performance when transferring to solve tasks within the more widely-used CityPersons and KITTI datasets. &nbsp;&nbsp;Annotations: Pedestrians and vehicle riders are annotated in the dataset. If a rider, they are also annotated with sub-labels to describe their vehicle, such as bicycle, buggy, motorbike, scooter, tricycle, and wheelchair. The researchers also annotate confounding images, like posters that depict people, or images that catch reflections of people in windows, and additional phenomena like lens flares, motion blurs, raindrops, and so on. Annotations were performed via hand. &nbsp;&nbsp;Baselines: Four approaches &ndash; R-CNN, R-FCN, SSD, and YOLOv3 &ndash; are tested on the dataset to create baseline performance figures. Different variants of R-CNN perform best on all three tasks, followed by the performance of YOLOv3. &ldquo;Processing rates for the R-FCN, Faster R-CNN, SSD and YOLOv3 on non-upscaled test images were 1.2 fps, 1.7 fps, 2.4 fps and 3.8 fps, respectively, on a Intel(R) Core i7-5960X CPU 3.00 GHz processor and a NVidia GeForce GTX TITAN X with 12.2 GB memory&rdquo;.&nbsp;&nbsp;Why it matters: Datasets tend to motivate work on problems contained within them. Given the breadth and scale of EuroCity, it&rsquo;s likely its release will improve the state-of-the-art when it comes to pedestrian detection in busy or partially occluded scenes. It also hints at a future where hundreds of thousands of cars with dash cams are used to grow and augment continent-scale datasets. &nbsp;&nbsp;Read more: The EuroCity Persons Dataset: A Novel Benchmark for Object Detection (Arxiv).
&ldquo;I can guess your heart rate!&rdquo; (with DeepPhys):&hellip;Trained system predicts your heart beat from pixel inputs alone&hellip;MIT and Microsoft researchers have built DeepPhys, a network that can crudely predict a person&rsquo;s heart rate and breathing rate from RGB or infrared videos. They developed the network by building a couple of specific classification models based on domain knowledge about how to detect and analyze skin appearance and changes over time to better infer underlying biological phenomena.&nbsp; Results: The researchers test their system on four datasets, three recorded under controlled and uncontrolled lighting conditions, and the fourth involving infrared. Their approach outperforms other systems on a variety of evaluation criteria. Additionally, further tests showed that training the system on diverse data inputs can lead to better performance. &ldquo;The performance improvements were especially good for the tasks with increasing range and angular velocities of head rotation,&rdquo; they write. &ldquo;We attribute this improvement to the end-to-end nature of the model which is able to learn an improved mapping between the video color and motion information&rdquo;. &nbsp;&nbsp;Why it matters: Systems like this bring us closer to a world where the majority of cameras around us are performing a multitude of different analysis tasks, including ones we may not suspect are possible, like predicting our heart rate from images taken from security camera feeds.&nbsp; Read more: DeepPhys: Video-Based Physiological Measurement Using Convolutional Attention Networks (Arxiv).
Funny dogs no more! Google and Rutgers introduce &lsquo;SAGAN&rsquo;:&hellip;Want to be a better artist? Look inside yourself&hellip;One of the classic problems with GAN-generated images is the number of dog legs. What I mean by that is though these systems have become adept in recent years at generating synthetic imagery in a bunch of different domains, they&rsquo;ve remained stubbornly bad at modeling aspects of images that require a holistic understanding of the whole &ndash; like getting the number of legs right on a dogs body, or figuring out the correct physical dimensions of a cat&rsquo;s tail and paw relationship, and so on. &nbsp;&ldquo;While the state-of-the-art ImageNet GAN model excels at synthesizing image classes with few structural constraints (e.g. ocean, sky and landscape classes, which are distinguished more by texture than by geometry), it fails to capture geometric or structural patterns that occur consistently in some classes (for example, dogs are often drawn with realistic fur texture but without clearly defined separate feet),&rdquo; the researchers explain.&nbsp; Attention to the rescue: The researchers, which include GAN-inventor Ian Goodfellow, get around this issue by implementing what they call a Self-Attention Generative Adversarial Network (SAGAN). A SAGAN works by pairing a self-attention mechanism with the traditional machinery of GAN. &ldquo;The self-attention module is complementary to convolutions and helps with modeling long range, multi-level dependencies across image regions. Armed with self-attention, the generator can draw images in which fine details at every location are carefully coordinated with fine details in distant portions of the image,&rdquo; they write. &nbsp;&nbsp;Results: The resulting systems dramatically outperform other approaches when assessed by the Incepti…